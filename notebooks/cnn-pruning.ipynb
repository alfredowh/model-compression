{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-28T18:56:02.664407Z",
     "start_time": "2024-12-28T18:55:56.371376Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:56:02.727612Z",
     "start_time": "2024-12-28T18:56:02.665416Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6aecd501748df270",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Model Model",
   "id": "4a5a6bda4c50f27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:56:04.048595Z",
     "start_time": "2024-12-28T18:56:02.727612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "model = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ],
   "id": "976d565cbc3f66e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prediction",
   "id": "39c7306cc95ddd97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:45:38.600774Z",
     "start_time": "2024-12-03T23:45:36.883902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torchvision import transforms\n",
    "import urllib\n",
    "from PIL import Image\n",
    "\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Load the weights and associated transforms\n",
    "input_image = Image.open(filename)\n",
    "# input_image.show()\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(232),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "preprocess = MobileNet_V2_Weights.IMAGENET1K_V1.transforms()\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch.to(DEVICE))\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# print(probabilities)"
   ],
   "id": "9aa9540004879df2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-7.4507e-01, -1.9598e+00, -1.2297e+00, -2.4998e+00,  6.1957e-01,\n",
      "        -1.5875e+00, -6.6037e-01,  8.7215e-01,  8.0851e-01, -4.4422e+00,\n",
      "         2.1560e+00,  2.2114e+00,  2.2380e+00,  2.4647e+00,  8.6744e-01,\n",
      "         2.8453e+00,  2.2687e+00,  3.6068e+00,  3.5936e+00, -6.2327e-01,\n",
      "        -1.0283e-01, -1.1037e+00, -8.8632e-01, -1.6670e+00, -1.5433e-01,\n",
      "        -2.1496e+00, -2.7370e+00, -2.1549e+00, -2.4847e+00, -1.6090e+00,\n",
      "        -1.7374e+00, -3.4063e+00, -2.0511e+00, -6.3207e-01, -5.3296e-01,\n",
      "         2.0916e+00,  1.6979e+00,  1.8955e+00, -2.8600e+00,  2.1813e+00,\n",
      "         5.0884e-01,  7.7504e-01,  1.1569e+00,  2.4549e+00,  1.9504e-01,\n",
      "         1.1840e+00,  3.3721e+00, -2.3278e+00, -8.8066e-02, -2.3595e+00,\n",
      "         4.1852e-01, -2.5609e+00,  5.4258e-01, -5.0072e-01, -2.3071e-01,\n",
      "        -5.4639e-01, -1.9046e+00, -1.0521e+00,  4.8525e-01, -3.4383e-01,\n",
      "        -1.1849e+00, -2.1489e+00, -2.6166e+00, -2.4700e+00, -4.4042e-01,\n",
      "        -2.4792e+00, -2.6473e+00, -1.2092e+00, -2.6480e+00, -6.7518e-02,\n",
      "        -3.8349e-01,  1.1367e+00,  1.1853e+00,  2.1902e+00,  9.3228e-01,\n",
      "         9.7322e-01,  1.2778e+00,  1.9096e+00,  5.2688e-01,  8.5522e-01,\n",
      "        -1.1398e+00,  1.7283e+00, -7.9086e-01, -1.7384e+00,  1.3265e+00,\n",
      "        -5.4155e-01, -7.1631e-01,  1.3128e+00,  8.9562e-01,  2.9265e+00,\n",
      "        -8.7652e-01,  8.8475e-03,  3.1416e+00, -2.7593e+00,  1.3515e+00,\n",
      "        -1.1707e+00, -1.1697e+00,  4.5468e+00, -4.3255e-01,  2.7225e+00,\n",
      "         1.0316e+00, -4.7577e+00,  6.1203e-01, -6.8390e-01,  2.1508e+00,\n",
      "        -2.0948e-01,  4.5131e-01,  2.2897e+00, -2.4603e+00, -2.2006e+00,\n",
      "        -6.6120e+00, -3.0625e+00,  5.4086e-01, -8.5284e-01, -1.6476e+00,\n",
      "        -5.2114e-01, -2.6044e+00,  2.3503e+00,  3.9485e+00, -9.8360e-01,\n",
      "         3.7058e-02,  1.5033e+00, -1.6138e+00,  6.4320e-01,  3.0259e+00,\n",
      "         3.4232e-01, -1.1736e+00,  2.6639e+00,  1.5336e+00,  2.4022e-02,\n",
      "        -2.9077e+00,  6.5782e-02,  8.7655e-01,  5.1862e-01, -1.0510e-01,\n",
      "         1.3781e+00,  2.9551e+00,  1.2498e+00,  1.8469e+00,  4.4624e+00,\n",
      "         4.9443e-01,  3.2253e+00,  2.4717e-01,  1.3834e+00, -1.1975e+00,\n",
      "        -2.5071e+00,  4.1653e-01, -3.7696e+00, -3.9921e+00, -5.9491e+00,\n",
      "        -2.0666e+00,  3.2975e+00, -2.4101e+00,  1.1440e+00, -1.4587e+00,\n",
      "         7.4567e-01, -1.3020e+00, -1.4325e+00,  2.6175e+00,  2.0892e+00,\n",
      "        -2.3463e+00, -6.5934e-01,  3.8248e-01,  3.1898e-01, -4.1172e-01,\n",
      "         2.4031e+00, -3.7306e-02, -1.4144e+00,  5.4864e+00, -2.1437e+00,\n",
      "        -3.4092e-01,  5.3679e+00,  1.4676e+00,  3.8416e+00,  1.4328e+00,\n",
      "        -3.7143e+00, -3.6080e-02, -3.0481e+00,  3.2713e+00,  1.8404e+00,\n",
      "         3.7763e+00, -1.8322e+00,  2.0658e+00,  6.2141e-02,  3.6362e+00,\n",
      "        -2.4179e-02,  1.3395e-02, -3.9641e-01, -1.1142e+00, -1.7499e-01,\n",
      "        -5.6683e-01,  7.7809e-01,  1.3777e+00,  1.4916e+00, -4.3963e+00,\n",
      "         3.8113e+00,  2.5365e+00,  2.4681e+00,  2.5955e+00,  2.4169e+00,\n",
      "        -2.0628e-01, -3.2078e-02,  2.7553e+00,  2.3648e+00,  3.7929e-01,\n",
      "         2.8319e+00, -4.5739e-01,  1.2812e+00,  2.9984e+00,  2.9985e+00,\n",
      "         1.0214e+00,  1.8657e+00,  1.0418e-01,  6.4815e-01,  2.1788e-02,\n",
      "         5.8420e-01, -7.4180e-02,  1.9753e+00,  3.6850e-01, -1.7186e-01,\n",
      "        -1.7179e+00,  6.7790e-01, -9.9640e-01,  2.2471e+00, -1.2164e+00,\n",
      "         3.7620e+00, -6.5169e-01,  2.7456e+00, -1.0653e+00, -2.0844e+00,\n",
      "        -3.9197e-01, -9.3607e-01, -1.2266e+00, -2.4052e+00,  2.3541e+00,\n",
      "         1.2261e-01,  4.6956e+00,  3.7823e+00,  2.5035e+00,  5.1117e-02,\n",
      "         1.5223e+00,  9.3174e-01,  5.6696e+00,  2.2692e+00, -4.3323e+00,\n",
      "         5.0403e+00,  3.2058e+00, -2.6847e+00,  9.4572e-01,  2.4257e-01,\n",
      "         1.9851e+00,  4.1876e+00, -4.6474e-01,  3.1332e+00,  3.8295e+00,\n",
      "        -2.4853e+00, -1.8564e+00, -7.9047e-01, -4.5700e-01, -7.4618e-01,\n",
      "        -2.3996e+00, -3.2545e+00, -2.6375e-01,  7.5691e-01,  6.3860e-01,\n",
      "         1.5944e-01,  2.0392e+00,  1.5432e-01,  1.1580e+00, -2.2356e+00,\n",
      "        -1.6561e+00, -7.8775e-01,  2.1020e+00,  1.3766e+00,  1.0854e-01,\n",
      "        -2.5515e+00, -5.6272e-01, -6.7129e-01, -1.6876e+00, -1.8045e+00,\n",
      "         1.2012e+00,  3.1642e+00,  1.5683e+00, -1.9208e+00,  4.5340e+00,\n",
      "         4.2345e+00,  2.8246e+00,  3.5226e+00,  2.0402e-02, -2.1257e+00,\n",
      "        -6.9500e-01, -3.9610e+00, -3.3866e-01, -1.6239e+00, -4.2402e+00,\n",
      "        -2.0107e+00, -4.6497e+00, -7.4912e+00,  8.0085e-01,  9.7493e-01,\n",
      "        -1.4008e+00,  5.2200e-01,  8.0550e-01,  2.6637e-01, -3.5965e+00,\n",
      "         1.9676e-01,  1.7929e+00, -1.6987e+00, -1.7408e+00, -1.1199e+00,\n",
      "         4.7522e-01,  5.1330e-01,  3.4599e+00,  2.5140e+00,  6.9330e-01,\n",
      "         1.5659e+00,  3.3683e-01,  7.2567e-01, -1.8599e+00, -1.1070e+00,\n",
      "        -1.0128e+00,  2.4431e+00,  9.7556e-01,  2.0896e+00,  3.6221e+00,\n",
      "         6.8364e-01, -1.7709e-01,  1.7518e+00,  7.3430e-01, -3.0940e+00,\n",
      "         2.4895e+00,  1.6022e+00, -8.4433e-01, -6.8280e-01,  6.0421e-02,\n",
      "         6.4854e-01, -3.6465e+00,  2.3190e-02,  1.7633e-01, -3.2577e+00,\n",
      "        -4.4320e+00, -1.8285e+00, -2.6264e+00, -2.4961e+00, -3.1581e+00,\n",
      "        -4.3406e+00, -3.8197e+00, -4.2005e+00, -3.2824e+00, -2.2070e+00,\n",
      "        -3.5464e+00, -5.3571e+00, -1.6030e-01, -3.2278e-01, -5.0099e+00,\n",
      "        -1.1198e+00,  3.1959e+00,  3.2468e+00,  3.4029e-01,  1.9826e+00,\n",
      "        -2.3311e-01, -1.1011e+00, -1.8208e+00, -3.5713e-01, -2.0420e+00,\n",
      "        -6.6426e-01, -2.3067e+00, -2.8465e+00, -4.0740e+00, -2.4752e+00,\n",
      "        -1.7404e+00, -2.2100e+00, -1.9968e+00, -1.1393e+00, -3.8860e+00,\n",
      "        -3.5660e+00, -4.8099e+00, -2.2390e+00, -3.7012e+00, -2.8684e+00,\n",
      "        -3.6307e+00,  3.5797e-01, -8.2750e-01, -1.9880e+00, -6.7716e+00,\n",
      "        -4.7109e+00, -5.9656e+00, -3.1650e+00, -2.7203e+00, -6.1143e-01,\n",
      "        -7.8315e-01,  1.2445e+00, -4.2446e+00, -5.0524e+00, -6.0708e-01,\n",
      "         6.0794e-01, -6.8402e-01, -2.2876e-01, -1.0286e+00, -9.6447e-01,\n",
      "        -9.3735e-01, -1.5723e+00, -5.5408e-01, -3.6672e+00, -2.9179e+00,\n",
      "        -4.2328e+00, -1.3073e+00,  3.1540e-01, -3.5588e+00,  2.8438e-01,\n",
      "        -2.5533e+00,  2.9124e+00,  6.0220e+00, -1.8227e+00,  5.3900e+00,\n",
      "         5.9017e-01, -3.1893e-01, -1.3682e+00,  4.4002e+00,  2.4203e+00,\n",
      "         1.9230e+00, -2.7153e+00,  9.0006e-03,  3.3364e-01, -1.6839e+00,\n",
      "        -3.3164e+00, -1.4542e+00,  1.3429e+00,  2.0095e+00,  1.8532e+00,\n",
      "         2.9533e+00,  7.4092e-01,  1.8720e+00,  2.4081e+00,  2.9476e+00,\n",
      "        -3.9684e-01,  1.0501e+00, -2.3768e+00,  2.3380e+00, -1.8491e+00,\n",
      "         4.8444e-01,  5.7911e-01, -2.5091e+00,  3.3111e+00,  3.0921e-01,\n",
      "         2.3705e+00,  5.1972e+00, -2.1693e+00,  6.5812e-01, -2.0741e+00,\n",
      "        -4.6996e+00,  5.0017e-02,  1.8726e+00,  2.9848e+00,  4.6163e-01,\n",
      "         1.2590e+00,  1.4587e+00,  3.1617e+00, -2.9803e+00,  3.9494e+00,\n",
      "        -9.9979e-01,  2.1634e+00,  5.5253e+00,  3.8561e+00,  6.5238e-01,\n",
      "        -2.1928e+00, -3.1745e+00, -3.5490e+00, -3.1079e-01,  2.5050e-01,\n",
      "         1.9068e+00, -6.2474e+00,  7.7869e-01,  7.9353e-01,  1.3118e-01,\n",
      "         2.6352e-01, -4.2511e+00,  2.3310e+00,  2.5464e+00,  3.0582e-01,\n",
      "         2.3081e+00,  2.6601e+00,  1.1727e+00, -2.4930e+00, -3.0827e-01,\n",
      "         2.6286e+00,  2.7065e-02,  1.6922e+00, -1.5970e+00,  2.3877e-01,\n",
      "         2.2153e-01, -2.3859e+00, -5.7234e-02, -7.2758e-01,  3.3231e-01,\n",
      "         2.3256e+00,  3.0254e+00, -3.1231e+00,  1.0696e+00, -2.2154e+00,\n",
      "        -5.5860e+00,  3.8608e-01,  8.6353e-01,  3.4867e-01, -1.2882e+00,\n",
      "         1.1520e+00, -5.1637e-01,  6.3400e-01,  2.6679e+00, -2.3528e+00,\n",
      "        -3.2631e+00, -1.0711e+00,  4.7336e-01,  8.4969e-01,  2.6830e+00,\n",
      "        -7.2500e-02,  1.1533e+00, -3.8255e+00,  3.5809e+00, -2.5494e+00,\n",
      "        -2.2450e+00,  8.3829e-01,  1.2548e+00,  1.5293e+00, -7.6059e-02,\n",
      "        -4.8005e+00,  2.0590e+00,  3.2457e+00, -8.9358e-01,  4.5389e+00,\n",
      "        -5.8746e-02, -4.0273e-01,  9.4177e-01,  3.6331e+00,  2.9058e+00,\n",
      "        -2.8251e-01, -1.5190e+00, -2.9701e-01, -2.2559e+00,  2.7653e+00,\n",
      "        -1.9281e+00,  3.1560e+00,  3.4286e+00,  1.6264e+00, -7.9492e-01,\n",
      "         4.1127e+00,  7.0766e-02, -4.9902e+00,  7.9490e-01,  2.6325e+00,\n",
      "        -1.0162e+00,  1.2206e+00,  1.0981e+00,  5.5901e+00, -2.3624e+00,\n",
      "        -2.5248e+00, -5.1482e-01, -6.7361e-01, -1.0973e+00,  4.7287e+00,\n",
      "         2.9890e+00,  6.5152e-01, -5.6010e-01,  3.1625e+00, -1.8699e+00,\n",
      "        -1.1085e+00,  3.2977e+00,  3.2699e+00, -2.1137e+00, -1.9825e+00,\n",
      "         1.6591e+00, -1.6287e+00,  2.0629e+00,  4.6197e-01,  5.5841e+00,\n",
      "        -4.3048e-01, -3.7537e+00,  6.0091e-01,  3.1793e+00, -6.2046e-01,\n",
      "        -4.3399e+00, -2.8156e-01, -2.9689e+00, -3.9145e+00, -1.8601e+00,\n",
      "         3.2580e+00, -4.3617e+00,  1.2119e+00,  3.0967e+00,  3.3609e+00,\n",
      "         8.8502e-02,  2.8725e+00,  2.9980e+00, -1.7568e+00, -2.1726e+00,\n",
      "        -3.1673e+00, -8.8245e-01, -1.7617e+00, -4.4377e-01, -6.6550e-01,\n",
      "         3.9264e-01,  4.5645e+00, -2.2608e-01, -3.5537e+00,  9.6451e-01,\n",
      "         2.7258e+00,  1.3042e+00, -1.7517e+00,  1.3768e+00, -3.8767e+00,\n",
      "         3.0711e+00,  1.8842e+00, -2.2437e-01,  1.9665e+00,  2.1673e+00,\n",
      "         4.3485e+00,  1.2590e+00,  2.0011e+00, -8.4141e-01,  4.7177e+00,\n",
      "         4.9564e+00, -1.0485e+00,  1.5802e+00,  6.5081e-01,  2.1485e+00,\n",
      "        -2.1131e+00,  6.3708e-01, -1.8428e+00, -4.5930e+00, -3.3596e-01,\n",
      "         2.6481e+00,  3.8350e-01,  4.2782e+00,  6.1871e-01, -7.8626e+00,\n",
      "        -2.0448e+00,  2.9608e+00, -7.7528e-01,  3.1892e+00,  3.1379e+00,\n",
      "        -2.3267e+00, -5.1417e-01, -6.3074e-01,  2.4339e+00,  1.5268e+00,\n",
      "         1.7612e-01, -1.7753e+00,  1.2980e+00, -1.5127e+00, -4.2932e+00,\n",
      "         1.9094e-01,  6.7660e-01, -2.9989e+00, -7.7418e-01,  1.5309e+00,\n",
      "         6.1256e+00,  9.1134e-01, -2.0755e+00, -5.1329e-01,  1.5428e+00,\n",
      "        -2.4115e+00, -2.6312e+00,  4.4461e-03, -3.9346e+00,  4.4962e+00,\n",
      "         1.2498e+00, -3.6092e+00,  1.6616e+00, -5.0883e+00, -5.1757e-02,\n",
      "         1.9464e+00, -2.8762e+00, -8.4337e-01,  4.1224e+00,  3.8408e+00,\n",
      "        -3.7520e+00,  1.4912e+00,  6.6071e-01,  2.0694e+00,  1.1781e+00,\n",
      "         2.1383e+00,  3.4192e+00, -1.7836e+00,  2.4299e+00,  5.1716e-01,\n",
      "        -9.5399e-01,  2.7160e+00, -3.7699e+00,  2.3740e+00,  1.0268e+00,\n",
      "        -4.1376e+00,  2.1763e-01,  2.3881e+00,  3.0084e+00, -2.9274e+00,\n",
      "        -4.7918e+00,  2.2403e+00,  3.2653e+00, -2.8808e+00,  1.7775e+00,\n",
      "         5.1015e+00,  9.2415e-01, -1.1610e+00, -1.9952e+00, -3.0750e-01,\n",
      "        -1.1630e+00, -3.3008e+00, -3.0141e+00,  2.3141e+00,  4.0102e+00,\n",
      "         2.7734e+00,  7.3795e-01, -3.0136e-01,  2.9642e+00,  1.1066e+00,\n",
      "        -3.7218e+00, -2.0225e+00, -2.1453e+00,  1.0361e+00,  1.4708e+00,\n",
      "        -1.3035e-01,  5.3399e+00,  3.0418e+00,  2.8463e+00, -2.8961e+00,\n",
      "         7.7010e-01, -6.3793e+00, -1.1839e+00,  5.1797e+00, -1.0018e+00,\n",
      "        -2.6375e+00,  2.7157e+00,  3.9272e-01,  1.4761e+00, -4.0665e-01,\n",
      "         1.9561e+00, -1.0059e+00, -3.2382e-01, -3.9703e-01, -4.5634e+00,\n",
      "         2.5064e-02, -1.1472e+00,  4.7683e+00, -1.8917e+00, -1.0805e+00,\n",
      "         1.2167e-01,  6.3509e-01,  1.2855e+00,  3.0580e+00,  2.6653e+00,\n",
      "        -2.2585e-01, -3.5419e-01,  1.3319e+00,  3.6179e-01,  2.4074e+00,\n",
      "        -2.7049e+00, -7.3847e-01, -2.2817e-01,  1.1931e+00,  6.4054e-01,\n",
      "        -1.1722e+00,  3.1843e+00,  9.0086e-01, -4.6470e-01, -2.7329e+00,\n",
      "         3.8144e+00, -2.5740e+00,  2.0236e+00,  3.2914e+00,  2.0970e+00,\n",
      "         3.9182e+00,  2.0992e+00,  1.7005e+00,  1.4993e+00,  2.9728e+00,\n",
      "         3.6241e+00, -1.3534e-01, -8.8893e-01, -2.9348e-01, -1.8746e+00,\n",
      "        -1.9050e+00, -4.5662e-01,  4.6263e+00,  6.5617e-03,  2.1695e+00,\n",
      "         3.3838e+00,  4.2678e+00,  1.8525e+00,  1.9971e+00,  4.8185e+00,\n",
      "         1.2438e+00,  4.4648e+00,  2.4039e+00,  4.6964e+00, -5.3912e-01,\n",
      "        -7.8779e-01,  1.0197e+00,  5.0956e+00, -1.0656e+00,  4.2103e-01,\n",
      "        -5.2537e-02, -8.0654e-01, -1.4047e+00, -3.3241e+00,  3.5481e+00,\n",
      "         3.2643e+00,  3.4075e+00, -1.4261e+00,  5.9791e-01, -8.4712e-01,\n",
      "        -8.8500e-01,  4.8627e+00,  4.8591e-01,  9.2631e-01, -1.2840e+00,\n",
      "         2.7684e+00, -1.9009e+00, -2.1181e-01,  2.3805e+00, -1.8782e+00,\n",
      "        -7.9791e+00, -3.2758e+00,  2.5269e+00,  3.8789e+00,  3.0394e+00,\n",
      "        -4.5645e+00, -4.8072e-01,  2.6137e+00, -3.9572e-01, -2.4536e+00,\n",
      "         2.5907e+00, -4.1102e-01, -4.0644e+00, -4.3483e+00,  9.4652e-01,\n",
      "        -3.2781e+00,  1.3887e-01, -3.8773e-01,  1.4676e+00, -5.2236e+00,\n",
      "         4.1986e+00,  6.6198e-02,  4.7357e+00, -2.2211e+00,  6.5403e-01,\n",
      "         2.2399e+00,  2.8482e+00, -6.1121e+00,  2.7325e+00, -1.7159e+00,\n",
      "        -2.0487e+00,  5.7020e+00,  5.5591e+00,  2.7804e-01,  1.1001e-01,\n",
      "        -7.7339e-01, -4.1650e+00, -2.5444e+00, -4.4335e+00,  5.0893e-01,\n",
      "        -2.3857e+00,  4.7608e+00,  1.0281e+00, -3.5112e+00, -4.5371e+00,\n",
      "        -3.5520e+00, -3.4144e+00, -1.6546e+00,  1.5048e+00,  8.6838e-02,\n",
      "         1.5760e+00, -8.0901e-01, -2.1045e-01, -2.6901e+00, -1.7177e+00,\n",
      "         1.8640e+00,  1.1507e+00, -3.1399e-01, -8.9036e-01,  2.0236e+00,\n",
      "        -3.1493e-01, -1.6139e+00,  3.3430e+00,  7.1771e-01, -3.7081e+00,\n",
      "         2.1999e-01, -1.0723e+00,  1.6412e+00, -3.5063e+00,  1.7659e+00,\n",
      "         1.6695e+00,  2.1909e-01,  1.7532e+00,  5.0305e+00,  1.1266e+00,\n",
      "        -3.2399e+00,  3.3699e+00,  1.5549e+00,  6.4676e+00,  4.9610e+00,\n",
      "        -1.5969e+00, -5.8956e+00, -2.6950e+00,  3.2279e+00,  1.8471e+00,\n",
      "         4.9893e-01,  2.2332e+00, -1.2563e-01, -3.4804e+00,  3.5252e+00,\n",
      "         2.0267e+00,  2.8605e+00, -3.9185e+00, -1.3357e+00, -2.3520e+00,\n",
      "        -4.9034e+00, -3.0980e+00, -1.3895e+00, -4.9867e-01, -1.8208e+00,\n",
      "        -2.2824e+00, -2.4650e-01, -2.5096e+00, -1.3872e-02, -1.9757e+00,\n",
      "        -2.3843e+00,  3.2317e-01,  1.0886e+00,  1.1363e+00,  1.1452e+00,\n",
      "        -2.1289e+00,  7.1462e-02, -1.2858e+00, -1.7492e+00, -1.5527e+00,\n",
      "        -1.1081e+00,  6.2719e-01, -3.6708e-01, -1.4450e+00,  7.4251e-01,\n",
      "         9.6166e-01, -4.8432e-01, -7.2579e-01,  5.8667e-01, -1.4574e+00,\n",
      "        -3.0114e+00,  1.8997e-03, -3.5580e-01, -1.5509e+00,  8.1214e-01,\n",
      "        -2.1829e+00,  1.4036e-01, -3.2808e+00,  2.5235e+00,  1.1894e-01,\n",
      "        -1.2973e+00, -2.0218e+00, -2.3316e+00,  8.8103e-01, -1.9138e+00,\n",
      "        -1.5128e-02, -1.6660e+00, -5.8577e-01, -4.9201e-03, -2.8509e+00,\n",
      "        -3.2178e+00, -1.5144e+00, -2.0948e+00, -1.2525e-01, -6.6034e-01,\n",
      "        -6.2481e+00,  5.7787e-01, -3.3919e+00, -6.5289e-01, -4.0173e+00,\n",
      "        -5.2907e-01, -3.4276e-01,  3.6658e+00,  9.1423e-01, -4.6686e+00,\n",
      "        -3.8401e+00, -3.7297e+00,  1.6892e+00, -2.1463e+00, -4.0112e-01,\n",
      "         5.9968e-01, -4.0190e+00,  2.7346e+00, -2.6258e+00,  1.5654e-01,\n",
      "        -8.3719e-01, -1.2146e+00,  4.1526e-01, -3.0071e+00,  5.0407e-02,\n",
      "         5.6579e-01, -2.1142e+00,  3.6105e-01,  4.3543e+00,  2.1181e+00],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:41:14.689426Z",
     "start_time": "2024-11-27T22:41:14.680454Z"
    }
   },
   "cell_type": "code",
   "source": "output[0].max()",
   "id": "99e21448b54e5f49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3573, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T23:32:32.979718Z",
     "start_time": "2024-11-25T23:32:32.974222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the categories\n",
    "with open(\"./imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ],
   "id": "7d4a463b77f3fe59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.8303830623626709\n",
      "Pomeranian 0.06986550986766815\n",
      "keeshond 0.012912546284496784\n",
      "collie 0.01080403383821249\n",
      "Great Pyrenees 0.009873803704977036\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Model",
   "id": "4656ad2ef44ee21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:45:43.464678Z",
     "start_time": "2024-12-03T23:45:42.929232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "\n",
    "# test_transforms = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = datasets.ImageNet(root='./data',\n",
    "                                 split='val',\n",
    "                                transform=preprocess)"
   ],
   "id": "339450c93917958f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T23:13:51.916260Z",
     "start_time": "2024-11-27T23:13:51.910370Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset",
   "id": "711e7ee05a652902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNet\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: val\n",
       "    StandardTransform\n",
       "Transform: ImageClassification(\n",
       "               crop_size=[224]\n",
       "               resize_size=[256]\n",
       "               mean=[0.485, 0.456, 0.406]\n",
       "               std=[0.229, 0.224, 0.225]\n",
       "               interpolation=InterpolationMode.BILINEAR\n",
       "           )"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:45:46.581481Z",
     "start_time": "2024-12-03T23:45:46.569151Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)",
   "id": "347c6180cff3987f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:51:45.771865Z",
     "start_time": "2024-12-03T23:51:45.761005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = 0.0\n",
    "    total_predictions = 0\n",
    "    true_predictions = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) / inputs.size(0)\n",
    "            losses += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            batch_total_predictions = outputs.size(0)\n",
    "            batch_true_predictions = (predicted == targets).sum().item()\n",
    "            total_predictions += batch_total_predictions\n",
    "            true_predictions += batch_true_predictions\n",
    "\n",
    "            print(f'Batch {batch_idx}, Loss: {loss:.4f}, Accuracy: {batch_true_predictions/batch_total_predictions*100:.2f}%')\n",
    "            \n",
    "    return true_predictions / total_predictions, losses"
   ],
   "id": "a1240ffa296a7208",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T01:32:10.022546Z",
     "start_time": "2024-12-04T01:21:16.316444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy_top1, accuracy_top5, losses = evaluate(model, DEVICE, test_dataloader)\n",
    "print(f\"acc@1: {accuracy_top1*100}%, acc@5: {accuracy_top5*100}%, loss: {losses}\")\n"
   ],
   "id": "373a93fb139ea1d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.0097, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 1, Loss: 0.0076, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 2, Loss: 0.0102, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 3, Loss: 0.0096, Accuracy@1: 79.69%, Accuracy@5: 98.44%\n",
      "Batch 4, Loss: 0.0164, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 5, Loss: 0.0160, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 6, Loss: 0.0092, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 7, Loss: 0.0036, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 8, Loss: 0.0048, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 9, Loss: 0.0060, Accuracy@1: 90.62%, Accuracy@5: 95.31%\n",
      "Batch 10, Loss: 0.0023, Accuracy@1: 95.31%, Accuracy@5: 98.44%\n",
      "Batch 11, Loss: 0.0058, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 12, Loss: 0.0084, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 13, Loss: 0.0086, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 14, Loss: 0.0116, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 15, Loss: 0.0043, Accuracy@1: 95.31%, Accuracy@5: 96.88%\n",
      "Batch 16, Loss: 0.0133, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 17, Loss: 0.0061, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 18, Loss: 0.0058, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 19, Loss: 0.0028, Accuracy@1: 96.88%, Accuracy@5: 98.44%\n",
      "Batch 20, Loss: 0.0115, Accuracy@1: 75.00%, Accuracy@5: 100.00%\n",
      "Batch 21, Loss: 0.0214, Accuracy@1: 71.88%, Accuracy@5: 84.38%\n",
      "Batch 22, Loss: 0.0068, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 23, Loss: 0.0124, Accuracy@1: 87.50%, Accuracy@5: 92.19%\n",
      "Batch 24, Loss: 0.0112, Accuracy@1: 81.25%, Accuracy@5: 98.44%\n",
      "Batch 25, Loss: 0.0317, Accuracy@1: 50.00%, Accuracy@5: 82.81%\n",
      "Batch 26, Loss: 0.0171, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 27, Loss: 0.0226, Accuracy@1: 57.81%, Accuracy@5: 85.94%\n",
      "Batch 28, Loss: 0.0190, Accuracy@1: 62.50%, Accuracy@5: 95.31%\n",
      "Batch 29, Loss: 0.0112, Accuracy@1: 78.12%, Accuracy@5: 96.88%\n",
      "Batch 30, Loss: 0.0128, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 31, Loss: 0.0147, Accuracy@1: 64.06%, Accuracy@5: 100.00%\n",
      "Batch 32, Loss: 0.0103, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 33, Loss: 0.0136, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 34, Loss: 0.0206, Accuracy@1: 75.00%, Accuracy@5: 82.81%\n",
      "Batch 35, Loss: 0.0217, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 36, Loss: 0.0205, Accuracy@1: 56.25%, Accuracy@5: 92.19%\n",
      "Batch 37, Loss: 0.0166, Accuracy@1: 79.69%, Accuracy@5: 87.50%\n",
      "Batch 38, Loss: 0.0196, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 39, Loss: 0.0100, Accuracy@1: 78.12%, Accuracy@5: 96.88%\n",
      "Batch 40, Loss: 0.0063, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 41, Loss: 0.0168, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 42, Loss: 0.0282, Accuracy@1: 60.94%, Accuracy@5: 87.50%\n",
      "Batch 43, Loss: 0.0187, Accuracy@1: 62.50%, Accuracy@5: 93.75%\n",
      "Batch 44, Loss: 0.0094, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 45, Loss: 0.0136, Accuracy@1: 73.44%, Accuracy@5: 93.75%\n",
      "Batch 46, Loss: 0.0337, Accuracy@1: 57.81%, Accuracy@5: 82.81%\n",
      "Batch 47, Loss: 0.0296, Accuracy@1: 51.56%, Accuracy@5: 87.50%\n",
      "Batch 48, Loss: 0.0201, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 49, Loss: 0.0164, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 50, Loss: 0.0248, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 51, Loss: 0.0246, Accuracy@1: 60.94%, Accuracy@5: 81.25%\n",
      "Batch 52, Loss: 0.0211, Accuracy@1: 56.25%, Accuracy@5: 93.75%\n",
      "Batch 53, Loss: 0.0251, Accuracy@1: 48.44%, Accuracy@5: 85.94%\n",
      "Batch 54, Loss: 0.0057, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 55, Loss: 0.0096, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 56, Loss: 0.0107, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 57, Loss: 0.0243, Accuracy@1: 43.75%, Accuracy@5: 95.31%\n",
      "Batch 58, Loss: 0.0103, Accuracy@1: 75.00%, Accuracy@5: 96.88%\n",
      "Batch 59, Loss: 0.0120, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 60, Loss: 0.0143, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 61, Loss: 0.0267, Accuracy@1: 73.44%, Accuracy@5: 85.94%\n",
      "Batch 62, Loss: 0.0138, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 63, Loss: 0.0103, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 64, Loss: 0.0073, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 65, Loss: 0.0051, Accuracy@1: 93.75%, Accuracy@5: 95.31%\n",
      "Batch 66, Loss: 0.0122, Accuracy@1: 85.94%, Accuracy@5: 92.19%\n",
      "Batch 67, Loss: 0.0102, Accuracy@1: 81.25%, Accuracy@5: 100.00%\n",
      "Batch 68, Loss: 0.0077, Accuracy@1: 92.19%, Accuracy@5: 95.31%\n",
      "Batch 69, Loss: 0.0036, Accuracy@1: 96.88%, Accuracy@5: 100.00%\n",
      "Batch 70, Loss: 0.0041, Accuracy@1: 95.31%, Accuracy@5: 98.44%\n",
      "Batch 71, Loss: 0.0091, Accuracy@1: 90.62%, Accuracy@5: 92.19%\n",
      "Batch 72, Loss: 0.0049, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 73, Loss: 0.0057, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 74, Loss: 0.0069, Accuracy@1: 92.19%, Accuracy@5: 93.75%\n",
      "Batch 75, Loss: 0.0089, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 76, Loss: 0.0092, Accuracy@1: 85.94%, Accuracy@5: 92.19%\n",
      "Batch 77, Loss: 0.0129, Accuracy@1: 71.88%, Accuracy@5: 95.31%\n",
      "Batch 78, Loss: 0.0071, Accuracy@1: 90.62%, Accuracy@5: 95.31%\n",
      "Batch 79, Loss: 0.0115, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 80, Loss: 0.0101, Accuracy@1: 87.50%, Accuracy@5: 92.19%\n",
      "Batch 81, Loss: 0.0129, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 82, Loss: 0.0063, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 83, Loss: 0.0209, Accuracy@1: 78.12%, Accuracy@5: 89.06%\n",
      "Batch 84, Loss: 0.0178, Accuracy@1: 68.75%, Accuracy@5: 92.19%\n",
      "Batch 85, Loss: 0.0081, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 86, Loss: 0.0142, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 87, Loss: 0.0099, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 88, Loss: 0.0148, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 89, Loss: 0.0165, Accuracy@1: 76.56%, Accuracy@5: 87.50%\n",
      "Batch 90, Loss: 0.0088, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 91, Loss: 0.0141, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 92, Loss: 0.0136, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 93, Loss: 0.0188, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 94, Loss: 0.0160, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 95, Loss: 0.0084, Accuracy@1: 82.81%, Accuracy@5: 98.44%\n",
      "Batch 96, Loss: 0.0148, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 97, Loss: 0.0243, Accuracy@1: 56.25%, Accuracy@5: 96.88%\n",
      "Batch 98, Loss: 0.0237, Accuracy@1: 73.44%, Accuracy@5: 85.94%\n",
      "Batch 99, Loss: 0.0145, Accuracy@1: 82.81%, Accuracy@5: 89.06%\n",
      "Batch 100, Loss: 0.0107, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 101, Loss: 0.0029, Accuracy@1: 95.31%, Accuracy@5: 96.88%\n",
      "Batch 102, Loss: 0.0059, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 103, Loss: 0.0067, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 104, Loss: 0.0099, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 105, Loss: 0.0090, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 106, Loss: 0.0049, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 107, Loss: 0.0054, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 108, Loss: 0.0035, Accuracy@1: 93.75%, Accuracy@5: 95.31%\n",
      "Batch 109, Loss: 0.0077, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 110, Loss: 0.0076, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 111, Loss: 0.0077, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 112, Loss: 0.0040, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 113, Loss: 0.0043, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 114, Loss: 0.0045, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 115, Loss: 0.0075, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 116, Loss: 0.0089, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 117, Loss: 0.0095, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 118, Loss: 0.0231, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 119, Loss: 0.0111, Accuracy@1: 71.88%, Accuracy@5: 100.00%\n",
      "Batch 120, Loss: 0.0165, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 121, Loss: 0.0133, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 122, Loss: 0.0063, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 123, Loss: 0.0144, Accuracy@1: 75.00%, Accuracy@5: 95.31%\n",
      "Batch 124, Loss: 0.0150, Accuracy@1: 70.31%, Accuracy@5: 96.88%\n",
      "Batch 125, Loss: 0.0084, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 126, Loss: 0.0108, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 127, Loss: 0.0314, Accuracy@1: 57.81%, Accuracy@5: 84.38%\n",
      "Batch 128, Loss: 0.0130, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 129, Loss: 0.0294, Accuracy@1: 57.81%, Accuracy@5: 84.38%\n",
      "Batch 130, Loss: 0.0278, Accuracy@1: 43.75%, Accuracy@5: 85.94%\n",
      "Batch 131, Loss: 0.0232, Accuracy@1: 54.69%, Accuracy@5: 93.75%\n",
      "Batch 132, Loss: 0.0086, Accuracy@1: 81.25%, Accuracy@5: 98.44%\n",
      "Batch 133, Loss: 0.0307, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 134, Loss: 0.0094, Accuracy@1: 78.12%, Accuracy@5: 96.88%\n",
      "Batch 135, Loss: 0.0211, Accuracy@1: 59.38%, Accuracy@5: 90.62%\n",
      "Batch 136, Loss: 0.0106, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 137, Loss: 0.0225, Accuracy@1: 64.06%, Accuracy@5: 92.19%\n",
      "Batch 138, Loss: 0.0100, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 139, Loss: 0.0070, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 140, Loss: 0.0196, Accuracy@1: 62.50%, Accuracy@5: 95.31%\n",
      "Batch 141, Loss: 0.0115, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 142, Loss: 0.0129, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 143, Loss: 0.0130, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 144, Loss: 0.0176, Accuracy@1: 71.88%, Accuracy@5: 92.19%\n",
      "Batch 145, Loss: 0.0157, Accuracy@1: 60.94%, Accuracy@5: 100.00%\n",
      "Batch 146, Loss: 0.0258, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 147, Loss: 0.0161, Accuracy@1: 68.75%, Accuracy@5: 96.88%\n",
      "Batch 148, Loss: 0.0148, Accuracy@1: 79.69%, Accuracy@5: 87.50%\n",
      "Batch 149, Loss: 0.0146, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 150, Loss: 0.0167, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 151, Loss: 0.0218, Accuracy@1: 67.19%, Accuracy@5: 81.25%\n",
      "Batch 152, Loss: 0.0074, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 153, Loss: 0.0193, Accuracy@1: 71.88%, Accuracy@5: 92.19%\n",
      "Batch 154, Loss: 0.0101, Accuracy@1: 78.12%, Accuracy@5: 98.44%\n",
      "Batch 155, Loss: 0.0153, Accuracy@1: 71.88%, Accuracy@5: 96.88%\n",
      "Batch 156, Loss: 0.0143, Accuracy@1: 71.88%, Accuracy@5: 98.44%\n",
      "Batch 157, Loss: 0.0143, Accuracy@1: 73.44%, Accuracy@5: 93.75%\n",
      "Batch 158, Loss: 0.0123, Accuracy@1: 73.44%, Accuracy@5: 98.44%\n",
      "Batch 159, Loss: 0.0124, Accuracy@1: 75.00%, Accuracy@5: 96.88%\n",
      "Batch 160, Loss: 0.0149, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 161, Loss: 0.0119, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 162, Loss: 0.0095, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 163, Loss: 0.0127, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 164, Loss: 0.0129, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 165, Loss: 0.0154, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 166, Loss: 0.0162, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 167, Loss: 0.0122, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 168, Loss: 0.0094, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 169, Loss: 0.0047, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 170, Loss: 0.0046, Accuracy@1: 93.75%, Accuracy@5: 100.00%\n",
      "Batch 171, Loss: 0.0131, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 172, Loss: 0.0212, Accuracy@1: 75.00%, Accuracy@5: 84.38%\n",
      "Batch 173, Loss: 0.0188, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 174, Loss: 0.0115, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 175, Loss: 0.0140, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 176, Loss: 0.0175, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 177, Loss: 0.0276, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 178, Loss: 0.0065, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 179, Loss: 0.0139, Accuracy@1: 78.12%, Accuracy@5: 95.31%\n",
      "Batch 180, Loss: 0.0166, Accuracy@1: 68.75%, Accuracy@5: 92.19%\n",
      "Batch 181, Loss: 0.0142, Accuracy@1: 67.19%, Accuracy@5: 95.31%\n",
      "Batch 182, Loss: 0.0155, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 183, Loss: 0.0112, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 184, Loss: 0.0140, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 185, Loss: 0.0196, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 186, Loss: 0.0182, Accuracy@1: 65.62%, Accuracy@5: 93.75%\n",
      "Batch 187, Loss: 0.0153, Accuracy@1: 57.81%, Accuracy@5: 100.00%\n",
      "Batch 188, Loss: 0.0154, Accuracy@1: 56.25%, Accuracy@5: 96.88%\n",
      "Batch 189, Loss: 0.0141, Accuracy@1: 78.12%, Accuracy@5: 95.31%\n",
      "Batch 190, Loss: 0.0088, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 191, Loss: 0.0083, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 192, Loss: 0.0217, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 193, Loss: 0.0095, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 194, Loss: 0.0160, Accuracy@1: 56.25%, Accuracy@5: 98.44%\n",
      "Batch 195, Loss: 0.0198, Accuracy@1: 57.81%, Accuracy@5: 95.31%\n",
      "Batch 196, Loss: 0.0080, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 197, Loss: 0.0088, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 198, Loss: 0.0049, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 199, Loss: 0.0038, Accuracy@1: 90.62%, Accuracy@5: 100.00%\n",
      "Batch 200, Loss: 0.0155, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 201, Loss: 0.0091, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 202, Loss: 0.0033, Accuracy@1: 93.75%, Accuracy@5: 100.00%\n",
      "Batch 203, Loss: 0.0089, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 204, Loss: 0.0077, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 205, Loss: 0.0086, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 206, Loss: 0.0086, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 207, Loss: 0.0226, Accuracy@1: 54.69%, Accuracy@5: 90.62%\n",
      "Batch 208, Loss: 0.0184, Accuracy@1: 60.94%, Accuracy@5: 92.19%\n",
      "Batch 209, Loss: 0.0059, Accuracy@1: 93.75%, Accuracy@5: 100.00%\n",
      "Batch 210, Loss: 0.0137, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 211, Loss: 0.0153, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 212, Loss: 0.0296, Accuracy@1: 57.81%, Accuracy@5: 82.81%\n",
      "Batch 213, Loss: 0.0216, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 214, Loss: 0.0094, Accuracy@1: 90.62%, Accuracy@5: 95.31%\n",
      "Batch 215, Loss: 0.0025, Accuracy@1: 96.88%, Accuracy@5: 100.00%\n",
      "Batch 216, Loss: 0.0156, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 217, Loss: 0.0169, Accuracy@1: 68.75%, Accuracy@5: 93.75%\n",
      "Batch 218, Loss: 0.0078, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 219, Loss: 0.0180, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 220, Loss: 0.0308, Accuracy@1: 31.25%, Accuracy@5: 89.06%\n",
      "Batch 221, Loss: 0.0079, Accuracy@1: 79.69%, Accuracy@5: 98.44%\n",
      "Batch 222, Loss: 0.0163, Accuracy@1: 78.12%, Accuracy@5: 87.50%\n",
      "Batch 223, Loss: 0.0191, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 224, Loss: 0.0211, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 225, Loss: 0.0088, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 226, Loss: 0.0128, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 227, Loss: 0.0080, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 228, Loss: 0.0070, Accuracy@1: 89.06%, Accuracy@5: 100.00%\n",
      "Batch 229, Loss: 0.0037, Accuracy@1: 96.88%, Accuracy@5: 100.00%\n",
      "Batch 230, Loss: 0.0056, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 231, Loss: 0.0080, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 232, Loss: 0.0221, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 233, Loss: 0.0159, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 234, Loss: 0.0081, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 235, Loss: 0.0093, Accuracy@1: 84.38%, Accuracy@5: 98.44%\n",
      "Batch 236, Loss: 0.0204, Accuracy@1: 67.19%, Accuracy@5: 100.00%\n",
      "Batch 237, Loss: 0.0208, Accuracy@1: 60.94%, Accuracy@5: 93.75%\n",
      "Batch 238, Loss: 0.0125, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 239, Loss: 0.0069, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 240, Loss: 0.0108, Accuracy@1: 78.12%, Accuracy@5: 96.88%\n",
      "Batch 241, Loss: 0.0150, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 242, Loss: 0.0257, Accuracy@1: 70.31%, Accuracy@5: 87.50%\n",
      "Batch 243, Loss: 0.0194, Accuracy@1: 57.81%, Accuracy@5: 93.75%\n",
      "Batch 244, Loss: 0.0146, Accuracy@1: 70.31%, Accuracy@5: 98.44%\n",
      "Batch 245, Loss: 0.0244, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 246, Loss: 0.0214, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 247, Loss: 0.0082, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 248, Loss: 0.0151, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 249, Loss: 0.0112, Accuracy@1: 78.12%, Accuracy@5: 96.88%\n",
      "Batch 250, Loss: 0.0073, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 251, Loss: 0.0017, Accuracy@1: 96.88%, Accuracy@5: 100.00%\n",
      "Batch 252, Loss: 0.0075, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 253, Loss: 0.0041, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 254, Loss: 0.0046, Accuracy@1: 90.62%, Accuracy@5: 100.00%\n",
      "Batch 255, Loss: 0.0106, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 256, Loss: 0.0106, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 257, Loss: 0.0183, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 258, Loss: 0.0132, Accuracy@1: 73.44%, Accuracy@5: 98.44%\n",
      "Batch 259, Loss: 0.0060, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 260, Loss: 0.0017, Accuracy@1: 96.88%, Accuracy@5: 98.44%\n",
      "Batch 261, Loss: 0.0109, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 262, Loss: 0.0087, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 263, Loss: 0.0178, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 264, Loss: 0.0117, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 265, Loss: 0.0032, Accuracy@1: 95.31%, Accuracy@5: 98.44%\n",
      "Batch 266, Loss: 0.0155, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 267, Loss: 0.0179, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 268, Loss: 0.0060, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 269, Loss: 0.0108, Accuracy@1: 78.12%, Accuracy@5: 98.44%\n",
      "Batch 270, Loss: 0.0151, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 271, Loss: 0.0082, Accuracy@1: 82.81%, Accuracy@5: 98.44%\n",
      "Batch 272, Loss: 0.0136, Accuracy@1: 65.62%, Accuracy@5: 96.88%\n",
      "Batch 273, Loss: 0.0091, Accuracy@1: 75.00%, Accuracy@5: 96.88%\n",
      "Batch 274, Loss: 0.0028, Accuracy@1: 93.75%, Accuracy@5: 100.00%\n",
      "Batch 275, Loss: 0.0132, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 276, Loss: 0.0122, Accuracy@1: 78.12%, Accuracy@5: 95.31%\n",
      "Batch 277, Loss: 0.0069, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 278, Loss: 0.0292, Accuracy@1: 51.56%, Accuracy@5: 95.31%\n",
      "Batch 279, Loss: 0.0222, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 280, Loss: 0.0172, Accuracy@1: 62.50%, Accuracy@5: 98.44%\n",
      "Batch 281, Loss: 0.0145, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 282, Loss: 0.0071, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 283, Loss: 0.0122, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 284, Loss: 0.0050, Accuracy@1: 96.88%, Accuracy@5: 96.88%\n",
      "Batch 285, Loss: 0.0100, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 286, Loss: 0.0149, Accuracy@1: 76.56%, Accuracy@5: 90.62%\n",
      "Batch 287, Loss: 0.0108, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 288, Loss: 0.0182, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 289, Loss: 0.0159, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 290, Loss: 0.0240, Accuracy@1: 73.44%, Accuracy@5: 90.62%\n",
      "Batch 291, Loss: 0.0167, Accuracy@1: 73.44%, Accuracy@5: 85.94%\n",
      "Batch 292, Loss: 0.0239, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 293, Loss: 0.0095, Accuracy@1: 87.50%, Accuracy@5: 90.62%\n",
      "Batch 294, Loss: 0.0061, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 295, Loss: 0.0199, Accuracy@1: 68.75%, Accuracy@5: 92.19%\n",
      "Batch 296, Loss: 0.0113, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 297, Loss: 0.0244, Accuracy@1: 51.56%, Accuracy@5: 90.62%\n",
      "Batch 298, Loss: 0.0281, Accuracy@1: 56.25%, Accuracy@5: 87.50%\n",
      "Batch 299, Loss: 0.0163, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 300, Loss: 0.0108, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 301, Loss: 0.0160, Accuracy@1: 67.19%, Accuracy@5: 95.31%\n",
      "Batch 302, Loss: 0.0153, Accuracy@1: 75.00%, Accuracy@5: 95.31%\n",
      "Batch 303, Loss: 0.0097, Accuracy@1: 90.62%, Accuracy@5: 92.19%\n",
      "Batch 304, Loss: 0.0166, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 305, Loss: 0.0158, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 306, Loss: 0.0064, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 307, Loss: 0.0083, Accuracy@1: 76.56%, Accuracy@5: 100.00%\n",
      "Batch 308, Loss: 0.0141, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 309, Loss: 0.0138, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 310, Loss: 0.0129, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 311, Loss: 0.0211, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 312, Loss: 0.0190, Accuracy@1: 60.94%, Accuracy@5: 90.62%\n",
      "Batch 313, Loss: 0.0203, Accuracy@1: 64.06%, Accuracy@5: 85.94%\n",
      "Batch 314, Loss: 0.0330, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 315, Loss: 0.0081, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 316, Loss: 0.0143, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 317, Loss: 0.0207, Accuracy@1: 59.38%, Accuracy@5: 93.75%\n",
      "Batch 318, Loss: 0.0088, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 319, Loss: 0.0210, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 320, Loss: 0.0099, Accuracy@1: 78.12%, Accuracy@5: 100.00%\n",
      "Batch 321, Loss: 0.0179, Accuracy@1: 70.31%, Accuracy@5: 85.94%\n",
      "Batch 322, Loss: 0.0332, Accuracy@1: 43.75%, Accuracy@5: 82.81%\n",
      "Batch 323, Loss: 0.0380, Accuracy@1: 48.44%, Accuracy@5: 79.69%\n",
      "Batch 324, Loss: 0.0453, Accuracy@1: 35.94%, Accuracy@5: 62.50%\n",
      "Batch 325, Loss: 0.0126, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 326, Loss: 0.0164, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 327, Loss: 0.0227, Accuracy@1: 64.06%, Accuracy@5: 92.19%\n",
      "Batch 328, Loss: 0.0107, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 329, Loss: 0.0202, Accuracy@1: 60.94%, Accuracy@5: 95.31%\n",
      "Batch 330, Loss: 0.0196, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 331, Loss: 0.0235, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 332, Loss: 0.0086, Accuracy@1: 92.19%, Accuracy@5: 96.88%\n",
      "Batch 333, Loss: 0.0174, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 334, Loss: 0.0244, Accuracy@1: 60.94%, Accuracy@5: 87.50%\n",
      "Batch 335, Loss: 0.0137, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 336, Loss: 0.0076, Accuracy@1: 87.50%, Accuracy@5: 100.00%\n",
      "Batch 337, Loss: 0.0164, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 338, Loss: 0.0216, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 339, Loss: 0.0266, Accuracy@1: 57.81%, Accuracy@5: 82.81%\n",
      "Batch 340, Loss: 0.0309, Accuracy@1: 43.75%, Accuracy@5: 89.06%\n",
      "Batch 341, Loss: 0.0220, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 342, Loss: 0.0332, Accuracy@1: 54.69%, Accuracy@5: 75.00%\n",
      "Batch 343, Loss: 0.0145, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 344, Loss: 0.0167, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 345, Loss: 0.0178, Accuracy@1: 67.19%, Accuracy@5: 92.19%\n",
      "Batch 346, Loss: 0.0259, Accuracy@1: 64.06%, Accuracy@5: 82.81%\n",
      "Batch 347, Loss: 0.0198, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 348, Loss: 0.0273, Accuracy@1: 57.81%, Accuracy@5: 84.38%\n",
      "Batch 349, Loss: 0.0277, Accuracy@1: 54.69%, Accuracy@5: 85.94%\n",
      "Batch 350, Loss: 0.0094, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 351, Loss: 0.0118, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 352, Loss: 0.0126, Accuracy@1: 82.81%, Accuracy@5: 87.50%\n",
      "Batch 353, Loss: 0.0217, Accuracy@1: 65.62%, Accuracy@5: 85.94%\n",
      "Batch 354, Loss: 0.0222, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 355, Loss: 0.0266, Accuracy@1: 60.94%, Accuracy@5: 87.50%\n",
      "Batch 356, Loss: 0.0216, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 357, Loss: 0.0174, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 358, Loss: 0.0204, Accuracy@1: 70.31%, Accuracy@5: 87.50%\n",
      "Batch 359, Loss: 0.0269, Accuracy@1: 60.94%, Accuracy@5: 81.25%\n",
      "Batch 360, Loss: 0.0233, Accuracy@1: 53.12%, Accuracy@5: 90.62%\n",
      "Batch 361, Loss: 0.0399, Accuracy@1: 56.25%, Accuracy@5: 76.56%\n",
      "Batch 362, Loss: 0.0355, Accuracy@1: 56.25%, Accuracy@5: 71.88%\n",
      "Batch 363, Loss: 0.0342, Accuracy@1: 53.12%, Accuracy@5: 82.81%\n",
      "Batch 364, Loss: 0.0078, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 365, Loss: 0.0143, Accuracy@1: 76.56%, Accuracy@5: 87.50%\n",
      "Batch 366, Loss: 0.0323, Accuracy@1: 48.44%, Accuracy@5: 73.44%\n",
      "Batch 367, Loss: 0.0356, Accuracy@1: 57.81%, Accuracy@5: 78.12%\n",
      "Batch 368, Loss: 0.0119, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 369, Loss: 0.0170, Accuracy@1: 76.56%, Accuracy@5: 85.94%\n",
      "Batch 370, Loss: 0.0228, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 371, Loss: 0.0082, Accuracy@1: 87.50%, Accuracy@5: 93.75%\n",
      "Batch 372, Loss: 0.0133, Accuracy@1: 81.25%, Accuracy@5: 87.50%\n",
      "Batch 373, Loss: 0.0354, Accuracy@1: 60.94%, Accuracy@5: 78.12%\n",
      "Batch 374, Loss: 0.0350, Accuracy@1: 39.06%, Accuracy@5: 84.38%\n",
      "Batch 375, Loss: 0.0197, Accuracy@1: 67.19%, Accuracy@5: 89.06%\n",
      "Batch 376, Loss: 0.0293, Accuracy@1: 42.19%, Accuracy@5: 89.06%\n",
      "Batch 377, Loss: 0.0234, Accuracy@1: 57.81%, Accuracy@5: 85.94%\n",
      "Batch 378, Loss: 0.0155, Accuracy@1: 71.88%, Accuracy@5: 95.31%\n",
      "Batch 379, Loss: 0.0215, Accuracy@1: 56.25%, Accuracy@5: 93.75%\n",
      "Batch 380, Loss: 0.0143, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 381, Loss: 0.0487, Accuracy@1: 37.50%, Accuracy@5: 73.44%\n",
      "Batch 382, Loss: 0.0267, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 383, Loss: 0.0182, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 384, Loss: 0.0157, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 385, Loss: 0.0484, Accuracy@1: 37.50%, Accuracy@5: 64.06%\n",
      "Batch 386, Loss: 0.0253, Accuracy@1: 56.25%, Accuracy@5: 84.38%\n",
      "Batch 387, Loss: 0.0106, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 388, Loss: 0.0172, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 389, Loss: 0.0294, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 390, Loss: 0.0326, Accuracy@1: 46.88%, Accuracy@5: 71.88%\n",
      "Batch 391, Loss: 0.0335, Accuracy@1: 60.94%, Accuracy@5: 79.69%\n",
      "Batch 392, Loss: 0.0295, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 393, Loss: 0.0312, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 394, Loss: 0.0349, Accuracy@1: 57.81%, Accuracy@5: 78.12%\n",
      "Batch 395, Loss: 0.0229, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 396, Loss: 0.0177, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 397, Loss: 0.0262, Accuracy@1: 59.38%, Accuracy@5: 84.38%\n",
      "Batch 398, Loss: 0.0165, Accuracy@1: 68.75%, Accuracy@5: 87.50%\n",
      "Batch 399, Loss: 0.0094, Accuracy@1: 78.12%, Accuracy@5: 100.00%\n",
      "Batch 400, Loss: 0.0128, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 401, Loss: 0.0198, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 402, Loss: 0.0254, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 403, Loss: 0.0315, Accuracy@1: 50.00%, Accuracy@5: 81.25%\n",
      "Batch 404, Loss: 0.0115, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 405, Loss: 0.0228, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 406, Loss: 0.0197, Accuracy@1: 65.62%, Accuracy@5: 92.19%\n",
      "Batch 407, Loss: 0.0147, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 408, Loss: 0.0217, Accuracy@1: 62.50%, Accuracy@5: 82.81%\n",
      "Batch 409, Loss: 0.0318, Accuracy@1: 46.88%, Accuracy@5: 78.12%\n",
      "Batch 410, Loss: 0.0169, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 411, Loss: 0.0247, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 412, Loss: 0.0155, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 413, Loss: 0.0268, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 414, Loss: 0.0289, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 415, Loss: 0.0226, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 416, Loss: 0.0095, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 417, Loss: 0.0298, Accuracy@1: 59.38%, Accuracy@5: 78.12%\n",
      "Batch 418, Loss: 0.0123, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 419, Loss: 0.0142, Accuracy@1: 75.00%, Accuracy@5: 98.44%\n",
      "Batch 420, Loss: 0.0090, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 421, Loss: 0.0288, Accuracy@1: 57.81%, Accuracy@5: 79.69%\n",
      "Batch 422, Loss: 0.0198, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 423, Loss: 0.0324, Accuracy@1: 51.56%, Accuracy@5: 73.44%\n",
      "Batch 424, Loss: 0.0275, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 425, Loss: 0.0182, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 426, Loss: 0.0167, Accuracy@1: 78.12%, Accuracy@5: 89.06%\n",
      "Batch 427, Loss: 0.0136, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 428, Loss: 0.0061, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 429, Loss: 0.0262, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 430, Loss: 0.0155, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 431, Loss: 0.0185, Accuracy@1: 76.56%, Accuracy@5: 85.94%\n",
      "Batch 432, Loss: 0.0101, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 433, Loss: 0.0058, Accuracy@1: 90.62%, Accuracy@5: 95.31%\n",
      "Batch 434, Loss: 0.0317, Accuracy@1: 57.81%, Accuracy@5: 78.12%\n",
      "Batch 435, Loss: 0.0271, Accuracy@1: 67.19%, Accuracy@5: 82.81%\n",
      "Batch 436, Loss: 0.0438, Accuracy@1: 40.62%, Accuracy@5: 68.75%\n",
      "Batch 437, Loss: 0.0165, Accuracy@1: 79.69%, Accuracy@5: 85.94%\n",
      "Batch 438, Loss: 0.0082, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 439, Loss: 0.0106, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 440, Loss: 0.0110, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 441, Loss: 0.0073, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 442, Loss: 0.0129, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 443, Loss: 0.0328, Accuracy@1: 51.56%, Accuracy@5: 81.25%\n",
      "Batch 444, Loss: 0.0081, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 445, Loss: 0.0121, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 446, Loss: 0.0165, Accuracy@1: 73.44%, Accuracy@5: 90.62%\n",
      "Batch 447, Loss: 0.0149, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 448, Loss: 0.0099, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 449, Loss: 0.0157, Accuracy@1: 79.69%, Accuracy@5: 89.06%\n",
      "Batch 450, Loss: 0.0105, Accuracy@1: 89.06%, Accuracy@5: 93.75%\n",
      "Batch 451, Loss: 0.0111, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 452, Loss: 0.0167, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 453, Loss: 0.0122, Accuracy@1: 79.69%, Accuracy@5: 96.88%\n",
      "Batch 454, Loss: 0.0245, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 455, Loss: 0.0202, Accuracy@1: 81.25%, Accuracy@5: 89.06%\n",
      "Batch 456, Loss: 0.0373, Accuracy@1: 53.12%, Accuracy@5: 75.00%\n",
      "Batch 457, Loss: 0.0351, Accuracy@1: 43.75%, Accuracy@5: 75.00%\n",
      "Batch 458, Loss: 0.0238, Accuracy@1: 70.31%, Accuracy@5: 82.81%\n",
      "Batch 459, Loss: 0.0321, Accuracy@1: 59.38%, Accuracy@5: 81.25%\n",
      "Batch 460, Loss: 0.0335, Accuracy@1: 53.12%, Accuracy@5: 73.44%\n",
      "Batch 461, Loss: 0.0258, Accuracy@1: 51.56%, Accuracy@5: 87.50%\n",
      "Batch 462, Loss: 0.0144, Accuracy@1: 79.69%, Accuracy@5: 89.06%\n",
      "Batch 463, Loss: 0.0210, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 464, Loss: 0.0087, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 465, Loss: 0.0194, Accuracy@1: 60.94%, Accuracy@5: 93.75%\n",
      "Batch 466, Loss: 0.0322, Accuracy@1: 60.94%, Accuracy@5: 70.31%\n",
      "Batch 467, Loss: 0.0185, Accuracy@1: 68.75%, Accuracy@5: 92.19%\n",
      "Batch 468, Loss: 0.0285, Accuracy@1: 59.38%, Accuracy@5: 89.06%\n",
      "Batch 469, Loss: 0.0461, Accuracy@1: 45.31%, Accuracy@5: 65.62%\n",
      "Batch 470, Loss: 0.0245, Accuracy@1: 59.38%, Accuracy@5: 85.94%\n",
      "Batch 471, Loss: 0.0128, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 472, Loss: 0.0124, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 473, Loss: 0.0113, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 474, Loss: 0.0091, Accuracy@1: 89.06%, Accuracy@5: 92.19%\n",
      "Batch 475, Loss: 0.0277, Accuracy@1: 67.19%, Accuracy@5: 78.12%\n",
      "Batch 476, Loss: 0.0167, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 477, Loss: 0.0136, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 478, Loss: 0.0103, Accuracy@1: 87.50%, Accuracy@5: 93.75%\n",
      "Batch 479, Loss: 0.0139, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 480, Loss: 0.0153, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 481, Loss: 0.0233, Accuracy@1: 65.62%, Accuracy@5: 85.94%\n",
      "Batch 482, Loss: 0.0224, Accuracy@1: 65.62%, Accuracy@5: 85.94%\n",
      "Batch 483, Loss: 0.0354, Accuracy@1: 39.06%, Accuracy@5: 75.00%\n",
      "Batch 484, Loss: 0.0317, Accuracy@1: 37.50%, Accuracy@5: 85.94%\n",
      "Batch 485, Loss: 0.0159, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 486, Loss: 0.0443, Accuracy@1: 43.75%, Accuracy@5: 67.19%\n",
      "Batch 487, Loss: 0.0362, Accuracy@1: 42.19%, Accuracy@5: 73.44%\n",
      "Batch 488, Loss: 0.0124, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 489, Loss: 0.0263, Accuracy@1: 60.94%, Accuracy@5: 81.25%\n",
      "Batch 490, Loss: 0.0136, Accuracy@1: 78.12%, Accuracy@5: 89.06%\n",
      "Batch 491, Loss: 0.0134, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 492, Loss: 0.0119, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 493, Loss: 0.0274, Accuracy@1: 60.94%, Accuracy@5: 79.69%\n",
      "Batch 494, Loss: 0.0446, Accuracy@1: 51.56%, Accuracy@5: 64.06%\n",
      "Batch 495, Loss: 0.0403, Accuracy@1: 56.25%, Accuracy@5: 71.88%\n",
      "Batch 496, Loss: 0.0275, Accuracy@1: 56.25%, Accuracy@5: 78.12%\n",
      "Batch 497, Loss: 0.0181, Accuracy@1: 62.50%, Accuracy@5: 92.19%\n",
      "Batch 498, Loss: 0.0204, Accuracy@1: 53.12%, Accuracy@5: 92.19%\n",
      "Batch 499, Loss: 0.0293, Accuracy@1: 37.50%, Accuracy@5: 89.06%\n",
      "Batch 500, Loss: 0.0067, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 501, Loss: 0.0178, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 502, Loss: 0.0258, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 503, Loss: 0.0251, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 504, Loss: 0.0040, Accuracy@1: 95.31%, Accuracy@5: 98.44%\n",
      "Batch 505, Loss: 0.0191, Accuracy@1: 71.88%, Accuracy@5: 84.38%\n",
      "Batch 506, Loss: 0.0308, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 507, Loss: 0.0191, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 508, Loss: 0.0369, Accuracy@1: 50.00%, Accuracy@5: 73.44%\n",
      "Batch 509, Loss: 0.0319, Accuracy@1: 56.25%, Accuracy@5: 79.69%\n",
      "Batch 510, Loss: 0.0295, Accuracy@1: 65.62%, Accuracy@5: 78.12%\n",
      "Batch 511, Loss: 0.0189, Accuracy@1: 70.31%, Accuracy@5: 95.31%\n",
      "Batch 512, Loss: 0.0306, Accuracy@1: 51.56%, Accuracy@5: 85.94%\n",
      "Batch 513, Loss: 0.0342, Accuracy@1: 32.81%, Accuracy@5: 81.25%\n",
      "Batch 514, Loss: 0.0194, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 515, Loss: 0.0170, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 516, Loss: 0.0169, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 517, Loss: 0.0226, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 518, Loss: 0.0293, Accuracy@1: 48.44%, Accuracy@5: 89.06%\n",
      "Batch 519, Loss: 0.0271, Accuracy@1: 48.44%, Accuracy@5: 93.75%\n",
      "Batch 520, Loss: 0.0204, Accuracy@1: 60.94%, Accuracy@5: 90.62%\n",
      "Batch 521, Loss: 0.0231, Accuracy@1: 59.38%, Accuracy@5: 90.62%\n",
      "Batch 522, Loss: 0.0054, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 523, Loss: 0.0087, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 524, Loss: 0.0142, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 525, Loss: 0.0143, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 526, Loss: 0.0272, Accuracy@1: 51.56%, Accuracy@5: 90.62%\n",
      "Batch 527, Loss: 0.0330, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 528, Loss: 0.0318, Accuracy@1: 51.56%, Accuracy@5: 81.25%\n",
      "Batch 529, Loss: 0.0484, Accuracy@1: 50.00%, Accuracy@5: 62.50%\n",
      "Batch 530, Loss: 0.0153, Accuracy@1: 79.69%, Accuracy@5: 89.06%\n",
      "Batch 531, Loss: 0.0114, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 532, Loss: 0.0320, Accuracy@1: 43.75%, Accuracy@5: 76.56%\n",
      "Batch 533, Loss: 0.0254, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 534, Loss: 0.0216, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 535, Loss: 0.0038, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 536, Loss: 0.0234, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 537, Loss: 0.0088, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 538, Loss: 0.0355, Accuracy@1: 40.62%, Accuracy@5: 79.69%\n",
      "Batch 539, Loss: 0.0187, Accuracy@1: 70.31%, Accuracy@5: 87.50%\n",
      "Batch 540, Loss: 0.0323, Accuracy@1: 57.81%, Accuracy@5: 79.69%\n",
      "Batch 541, Loss: 0.0268, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 542, Loss: 0.0112, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 543, Loss: 0.0273, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 544, Loss: 0.0214, Accuracy@1: 62.50%, Accuracy@5: 90.62%\n",
      "Batch 545, Loss: 0.0202, Accuracy@1: 54.69%, Accuracy@5: 95.31%\n",
      "Batch 546, Loss: 0.0179, Accuracy@1: 73.44%, Accuracy@5: 81.25%\n",
      "Batch 547, Loss: 0.0190, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 548, Loss: 0.0105, Accuracy@1: 78.12%, Accuracy@5: 98.44%\n",
      "Batch 549, Loss: 0.0190, Accuracy@1: 71.88%, Accuracy@5: 84.38%\n",
      "Batch 550, Loss: 0.0136, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 551, Loss: 0.0389, Accuracy@1: 56.25%, Accuracy@5: 78.12%\n",
      "Batch 552, Loss: 0.0207, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 553, Loss: 0.0251, Accuracy@1: 56.25%, Accuracy@5: 92.19%\n",
      "Batch 554, Loss: 0.0181, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 555, Loss: 0.0324, Accuracy@1: 54.69%, Accuracy@5: 76.56%\n",
      "Batch 556, Loss: 0.0303, Accuracy@1: 57.81%, Accuracy@5: 79.69%\n",
      "Batch 557, Loss: 0.0068, Accuracy@1: 85.94%, Accuracy@5: 100.00%\n",
      "Batch 558, Loss: 0.0175, Accuracy@1: 76.56%, Accuracy@5: 90.62%\n",
      "Batch 559, Loss: 0.0171, Accuracy@1: 82.81%, Accuracy@5: 90.62%\n",
      "Batch 560, Loss: 0.0202, Accuracy@1: 65.62%, Accuracy@5: 93.75%\n",
      "Batch 561, Loss: 0.0249, Accuracy@1: 56.25%, Accuracy@5: 89.06%\n",
      "Batch 562, Loss: 0.0178, Accuracy@1: 76.56%, Accuracy@5: 87.50%\n",
      "Batch 563, Loss: 0.0194, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 564, Loss: 0.0110, Accuracy@1: 87.50%, Accuracy@5: 92.19%\n",
      "Batch 565, Loss: 0.0148, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 566, Loss: 0.0220, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 567, Loss: 0.0127, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 568, Loss: 0.0254, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 569, Loss: 0.0488, Accuracy@1: 46.88%, Accuracy@5: 62.50%\n",
      "Batch 570, Loss: 0.0223, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 571, Loss: 0.0322, Accuracy@1: 54.69%, Accuracy@5: 76.56%\n",
      "Batch 572, Loss: 0.0204, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 573, Loss: 0.0231, Accuracy@1: 64.06%, Accuracy@5: 85.94%\n",
      "Batch 574, Loss: 0.0203, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 575, Loss: 0.0182, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 576, Loss: 0.0154, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 577, Loss: 0.0121, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 578, Loss: 0.0347, Accuracy@1: 48.44%, Accuracy@5: 71.88%\n",
      "Batch 579, Loss: 0.0283, Accuracy@1: 65.62%, Accuracy@5: 78.12%\n",
      "Batch 580, Loss: 0.0232, Accuracy@1: 56.25%, Accuracy@5: 82.81%\n",
      "Batch 581, Loss: 0.0251, Accuracy@1: 35.94%, Accuracy@5: 89.06%\n",
      "Batch 582, Loss: 0.0210, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 583, Loss: 0.0158, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 584, Loss: 0.0270, Accuracy@1: 53.12%, Accuracy@5: 85.94%\n",
      "Batch 585, Loss: 0.0243, Accuracy@1: 68.75%, Accuracy@5: 78.12%\n",
      "Batch 586, Loss: 0.0198, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 587, Loss: 0.0162, Accuracy@1: 73.44%, Accuracy@5: 93.75%\n",
      "Batch 588, Loss: 0.0240, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 589, Loss: 0.0183, Accuracy@1: 68.75%, Accuracy@5: 93.75%\n",
      "Batch 590, Loss: 0.0096, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 591, Loss: 0.0160, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 592, Loss: 0.0264, Accuracy@1: 60.94%, Accuracy@5: 82.81%\n",
      "Batch 593, Loss: 0.0174, Accuracy@1: 73.44%, Accuracy@5: 90.62%\n",
      "Batch 594, Loss: 0.0205, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 595, Loss: 0.0336, Accuracy@1: 57.81%, Accuracy@5: 78.12%\n",
      "Batch 596, Loss: 0.0140, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 597, Loss: 0.0369, Accuracy@1: 51.56%, Accuracy@5: 76.56%\n",
      "Batch 598, Loss: 0.0155, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 599, Loss: 0.0263, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 600, Loss: 0.0162, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 601, Loss: 0.0226, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 602, Loss: 0.0134, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 603, Loss: 0.0336, Accuracy@1: 62.50%, Accuracy@5: 76.56%\n",
      "Batch 604, Loss: 0.0239, Accuracy@1: 64.06%, Accuracy@5: 82.81%\n",
      "Batch 605, Loss: 0.0227, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 606, Loss: 0.0259, Accuracy@1: 60.94%, Accuracy@5: 79.69%\n",
      "Batch 607, Loss: 0.0183, Accuracy@1: 73.44%, Accuracy@5: 85.94%\n",
      "Batch 608, Loss: 0.0186, Accuracy@1: 78.12%, Accuracy@5: 84.38%\n",
      "Batch 609, Loss: 0.0089, Accuracy@1: 81.25%, Accuracy@5: 98.44%\n",
      "Batch 610, Loss: 0.0078, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 611, Loss: 0.0347, Accuracy@1: 32.81%, Accuracy@5: 81.25%\n",
      "Batch 612, Loss: 0.0286, Accuracy@1: 64.06%, Accuracy@5: 81.25%\n",
      "Batch 613, Loss: 0.0259, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 614, Loss: 0.0215, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 615, Loss: 0.0251, Accuracy@1: 67.19%, Accuracy@5: 89.06%\n",
      "Batch 616, Loss: 0.0150, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 617, Loss: 0.0251, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 618, Loss: 0.0201, Accuracy@1: 64.06%, Accuracy@5: 85.94%\n",
      "Batch 619, Loss: 0.0276, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 620, Loss: 0.0277, Accuracy@1: 65.62%, Accuracy@5: 78.12%\n",
      "Batch 621, Loss: 0.0178, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 622, Loss: 0.0241, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 623, Loss: 0.0289, Accuracy@1: 65.62%, Accuracy@5: 78.12%\n",
      "Batch 624, Loss: 0.0384, Accuracy@1: 50.00%, Accuracy@5: 78.12%\n",
      "Batch 625, Loss: 0.0042, Accuracy@1: 96.88%, Accuracy@5: 96.88%\n",
      "Batch 626, Loss: 0.0056, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 627, Loss: 0.0077, Accuracy@1: 84.38%, Accuracy@5: 98.44%\n",
      "Batch 628, Loss: 0.0252, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 629, Loss: 0.0162, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 630, Loss: 0.0259, Accuracy@1: 68.75%, Accuracy@5: 85.94%\n",
      "Batch 631, Loss: 0.0292, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 632, Loss: 0.0222, Accuracy@1: 53.12%, Accuracy@5: 90.62%\n",
      "Batch 633, Loss: 0.0374, Accuracy@1: 45.31%, Accuracy@5: 76.56%\n",
      "Batch 634, Loss: 0.0152, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 635, Loss: 0.0401, Accuracy@1: 43.75%, Accuracy@5: 75.00%\n",
      "Batch 636, Loss: 0.0123, Accuracy@1: 79.69%, Accuracy@5: 95.31%\n",
      "Batch 637, Loss: 0.0129, Accuracy@1: 81.25%, Accuracy@5: 98.44%\n",
      "Batch 638, Loss: 0.0161, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 639, Loss: 0.0493, Accuracy@1: 32.81%, Accuracy@5: 59.38%\n",
      "Batch 640, Loss: 0.0170, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 641, Loss: 0.0061, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 642, Loss: 0.0116, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 643, Loss: 0.0282, Accuracy@1: 59.38%, Accuracy@5: 85.94%\n",
      "Batch 644, Loss: 0.0218, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 645, Loss: 0.0281, Accuracy@1: 59.38%, Accuracy@5: 89.06%\n",
      "Batch 646, Loss: 0.0339, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 647, Loss: 0.0225, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 648, Loss: 0.0211, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 649, Loss: 0.0171, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 650, Loss: 0.0095, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 651, Loss: 0.0249, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 652, Loss: 0.0300, Accuracy@1: 53.12%, Accuracy@5: 75.00%\n",
      "Batch 653, Loss: 0.0469, Accuracy@1: 21.88%, Accuracy@5: 65.62%\n",
      "Batch 654, Loss: 0.0434, Accuracy@1: 32.81%, Accuracy@5: 67.19%\n",
      "Batch 655, Loss: 0.0313, Accuracy@1: 51.56%, Accuracy@5: 85.94%\n",
      "Batch 656, Loss: 0.0159, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 657, Loss: 0.0406, Accuracy@1: 35.94%, Accuracy@5: 68.75%\n",
      "Batch 658, Loss: 0.0228, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 659, Loss: 0.0221, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 660, Loss: 0.0350, Accuracy@1: 53.12%, Accuracy@5: 71.88%\n",
      "Batch 661, Loss: 0.0237, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 662, Loss: 0.0234, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 663, Loss: 0.0187, Accuracy@1: 62.50%, Accuracy@5: 92.19%\n",
      "Batch 664, Loss: 0.0143, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 665, Loss: 0.0218, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 666, Loss: 0.0114, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 667, Loss: 0.0158, Accuracy@1: 76.56%, Accuracy@5: 87.50%\n",
      "Batch 668, Loss: 0.0245, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 669, Loss: 0.0218, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 670, Loss: 0.0256, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 671, Loss: 0.0287, Accuracy@1: 60.94%, Accuracy@5: 71.88%\n",
      "Batch 672, Loss: 0.0333, Accuracy@1: 45.31%, Accuracy@5: 79.69%\n",
      "Batch 673, Loss: 0.0172, Accuracy@1: 73.44%, Accuracy@5: 90.62%\n",
      "Batch 674, Loss: 0.0116, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 675, Loss: 0.0209, Accuracy@1: 67.19%, Accuracy@5: 92.19%\n",
      "Batch 676, Loss: 0.0198, Accuracy@1: 71.88%, Accuracy@5: 92.19%\n",
      "Batch 677, Loss: 0.0136, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 678, Loss: 0.0331, Accuracy@1: 54.69%, Accuracy@5: 75.00%\n",
      "Batch 679, Loss: 0.0313, Accuracy@1: 64.06%, Accuracy@5: 76.56%\n",
      "Batch 680, Loss: 0.0116, Accuracy@1: 67.19%, Accuracy@5: 98.44%\n",
      "Batch 681, Loss: 0.0218, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 682, Loss: 0.0072, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 683, Loss: 0.0068, Accuracy@1: 82.81%, Accuracy@5: 100.00%\n",
      "Batch 684, Loss: 0.0283, Accuracy@1: 43.75%, Accuracy@5: 81.25%\n",
      "Batch 685, Loss: 0.0145, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 686, Loss: 0.0187, Accuracy@1: 67.19%, Accuracy@5: 89.06%\n",
      "Batch 687, Loss: 0.0181, Accuracy@1: 75.00%, Accuracy@5: 82.81%\n",
      "Batch 688, Loss: 0.0183, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 689, Loss: 0.0231, Accuracy@1: 67.19%, Accuracy@5: 82.81%\n",
      "Batch 690, Loss: 0.0246, Accuracy@1: 60.94%, Accuracy@5: 82.81%\n",
      "Batch 691, Loss: 0.0541, Accuracy@1: 35.94%, Accuracy@5: 62.50%\n",
      "Batch 692, Loss: 0.0168, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 693, Loss: 0.0282, Accuracy@1: 60.94%, Accuracy@5: 82.81%\n",
      "Batch 694, Loss: 0.0144, Accuracy@1: 68.75%, Accuracy@5: 93.75%\n",
      "Batch 695, Loss: 0.0099, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 696, Loss: 0.0192, Accuracy@1: 78.12%, Accuracy@5: 89.06%\n",
      "Batch 697, Loss: 0.0238, Accuracy@1: 54.69%, Accuracy@5: 85.94%\n",
      "Batch 698, Loss: 0.0213, Accuracy@1: 57.81%, Accuracy@5: 89.06%\n",
      "Batch 699, Loss: 0.0147, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 700, Loss: 0.0151, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 701, Loss: 0.0204, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 702, Loss: 0.0460, Accuracy@1: 32.81%, Accuracy@5: 71.88%\n",
      "Batch 703, Loss: 0.0151, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 704, Loss: 0.0222, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 705, Loss: 0.0153, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 706, Loss: 0.0280, Accuracy@1: 64.06%, Accuracy@5: 78.12%\n",
      "Batch 707, Loss: 0.0371, Accuracy@1: 54.69%, Accuracy@5: 73.44%\n",
      "Batch 708, Loss: 0.0423, Accuracy@1: 35.94%, Accuracy@5: 75.00%\n",
      "Batch 709, Loss: 0.0272, Accuracy@1: 46.88%, Accuracy@5: 84.38%\n",
      "Batch 710, Loss: 0.0287, Accuracy@1: 46.88%, Accuracy@5: 89.06%\n",
      "Batch 711, Loss: 0.0324, Accuracy@1: 48.44%, Accuracy@5: 87.50%\n",
      "Batch 712, Loss: 0.0228, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 713, Loss: 0.0140, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 714, Loss: 0.0109, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 715, Loss: 0.0039, Accuracy@1: 93.75%, Accuracy@5: 100.00%\n",
      "Batch 716, Loss: 0.0174, Accuracy@1: 82.81%, Accuracy@5: 90.62%\n",
      "Batch 717, Loss: 0.0131, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 718, Loss: 0.0228, Accuracy@1: 73.44%, Accuracy@5: 82.81%\n",
      "Batch 719, Loss: 0.0159, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 720, Loss: 0.0142, Accuracy@1: 70.31%, Accuracy@5: 95.31%\n",
      "Batch 721, Loss: 0.0336, Accuracy@1: 46.88%, Accuracy@5: 82.81%\n",
      "Batch 722, Loss: 0.0120, Accuracy@1: 75.00%, Accuracy@5: 95.31%\n",
      "Batch 723, Loss: 0.0195, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 724, Loss: 0.0056, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 725, Loss: 0.0213, Accuracy@1: 60.94%, Accuracy@5: 92.19%\n",
      "Batch 726, Loss: 0.0293, Accuracy@1: 59.38%, Accuracy@5: 82.81%\n",
      "Batch 727, Loss: 0.0209, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 728, Loss: 0.0173, Accuracy@1: 70.31%, Accuracy@5: 85.94%\n",
      "Batch 729, Loss: 0.0071, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 730, Loss: 0.0182, Accuracy@1: 65.62%, Accuracy@5: 92.19%\n",
      "Batch 731, Loss: 0.0120, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 732, Loss: 0.0050, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 733, Loss: 0.0095, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 734, Loss: 0.0101, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 735, Loss: 0.0098, Accuracy@1: 79.69%, Accuracy@5: 98.44%\n",
      "Batch 736, Loss: 0.0174, Accuracy@1: 67.19%, Accuracy@5: 92.19%\n",
      "Batch 737, Loss: 0.0141, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 738, Loss: 0.0074, Accuracy@1: 87.50%, Accuracy@5: 100.00%\n",
      "Batch 739, Loss: 0.0165, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 740, Loss: 0.0179, Accuracy@1: 59.38%, Accuracy@5: 98.44%\n",
      "Batch 741, Loss: 0.0158, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 742, Loss: 0.0158, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 743, Loss: 0.0196, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 744, Loss: 0.0075, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 745, Loss: 0.0085, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 746, Loss: 0.0043, Accuracy@1: 92.19%, Accuracy@5: 98.44%\n",
      "Batch 747, Loss: 0.0081, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 748, Loss: 0.0120, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 749, Loss: 0.0116, Accuracy@1: 82.81%, Accuracy@5: 89.06%\n",
      "Batch 750, Loss: 0.0355, Accuracy@1: 45.31%, Accuracy@5: 78.12%\n",
      "Batch 751, Loss: 0.0309, Accuracy@1: 45.31%, Accuracy@5: 89.06%\n",
      "Batch 752, Loss: 0.0191, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 753, Loss: 0.0230, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 754, Loss: 0.0148, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 755, Loss: 0.0282, Accuracy@1: 60.94%, Accuracy@5: 84.38%\n",
      "Batch 756, Loss: 0.0324, Accuracy@1: 51.56%, Accuracy@5: 76.56%\n",
      "Batch 757, Loss: 0.0417, Accuracy@1: 42.19%, Accuracy@5: 75.00%\n",
      "Batch 758, Loss: 0.0216, Accuracy@1: 57.81%, Accuracy@5: 92.19%\n",
      "Batch 759, Loss: 0.0208, Accuracy@1: 60.94%, Accuracy@5: 95.31%\n",
      "Batch 760, Loss: 0.0158, Accuracy@1: 75.00%, Accuracy@5: 95.31%\n",
      "Batch 761, Loss: 0.0118, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 762, Loss: 0.0312, Accuracy@1: 46.88%, Accuracy@5: 84.38%\n",
      "Batch 763, Loss: 0.0231, Accuracy@1: 56.25%, Accuracy@5: 92.19%\n",
      "Batch 764, Loss: 0.0322, Accuracy@1: 45.31%, Accuracy@5: 81.25%\n",
      "Batch 765, Loss: 0.0109, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 766, Loss: 0.0182, Accuracy@1: 78.12%, Accuracy@5: 87.50%\n",
      "Batch 767, Loss: 0.0216, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 768, Loss: 0.0057, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 769, Loss: 0.0049, Accuracy@1: 95.31%, Accuracy@5: 98.44%\n",
      "Batch 770, Loss: 0.0003, Accuracy@1: 100.00%, Accuracy@5: 100.00%\n",
      "Batch 771, Loss: 0.0170, Accuracy@1: 57.81%, Accuracy@5: 93.75%\n",
      "Batch 772, Loss: 0.0034, Accuracy@1: 95.31%, Accuracy@5: 96.88%\n",
      "Batch 773, Loss: 0.0052, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 774, Loss: 0.0027, Accuracy@1: 96.88%, Accuracy@5: 98.44%\n",
      "Batch 775, Loss: 0.0097, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 776, Loss: 0.0050, Accuracy@1: 90.62%, Accuracy@5: 96.88%\n",
      "Batch 777, Loss: 0.0043, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 778, Loss: 0.0093, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 779, Loss: 0.0159, Accuracy@1: 64.06%, Accuracy@5: 96.88%\n",
      "Batch 780, Loss: 0.0278, Accuracy@1: 37.50%, Accuracy@5: 92.19%\n",
      "Batch 781, Loss: 0.1948, Accuracy@1: 37.50%, Accuracy@5: 68.75%\n",
      "acc@1: 71.87%, acc@5: 90.31%, loss: 14.188950040435884\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:56:53.122615Z",
     "start_time": "2024-12-03T23:56:53.111922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = 0.0\n",
    "    total_predictions = 0\n",
    "    true_predictions_top1 = 0\n",
    "    true_predictions_top5 = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets) / inputs.size(0)\n",
    "            losses += loss.item()\n",
    "            \n",
    "            # Top-1 predictions\n",
    "            _, predicted_top1 = torch.max(outputs, 1)\n",
    "            batch_true_predictions_top1 = (predicted_top1 == targets).sum().item()\n",
    "            true_predictions_top1 += batch_true_predictions_top1\n",
    "            \n",
    "            # Top-5 predictions\n",
    "            _, predicted_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            batch_true_predictions_top5 = sum(\n",
    "                [targets[i].item() in predicted_top5[i].tolist() for i in range(targets.size(0))]\n",
    "            )\n",
    "            true_predictions_top5 += batch_true_predictions_top5\n",
    "            \n",
    "            # Update total predictions\n",
    "            batch_total_predictions = outputs.size(0)\n",
    "            total_predictions += batch_total_predictions\n",
    "            \n",
    "            # Print batch metrics\n",
    "            print(\n",
    "                f'Batch {batch_idx}, Loss: {loss:.4f}, '\n",
    "                f'Accuracy@1: {batch_true_predictions_top1/batch_total_predictions*100:.2f}%, '\n",
    "                f'Accuracy@5: {batch_true_predictions_top5/batch_total_predictions*100:.2f}%'\n",
    "            )\n",
    "    \n",
    "    # Compute overall accuracies\n",
    "    accuracy_top1 = true_predictions_top1 / total_predictions\n",
    "    accuracy_top5 = true_predictions_top5 / total_predictions\n",
    "    \n",
    "    return accuracy_top1, accuracy_top5, losses\n"
   ],
   "id": "550a11cf124331c4",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T01:12:49.089082Z",
     "start_time": "2024-12-04T00:49:13.516899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy_top1, accuracy_top5, losses = evaluate(model, DEVICE, test_dataloader)\n",
    "print(f\"acc@1: {accuracy_top1*100}%, acc@5: {accuracy_top5*100}%, loss: {losses}\")\n"
   ],
   "id": "1b30f5f810406f77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.0177, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 1, Loss: 0.0200, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 2, Loss: 0.0219, Accuracy@1: 78.12%, Accuracy@5: 98.44%\n",
      "Batch 3, Loss: 0.0237, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 4, Loss: 0.0297, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 5, Loss: 0.0288, Accuracy@1: 64.06%, Accuracy@5: 90.62%\n",
      "Batch 6, Loss: 0.0358, Accuracy@1: 60.94%, Accuracy@5: 84.38%\n",
      "Batch 7, Loss: 0.0215, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 8, Loss: 0.0172, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 9, Loss: 0.0209, Accuracy@1: 76.56%, Accuracy@5: 89.06%\n",
      "Batch 10, Loss: 0.0098, Accuracy@1: 90.62%, Accuracy@5: 100.00%\n",
      "Batch 11, Loss: 0.0140, Accuracy@1: 89.06%, Accuracy@5: 93.75%\n",
      "Batch 12, Loss: 0.0168, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 13, Loss: 0.0193, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 14, Loss: 0.0188, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 15, Loss: 0.0186, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 16, Loss: 0.0213, Accuracy@1: 79.69%, Accuracy@5: 89.06%\n",
      "Batch 17, Loss: 0.0137, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 18, Loss: 0.0233, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 19, Loss: 0.0221, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 20, Loss: 0.0259, Accuracy@1: 68.75%, Accuracy@5: 95.31%\n",
      "Batch 21, Loss: 0.0316, Accuracy@1: 59.38%, Accuracy@5: 81.25%\n",
      "Batch 22, Loss: 0.0257, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 23, Loss: 0.0249, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 24, Loss: 0.0352, Accuracy@1: 53.12%, Accuracy@5: 85.94%\n",
      "Batch 25, Loss: 0.0393, Accuracy@1: 35.94%, Accuracy@5: 73.44%\n",
      "Batch 26, Loss: 0.0312, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 27, Loss: 0.0310, Accuracy@1: 59.38%, Accuracy@5: 85.94%\n",
      "Batch 28, Loss: 0.0334, Accuracy@1: 43.75%, Accuracy@5: 90.62%\n",
      "Batch 29, Loss: 0.0304, Accuracy@1: 51.56%, Accuracy@5: 92.19%\n",
      "Batch 30, Loss: 0.0367, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 31, Loss: 0.0320, Accuracy@1: 29.69%, Accuracy@5: 92.19%\n",
      "Batch 32, Loss: 0.0301, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 33, Loss: 0.0415, Accuracy@1: 51.56%, Accuracy@5: 79.69%\n",
      "Batch 34, Loss: 0.0420, Accuracy@1: 50.00%, Accuracy@5: 78.12%\n",
      "Batch 35, Loss: 0.0428, Accuracy@1: 54.69%, Accuracy@5: 75.00%\n",
      "Batch 36, Loss: 0.0359, Accuracy@1: 51.56%, Accuracy@5: 82.81%\n",
      "Batch 37, Loss: 0.0379, Accuracy@1: 67.19%, Accuracy@5: 78.12%\n",
      "Batch 38, Loss: 0.0383, Accuracy@1: 59.38%, Accuracy@5: 76.56%\n",
      "Batch 39, Loss: 0.0366, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 40, Loss: 0.0269, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 41, Loss: 0.0349, Accuracy@1: 57.81%, Accuracy@5: 87.50%\n",
      "Batch 42, Loss: 0.0352, Accuracy@1: 51.56%, Accuracy@5: 89.06%\n",
      "Batch 43, Loss: 0.0333, Accuracy@1: 48.44%, Accuracy@5: 85.94%\n",
      "Batch 44, Loss: 0.0339, Accuracy@1: 53.12%, Accuracy@5: 82.81%\n",
      "Batch 45, Loss: 0.0289, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 46, Loss: 0.0370, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 47, Loss: 0.0371, Accuracy@1: 57.81%, Accuracy@5: 87.50%\n",
      "Batch 48, Loss: 0.0420, Accuracy@1: 46.88%, Accuracy@5: 75.00%\n",
      "Batch 49, Loss: 0.0326, Accuracy@1: 57.81%, Accuracy@5: 81.25%\n",
      "Batch 50, Loss: 0.0325, Accuracy@1: 51.56%, Accuracy@5: 84.38%\n",
      "Batch 51, Loss: 0.0382, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 52, Loss: 0.0372, Accuracy@1: 51.56%, Accuracy@5: 79.69%\n",
      "Batch 53, Loss: 0.0416, Accuracy@1: 35.94%, Accuracy@5: 76.56%\n",
      "Batch 54, Loss: 0.0220, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 55, Loss: 0.0253, Accuracy@1: 70.31%, Accuracy@5: 87.50%\n",
      "Batch 56, Loss: 0.0189, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 57, Loss: 0.0371, Accuracy@1: 28.12%, Accuracy@5: 85.94%\n",
      "Batch 58, Loss: 0.0182, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 59, Loss: 0.0139, Accuracy@1: 93.75%, Accuracy@5: 96.88%\n",
      "Batch 60, Loss: 0.0242, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 61, Loss: 0.0374, Accuracy@1: 62.50%, Accuracy@5: 78.12%\n",
      "Batch 62, Loss: 0.0295, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 63, Loss: 0.0230, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 64, Loss: 0.0193, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 65, Loss: 0.0132, Accuracy@1: 93.75%, Accuracy@5: 96.88%\n",
      "Batch 66, Loss: 0.0219, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 67, Loss: 0.0187, Accuracy@1: 76.56%, Accuracy@5: 100.00%\n",
      "Batch 68, Loss: 0.0214, Accuracy@1: 85.94%, Accuracy@5: 92.19%\n",
      "Batch 69, Loss: 0.0172, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 70, Loss: 0.0223, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 71, Loss: 0.0200, Accuracy@1: 84.38%, Accuracy@5: 90.62%\n",
      "Batch 72, Loss: 0.0212, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 73, Loss: 0.0278, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 74, Loss: 0.0174, Accuracy@1: 87.50%, Accuracy@5: 90.62%\n",
      "Batch 75, Loss: 0.0270, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 76, Loss: 0.0165, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 77, Loss: 0.0219, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 78, Loss: 0.0180, Accuracy@1: 85.94%, Accuracy@5: 92.19%\n",
      "Batch 79, Loss: 0.0280, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 80, Loss: 0.0299, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 81, Loss: 0.0288, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 82, Loss: 0.0160, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 83, Loss: 0.0283, Accuracy@1: 79.69%, Accuracy@5: 87.50%\n",
      "Batch 84, Loss: 0.0287, Accuracy@1: 54.69%, Accuracy@5: 90.62%\n",
      "Batch 85, Loss: 0.0221, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 86, Loss: 0.0321, Accuracy@1: 54.69%, Accuracy@5: 85.94%\n",
      "Batch 87, Loss: 0.0361, Accuracy@1: 67.19%, Accuracy@5: 85.94%\n",
      "Batch 88, Loss: 0.0347, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 89, Loss: 0.0392, Accuracy@1: 54.69%, Accuracy@5: 85.94%\n",
      "Batch 90, Loss: 0.0273, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 91, Loss: 0.0337, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 92, Loss: 0.0386, Accuracy@1: 50.00%, Accuracy@5: 78.12%\n",
      "Batch 93, Loss: 0.0407, Accuracy@1: 48.44%, Accuracy@5: 75.00%\n",
      "Batch 94, Loss: 0.0308, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 95, Loss: 0.0277, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 96, Loss: 0.0356, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 97, Loss: 0.0433, Accuracy@1: 51.56%, Accuracy@5: 84.38%\n",
      "Batch 98, Loss: 0.0504, Accuracy@1: 46.88%, Accuracy@5: 67.19%\n",
      "Batch 99, Loss: 0.0311, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 100, Loss: 0.0311, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 101, Loss: 0.0146, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 102, Loss: 0.0180, Accuracy@1: 87.50%, Accuracy@5: 93.75%\n",
      "Batch 103, Loss: 0.0182, Accuracy@1: 89.06%, Accuracy@5: 95.31%\n",
      "Batch 104, Loss: 0.0253, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 105, Loss: 0.0174, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 106, Loss: 0.0142, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 107, Loss: 0.0162, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 108, Loss: 0.0157, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 109, Loss: 0.0123, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 110, Loss: 0.0144, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 111, Loss: 0.0163, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 112, Loss: 0.0126, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 113, Loss: 0.0151, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 114, Loss: 0.0201, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 115, Loss: 0.0324, Accuracy@1: 70.31%, Accuracy@5: 85.94%\n",
      "Batch 116, Loss: 0.0306, Accuracy@1: 67.19%, Accuracy@5: 87.50%\n",
      "Batch 117, Loss: 0.0262, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 118, Loss: 0.0287, Accuracy@1: 65.62%, Accuracy@5: 85.94%\n",
      "Batch 119, Loss: 0.0219, Accuracy@1: 71.88%, Accuracy@5: 96.88%\n",
      "Batch 120, Loss: 0.0223, Accuracy@1: 68.75%, Accuracy@5: 90.62%\n",
      "Batch 121, Loss: 0.0137, Accuracy@1: 89.06%, Accuracy@5: 100.00%\n",
      "Batch 122, Loss: 0.0102, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 123, Loss: 0.0252, Accuracy@1: 56.25%, Accuracy@5: 93.75%\n",
      "Batch 124, Loss: 0.0343, Accuracy@1: 53.12%, Accuracy@5: 92.19%\n",
      "Batch 125, Loss: 0.0186, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 126, Loss: 0.0211, Accuracy@1: 68.75%, Accuracy@5: 95.31%\n",
      "Batch 127, Loss: 0.0373, Accuracy@1: 59.38%, Accuracy@5: 81.25%\n",
      "Batch 128, Loss: 0.0309, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 129, Loss: 0.0452, Accuracy@1: 39.06%, Accuracy@5: 71.88%\n",
      "Batch 130, Loss: 0.0361, Accuracy@1: 29.69%, Accuracy@5: 82.81%\n",
      "Batch 131, Loss: 0.0389, Accuracy@1: 51.56%, Accuracy@5: 85.94%\n",
      "Batch 132, Loss: 0.0238, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 133, Loss: 0.0454, Accuracy@1: 51.56%, Accuracy@5: 73.44%\n",
      "Batch 134, Loss: 0.0250, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 135, Loss: 0.0407, Accuracy@1: 45.31%, Accuracy@5: 75.00%\n",
      "Batch 136, Loss: 0.0344, Accuracy@1: 57.81%, Accuracy@5: 89.06%\n",
      "Batch 137, Loss: 0.0418, Accuracy@1: 39.06%, Accuracy@5: 71.88%\n",
      "Batch 138, Loss: 0.0395, Accuracy@1: 53.12%, Accuracy@5: 76.56%\n",
      "Batch 139, Loss: 0.0321, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 140, Loss: 0.0380, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 141, Loss: 0.0396, Accuracy@1: 45.31%, Accuracy@5: 78.12%\n",
      "Batch 142, Loss: 0.0294, Accuracy@1: 62.50%, Accuracy@5: 93.75%\n",
      "Batch 143, Loss: 0.0199, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 144, Loss: 0.0272, Accuracy@1: 65.62%, Accuracy@5: 93.75%\n",
      "Batch 145, Loss: 0.0226, Accuracy@1: 68.75%, Accuracy@5: 96.88%\n",
      "Batch 146, Loss: 0.0391, Accuracy@1: 43.75%, Accuracy@5: 78.12%\n",
      "Batch 147, Loss: 0.0341, Accuracy@1: 45.31%, Accuracy@5: 81.25%\n",
      "Batch 148, Loss: 0.0300, Accuracy@1: 67.19%, Accuracy@5: 81.25%\n",
      "Batch 149, Loss: 0.0221, Accuracy@1: 76.56%, Accuracy@5: 87.50%\n",
      "Batch 150, Loss: 0.0352, Accuracy@1: 46.88%, Accuracy@5: 75.00%\n",
      "Batch 151, Loss: 0.0355, Accuracy@1: 43.75%, Accuracy@5: 79.69%\n",
      "Batch 152, Loss: 0.0222, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 153, Loss: 0.0259, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 154, Loss: 0.0301, Accuracy@1: 43.75%, Accuracy@5: 90.62%\n",
      "Batch 155, Loss: 0.0283, Accuracy@1: 60.94%, Accuracy@5: 89.06%\n",
      "Batch 156, Loss: 0.0239, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 157, Loss: 0.0304, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 158, Loss: 0.0243, Accuracy@1: 78.12%, Accuracy@5: 98.44%\n",
      "Batch 159, Loss: 0.0224, Accuracy@1: 59.38%, Accuracy@5: 98.44%\n",
      "Batch 160, Loss: 0.0356, Accuracy@1: 57.81%, Accuracy@5: 81.25%\n",
      "Batch 161, Loss: 0.0244, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 162, Loss: 0.0324, Accuracy@1: 67.19%, Accuracy@5: 95.31%\n",
      "Batch 163, Loss: 0.0385, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 164, Loss: 0.0380, Accuracy@1: 53.12%, Accuracy@5: 82.81%\n",
      "Batch 165, Loss: 0.0369, Accuracy@1: 50.00%, Accuracy@5: 84.38%\n",
      "Batch 166, Loss: 0.0295, Accuracy@1: 62.50%, Accuracy@5: 82.81%\n",
      "Batch 167, Loss: 0.0228, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 168, Loss: 0.0226, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 169, Loss: 0.0153, Accuracy@1: 85.94%, Accuracy@5: 98.44%\n",
      "Batch 170, Loss: 0.0144, Accuracy@1: 76.56%, Accuracy@5: 100.00%\n",
      "Batch 171, Loss: 0.0246, Accuracy@1: 70.31%, Accuracy@5: 95.31%\n",
      "Batch 172, Loss: 0.0247, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 173, Loss: 0.0251, Accuracy@1: 67.19%, Accuracy@5: 85.94%\n",
      "Batch 174, Loss: 0.0325, Accuracy@1: 65.62%, Accuracy@5: 92.19%\n",
      "Batch 175, Loss: 0.0365, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 176, Loss: 0.0363, Accuracy@1: 48.44%, Accuracy@5: 81.25%\n",
      "Batch 177, Loss: 0.0461, Accuracy@1: 35.94%, Accuracy@5: 70.31%\n",
      "Batch 178, Loss: 0.0244, Accuracy@1: 73.44%, Accuracy@5: 93.75%\n",
      "Batch 179, Loss: 0.0262, Accuracy@1: 68.75%, Accuracy@5: 90.62%\n",
      "Batch 180, Loss: 0.0211, Accuracy@1: 50.00%, Accuracy@5: 93.75%\n",
      "Batch 181, Loss: 0.0284, Accuracy@1: 51.56%, Accuracy@5: 93.75%\n",
      "Batch 182, Loss: 0.0340, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 183, Loss: 0.0302, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 184, Loss: 0.0351, Accuracy@1: 57.81%, Accuracy@5: 87.50%\n",
      "Batch 185, Loss: 0.0316, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 186, Loss: 0.0209, Accuracy@1: 65.62%, Accuracy@5: 96.88%\n",
      "Batch 187, Loss: 0.0178, Accuracy@1: 64.06%, Accuracy@5: 100.00%\n",
      "Batch 188, Loss: 0.0260, Accuracy@1: 40.62%, Accuracy@5: 95.31%\n",
      "Batch 189, Loss: 0.0227, Accuracy@1: 75.00%, Accuracy@5: 95.31%\n",
      "Batch 190, Loss: 0.0255, Accuracy@1: 71.88%, Accuracy@5: 95.31%\n",
      "Batch 191, Loss: 0.0241, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 192, Loss: 0.0421, Accuracy@1: 43.75%, Accuracy@5: 70.31%\n",
      "Batch 193, Loss: 0.0163, Accuracy@1: 75.00%, Accuracy@5: 98.44%\n",
      "Batch 194, Loss: 0.0262, Accuracy@1: 54.69%, Accuracy@5: 98.44%\n",
      "Batch 195, Loss: 0.0242, Accuracy@1: 64.06%, Accuracy@5: 93.75%\n",
      "Batch 196, Loss: 0.0224, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 197, Loss: 0.0212, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 198, Loss: 0.0184, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 199, Loss: 0.0135, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 200, Loss: 0.0260, Accuracy@1: 71.88%, Accuracy@5: 95.31%\n",
      "Batch 201, Loss: 0.0222, Accuracy@1: 76.56%, Accuracy@5: 96.88%\n",
      "Batch 202, Loss: 0.0193, Accuracy@1: 82.81%, Accuracy@5: 96.88%\n",
      "Batch 203, Loss: 0.0287, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 204, Loss: 0.0327, Accuracy@1: 60.94%, Accuracy@5: 84.38%\n",
      "Batch 205, Loss: 0.0188, Accuracy@1: 79.69%, Accuracy@5: 98.44%\n",
      "Batch 206, Loss: 0.0151, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 207, Loss: 0.0264, Accuracy@1: 56.25%, Accuracy@5: 92.19%\n",
      "Batch 208, Loss: 0.0275, Accuracy@1: 39.06%, Accuracy@5: 90.62%\n",
      "Batch 209, Loss: 0.0309, Accuracy@1: 57.81%, Accuracy@5: 90.62%\n",
      "Batch 210, Loss: 0.0377, Accuracy@1: 37.50%, Accuracy@5: 82.81%\n",
      "Batch 211, Loss: 0.0311, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 212, Loss: 0.0384, Accuracy@1: 46.88%, Accuracy@5: 75.00%\n",
      "Batch 213, Loss: 0.0393, Accuracy@1: 53.12%, Accuracy@5: 79.69%\n",
      "Batch 214, Loss: 0.0200, Accuracy@1: 78.12%, Accuracy@5: 95.31%\n",
      "Batch 215, Loss: 0.0265, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 216, Loss: 0.0259, Accuracy@1: 68.75%, Accuracy@5: 90.62%\n",
      "Batch 217, Loss: 0.0255, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 218, Loss: 0.0281, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 219, Loss: 0.0329, Accuracy@1: 53.12%, Accuracy@5: 92.19%\n",
      "Batch 220, Loss: 0.0411, Accuracy@1: 26.56%, Accuracy@5: 79.69%\n",
      "Batch 221, Loss: 0.0290, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 222, Loss: 0.0278, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 223, Loss: 0.0319, Accuracy@1: 70.31%, Accuracy@5: 87.50%\n",
      "Batch 224, Loss: 0.0295, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 225, Loss: 0.0236, Accuracy@1: 73.44%, Accuracy@5: 96.88%\n",
      "Batch 226, Loss: 0.0285, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 227, Loss: 0.0306, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 228, Loss: 0.0210, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 229, Loss: 0.0214, Accuracy@1: 76.56%, Accuracy@5: 98.44%\n",
      "Batch 230, Loss: 0.0402, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 231, Loss: 0.0385, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 232, Loss: 0.0475, Accuracy@1: 45.31%, Accuracy@5: 65.62%\n",
      "Batch 233, Loss: 0.0262, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 234, Loss: 0.0176, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 235, Loss: 0.0314, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 236, Loss: 0.0383, Accuracy@1: 46.88%, Accuracy@5: 81.25%\n",
      "Batch 237, Loss: 0.0430, Accuracy@1: 35.94%, Accuracy@5: 71.88%\n",
      "Batch 238, Loss: 0.0223, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 239, Loss: 0.0242, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 240, Loss: 0.0249, Accuracy@1: 68.75%, Accuracy@5: 96.88%\n",
      "Batch 241, Loss: 0.0297, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 242, Loss: 0.0396, Accuracy@1: 59.38%, Accuracy@5: 78.12%\n",
      "Batch 243, Loss: 0.0440, Accuracy@1: 34.38%, Accuracy@5: 73.44%\n",
      "Batch 244, Loss: 0.0350, Accuracy@1: 54.69%, Accuracy@5: 89.06%\n",
      "Batch 245, Loss: 0.0407, Accuracy@1: 64.06%, Accuracy@5: 82.81%\n",
      "Batch 246, Loss: 0.0385, Accuracy@1: 54.69%, Accuracy@5: 76.56%\n",
      "Batch 247, Loss: 0.0177, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 248, Loss: 0.0304, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 249, Loss: 0.0217, Accuracy@1: 67.19%, Accuracy@5: 93.75%\n",
      "Batch 250, Loss: 0.0164, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 251, Loss: 0.0181, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 252, Loss: 0.0226, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 253, Loss: 0.0166, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 254, Loss: 0.0224, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 255, Loss: 0.0220, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 256, Loss: 0.0277, Accuracy@1: 67.19%, Accuracy@5: 81.25%\n",
      "Batch 257, Loss: 0.0297, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 258, Loss: 0.0143, Accuracy@1: 78.12%, Accuracy@5: 95.31%\n",
      "Batch 259, Loss: 0.0196, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 260, Loss: 0.0146, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 261, Loss: 0.0401, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 262, Loss: 0.0260, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 263, Loss: 0.0270, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 264, Loss: 0.0192, Accuracy@1: 84.38%, Accuracy@5: 95.31%\n",
      "Batch 265, Loss: 0.0219, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 266, Loss: 0.0428, Accuracy@1: 54.69%, Accuracy@5: 71.88%\n",
      "Batch 267, Loss: 0.0380, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 268, Loss: 0.0262, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 269, Loss: 0.0343, Accuracy@1: 48.44%, Accuracy@5: 85.94%\n",
      "Batch 270, Loss: 0.0330, Accuracy@1: 56.25%, Accuracy@5: 84.38%\n",
      "Batch 271, Loss: 0.0163, Accuracy@1: 81.25%, Accuracy@5: 98.44%\n",
      "Batch 272, Loss: 0.0271, Accuracy@1: 51.56%, Accuracy@5: 87.50%\n",
      "Batch 273, Loss: 0.0227, Accuracy@1: 60.94%, Accuracy@5: 92.19%\n",
      "Batch 274, Loss: 0.0125, Accuracy@1: 89.06%, Accuracy@5: 100.00%\n",
      "Batch 275, Loss: 0.0184, Accuracy@1: 73.44%, Accuracy@5: 95.31%\n",
      "Batch 276, Loss: 0.0253, Accuracy@1: 70.31%, Accuracy@5: 90.62%\n",
      "Batch 277, Loss: 0.0243, Accuracy@1: 81.25%, Accuracy@5: 90.62%\n",
      "Batch 278, Loss: 0.0311, Accuracy@1: 51.56%, Accuracy@5: 92.19%\n",
      "Batch 279, Loss: 0.0446, Accuracy@1: 32.81%, Accuracy@5: 78.12%\n",
      "Batch 280, Loss: 0.0323, Accuracy@1: 45.31%, Accuracy@5: 92.19%\n",
      "Batch 281, Loss: 0.0281, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 282, Loss: 0.0383, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 283, Loss: 0.0372, Accuracy@1: 62.50%, Accuracy@5: 76.56%\n",
      "Batch 284, Loss: 0.0180, Accuracy@1: 85.94%, Accuracy@5: 95.31%\n",
      "Batch 285, Loss: 0.0295, Accuracy@1: 79.69%, Accuracy@5: 87.50%\n",
      "Batch 286, Loss: 0.0497, Accuracy@1: 45.31%, Accuracy@5: 70.31%\n",
      "Batch 287, Loss: 0.0422, Accuracy@1: 35.94%, Accuracy@5: 73.44%\n",
      "Batch 288, Loss: 0.0437, Accuracy@1: 48.44%, Accuracy@5: 71.88%\n",
      "Batch 289, Loss: 0.0301, Accuracy@1: 59.38%, Accuracy@5: 85.94%\n",
      "Batch 290, Loss: 0.0393, Accuracy@1: 46.88%, Accuracy@5: 75.00%\n",
      "Batch 291, Loss: 0.0394, Accuracy@1: 45.31%, Accuracy@5: 75.00%\n",
      "Batch 292, Loss: 0.0432, Accuracy@1: 34.38%, Accuracy@5: 65.62%\n",
      "Batch 293, Loss: 0.0209, Accuracy@1: 82.81%, Accuracy@5: 87.50%\n",
      "Batch 294, Loss: 0.0188, Accuracy@1: 81.25%, Accuracy@5: 96.88%\n",
      "Batch 295, Loss: 0.0301, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 296, Loss: 0.0251, Accuracy@1: 60.94%, Accuracy@5: 93.75%\n",
      "Batch 297, Loss: 0.0244, Accuracy@1: 57.81%, Accuracy@5: 95.31%\n",
      "Batch 298, Loss: 0.0339, Accuracy@1: 37.50%, Accuracy@5: 89.06%\n",
      "Batch 299, Loss: 0.0301, Accuracy@1: 51.56%, Accuracy@5: 90.62%\n",
      "Batch 300, Loss: 0.0255, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 301, Loss: 0.0354, Accuracy@1: 48.44%, Accuracy@5: 85.94%\n",
      "Batch 302, Loss: 0.0194, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 303, Loss: 0.0328, Accuracy@1: 78.12%, Accuracy@5: 85.94%\n",
      "Batch 304, Loss: 0.0234, Accuracy@1: 59.38%, Accuracy@5: 90.62%\n",
      "Batch 305, Loss: 0.0371, Accuracy@1: 51.56%, Accuracy@5: 76.56%\n",
      "Batch 306, Loss: 0.0264, Accuracy@1: 59.38%, Accuracy@5: 85.94%\n",
      "Batch 307, Loss: 0.0292, Accuracy@1: 53.12%, Accuracy@5: 89.06%\n",
      "Batch 308, Loss: 0.0331, Accuracy@1: 54.69%, Accuracy@5: 76.56%\n",
      "Batch 309, Loss: 0.0231, Accuracy@1: 76.56%, Accuracy@5: 82.81%\n",
      "Batch 310, Loss: 0.0205, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 311, Loss: 0.0451, Accuracy@1: 51.56%, Accuracy@5: 70.31%\n",
      "Batch 312, Loss: 0.0499, Accuracy@1: 25.00%, Accuracy@5: 62.50%\n",
      "Batch 313, Loss: 0.0422, Accuracy@1: 45.31%, Accuracy@5: 68.75%\n",
      "Batch 314, Loss: 0.0423, Accuracy@1: 28.12%, Accuracy@5: 73.44%\n",
      "Batch 315, Loss: 0.0268, Accuracy@1: 50.00%, Accuracy@5: 89.06%\n",
      "Batch 316, Loss: 0.0276, Accuracy@1: 67.19%, Accuracy@5: 82.81%\n",
      "Batch 317, Loss: 0.0349, Accuracy@1: 50.00%, Accuracy@5: 90.62%\n",
      "Batch 318, Loss: 0.0144, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 319, Loss: 0.0245, Accuracy@1: 62.50%, Accuracy@5: 93.75%\n",
      "Batch 320, Loss: 0.0201, Accuracy@1: 82.81%, Accuracy@5: 98.44%\n",
      "Batch 321, Loss: 0.0520, Accuracy@1: 31.25%, Accuracy@5: 60.94%\n",
      "Batch 322, Loss: 0.0426, Accuracy@1: 35.94%, Accuracy@5: 78.12%\n",
      "Batch 323, Loss: 0.0493, Accuracy@1: 39.06%, Accuracy@5: 71.88%\n",
      "Batch 324, Loss: 0.0630, Accuracy@1: 20.31%, Accuracy@5: 43.75%\n",
      "Batch 325, Loss: 0.0433, Accuracy@1: 40.62%, Accuracy@5: 71.88%\n",
      "Batch 326, Loss: 0.0365, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 327, Loss: 0.0552, Accuracy@1: 42.19%, Accuracy@5: 68.75%\n",
      "Batch 328, Loss: 0.0214, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 329, Loss: 0.0417, Accuracy@1: 45.31%, Accuracy@5: 82.81%\n",
      "Batch 330, Loss: 0.0278, Accuracy@1: 64.06%, Accuracy@5: 85.94%\n",
      "Batch 331, Loss: 0.0386, Accuracy@1: 54.69%, Accuracy@5: 82.81%\n",
      "Batch 332, Loss: 0.0266, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 333, Loss: 0.0318, Accuracy@1: 48.44%, Accuracy@5: 90.62%\n",
      "Batch 334, Loss: 0.0397, Accuracy@1: 51.56%, Accuracy@5: 81.25%\n",
      "Batch 335, Loss: 0.0300, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 336, Loss: 0.0224, Accuracy@1: 75.00%, Accuracy@5: 92.19%\n",
      "Batch 337, Loss: 0.0243, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 338, Loss: 0.0370, Accuracy@1: 50.00%, Accuracy@5: 78.12%\n",
      "Batch 339, Loss: 0.0512, Accuracy@1: 32.81%, Accuracy@5: 60.94%\n",
      "Batch 340, Loss: 0.0261, Accuracy@1: 53.12%, Accuracy@5: 90.62%\n",
      "Batch 341, Loss: 0.0299, Accuracy@1: 64.06%, Accuracy@5: 87.50%\n",
      "Batch 342, Loss: 0.0429, Accuracy@1: 43.75%, Accuracy@5: 71.88%\n",
      "Batch 343, Loss: 0.0343, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 344, Loss: 0.0373, Accuracy@1: 59.38%, Accuracy@5: 76.56%\n",
      "Batch 345, Loss: 0.0384, Accuracy@1: 48.44%, Accuracy@5: 85.94%\n",
      "Batch 346, Loss: 0.0416, Accuracy@1: 54.69%, Accuracy@5: 73.44%\n",
      "Batch 347, Loss: 0.0246, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 348, Loss: 0.0478, Accuracy@1: 37.50%, Accuracy@5: 70.31%\n",
      "Batch 349, Loss: 0.0545, Accuracy@1: 37.50%, Accuracy@5: 60.94%\n",
      "Batch 350, Loss: 0.0439, Accuracy@1: 51.56%, Accuracy@5: 76.56%\n",
      "Batch 351, Loss: 0.0309, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 352, Loss: 0.0268, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 353, Loss: 0.0515, Accuracy@1: 42.19%, Accuracy@5: 62.50%\n",
      "Batch 354, Loss: 0.0407, Accuracy@1: 53.12%, Accuracy@5: 78.12%\n",
      "Batch 355, Loss: 0.0386, Accuracy@1: 43.75%, Accuracy@5: 79.69%\n",
      "Batch 356, Loss: 0.0462, Accuracy@1: 40.62%, Accuracy@5: 71.88%\n",
      "Batch 357, Loss: 0.0347, Accuracy@1: 60.94%, Accuracy@5: 84.38%\n",
      "Batch 358, Loss: 0.0435, Accuracy@1: 60.94%, Accuracy@5: 76.56%\n",
      "Batch 359, Loss: 0.0423, Accuracy@1: 45.31%, Accuracy@5: 76.56%\n",
      "Batch 360, Loss: 0.0382, Accuracy@1: 45.31%, Accuracy@5: 82.81%\n",
      "Batch 361, Loss: 0.0502, Accuracy@1: 40.62%, Accuracy@5: 64.06%\n",
      "Batch 362, Loss: 0.0542, Accuracy@1: 34.38%, Accuracy@5: 59.38%\n",
      "Batch 363, Loss: 0.0514, Accuracy@1: 34.38%, Accuracy@5: 64.06%\n",
      "Batch 364, Loss: 0.0209, Accuracy@1: 79.69%, Accuracy@5: 93.75%\n",
      "Batch 365, Loss: 0.0255, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 366, Loss: 0.0440, Accuracy@1: 37.50%, Accuracy@5: 70.31%\n",
      "Batch 367, Loss: 0.0373, Accuracy@1: 62.50%, Accuracy@5: 78.12%\n",
      "Batch 368, Loss: 0.0292, Accuracy@1: 73.44%, Accuracy@5: 82.81%\n",
      "Batch 369, Loss: 0.0303, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 370, Loss: 0.0422, Accuracy@1: 59.38%, Accuracy@5: 76.56%\n",
      "Batch 371, Loss: 0.0217, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 372, Loss: 0.0290, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 373, Loss: 0.0597, Accuracy@1: 29.69%, Accuracy@5: 51.56%\n",
      "Batch 374, Loss: 0.0457, Accuracy@1: 35.94%, Accuracy@5: 71.88%\n",
      "Batch 375, Loss: 0.0515, Accuracy@1: 28.12%, Accuracy@5: 59.38%\n",
      "Batch 376, Loss: 0.0517, Accuracy@1: 17.19%, Accuracy@5: 60.94%\n",
      "Batch 377, Loss: 0.0373, Accuracy@1: 51.56%, Accuracy@5: 79.69%\n",
      "Batch 378, Loss: 0.0194, Accuracy@1: 70.31%, Accuracy@5: 95.31%\n",
      "Batch 379, Loss: 0.0367, Accuracy@1: 46.88%, Accuracy@5: 81.25%\n",
      "Batch 380, Loss: 0.0274, Accuracy@1: 64.06%, Accuracy@5: 87.50%\n",
      "Batch 381, Loss: 0.0564, Accuracy@1: 25.00%, Accuracy@5: 57.81%\n",
      "Batch 382, Loss: 0.0481, Accuracy@1: 51.56%, Accuracy@5: 71.88%\n",
      "Batch 383, Loss: 0.0415, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 384, Loss: 0.0494, Accuracy@1: 50.00%, Accuracy@5: 70.31%\n",
      "Batch 385, Loss: 0.0532, Accuracy@1: 37.50%, Accuracy@5: 56.25%\n",
      "Batch 386, Loss: 0.0409, Accuracy@1: 46.88%, Accuracy@5: 73.44%\n",
      "Batch 387, Loss: 0.0314, Accuracy@1: 68.75%, Accuracy@5: 82.81%\n",
      "Batch 388, Loss: 0.0344, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 389, Loss: 0.0565, Accuracy@1: 28.12%, Accuracy@5: 51.56%\n",
      "Batch 390, Loss: 0.0622, Accuracy@1: 37.50%, Accuracy@5: 51.56%\n",
      "Batch 391, Loss: 0.0526, Accuracy@1: 46.88%, Accuracy@5: 64.06%\n",
      "Batch 392, Loss: 0.0595, Accuracy@1: 26.56%, Accuracy@5: 57.81%\n",
      "Batch 393, Loss: 0.0483, Accuracy@1: 37.50%, Accuracy@5: 64.06%\n",
      "Batch 394, Loss: 0.0510, Accuracy@1: 34.38%, Accuracy@5: 64.06%\n",
      "Batch 395, Loss: 0.0414, Accuracy@1: 45.31%, Accuracy@5: 82.81%\n",
      "Batch 396, Loss: 0.0346, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 397, Loss: 0.0332, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 398, Loss: 0.0361, Accuracy@1: 53.12%, Accuracy@5: 71.88%\n",
      "Batch 399, Loss: 0.0269, Accuracy@1: 68.75%, Accuracy@5: 96.88%\n",
      "Batch 400, Loss: 0.0374, Accuracy@1: 50.00%, Accuracy@5: 84.38%\n",
      "Batch 401, Loss: 0.0431, Accuracy@1: 39.06%, Accuracy@5: 67.19%\n",
      "Batch 402, Loss: 0.0531, Accuracy@1: 45.31%, Accuracy@5: 60.94%\n",
      "Batch 403, Loss: 0.0451, Accuracy@1: 28.12%, Accuracy@5: 75.00%\n",
      "Batch 404, Loss: 0.0343, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 405, Loss: 0.0490, Accuracy@1: 46.88%, Accuracy@5: 76.56%\n",
      "Batch 406, Loss: 0.0389, Accuracy@1: 56.25%, Accuracy@5: 79.69%\n",
      "Batch 407, Loss: 0.0336, Accuracy@1: 68.75%, Accuracy@5: 81.25%\n",
      "Batch 408, Loss: 0.0351, Accuracy@1: 56.25%, Accuracy@5: 81.25%\n",
      "Batch 409, Loss: 0.0505, Accuracy@1: 28.12%, Accuracy@5: 60.94%\n",
      "Batch 410, Loss: 0.0460, Accuracy@1: 42.19%, Accuracy@5: 71.88%\n",
      "Batch 411, Loss: 0.0405, Accuracy@1: 45.31%, Accuracy@5: 70.31%\n",
      "Batch 412, Loss: 0.0284, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 413, Loss: 0.0446, Accuracy@1: 42.19%, Accuracy@5: 75.00%\n",
      "Batch 414, Loss: 0.0365, Accuracy@1: 56.25%, Accuracy@5: 75.00%\n",
      "Batch 415, Loss: 0.0391, Accuracy@1: 37.50%, Accuracy@5: 82.81%\n",
      "Batch 416, Loss: 0.0353, Accuracy@1: 68.75%, Accuracy@5: 87.50%\n",
      "Batch 417, Loss: 0.0330, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 418, Loss: 0.0241, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 419, Loss: 0.0158, Accuracy@1: 84.38%, Accuracy@5: 98.44%\n",
      "Batch 420, Loss: 0.0173, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 421, Loss: 0.0421, Accuracy@1: 54.69%, Accuracy@5: 84.38%\n",
      "Batch 422, Loss: 0.0374, Accuracy@1: 48.44%, Accuracy@5: 75.00%\n",
      "Batch 423, Loss: 0.0426, Accuracy@1: 45.31%, Accuracy@5: 75.00%\n",
      "Batch 424, Loss: 0.0444, Accuracy@1: 32.81%, Accuracy@5: 71.88%\n",
      "Batch 425, Loss: 0.0366, Accuracy@1: 51.56%, Accuracy@5: 71.88%\n",
      "Batch 426, Loss: 0.0251, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 427, Loss: 0.0158, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 428, Loss: 0.0254, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 429, Loss: 0.0586, Accuracy@1: 32.81%, Accuracy@5: 51.56%\n",
      "Batch 430, Loss: 0.0426, Accuracy@1: 51.56%, Accuracy@5: 78.12%\n",
      "Batch 431, Loss: 0.0249, Accuracy@1: 71.88%, Accuracy@5: 85.94%\n",
      "Batch 432, Loss: 0.0200, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 433, Loss: 0.0189, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 434, Loss: 0.0540, Accuracy@1: 37.50%, Accuracy@5: 57.81%\n",
      "Batch 435, Loss: 0.0412, Accuracy@1: 56.25%, Accuracy@5: 82.81%\n",
      "Batch 436, Loss: 0.0538, Accuracy@1: 28.12%, Accuracy@5: 53.12%\n",
      "Batch 437, Loss: 0.0263, Accuracy@1: 76.56%, Accuracy@5: 85.94%\n",
      "Batch 438, Loss: 0.0290, Accuracy@1: 68.75%, Accuracy@5: 82.81%\n",
      "Batch 439, Loss: 0.0233, Accuracy@1: 79.69%, Accuracy@5: 90.62%\n",
      "Batch 440, Loss: 0.0469, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 441, Loss: 0.0286, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 442, Loss: 0.0197, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 443, Loss: 0.0506, Accuracy@1: 34.38%, Accuracy@5: 59.38%\n",
      "Batch 444, Loss: 0.0304, Accuracy@1: 78.12%, Accuracy@5: 92.19%\n",
      "Batch 445, Loss: 0.0401, Accuracy@1: 54.69%, Accuracy@5: 76.56%\n",
      "Batch 446, Loss: 0.0294, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 447, Loss: 0.0247, Accuracy@1: 71.88%, Accuracy@5: 92.19%\n",
      "Batch 448, Loss: 0.0242, Accuracy@1: 73.44%, Accuracy@5: 92.19%\n",
      "Batch 449, Loss: 0.0215, Accuracy@1: 76.56%, Accuracy@5: 92.19%\n",
      "Batch 450, Loss: 0.0241, Accuracy@1: 68.75%, Accuracy@5: 87.50%\n",
      "Batch 451, Loss: 0.0337, Accuracy@1: 53.12%, Accuracy@5: 90.62%\n",
      "Batch 452, Loss: 0.0334, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 453, Loss: 0.0203, Accuracy@1: 78.12%, Accuracy@5: 93.75%\n",
      "Batch 454, Loss: 0.0370, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 455, Loss: 0.0429, Accuracy@1: 60.94%, Accuracy@5: 79.69%\n",
      "Batch 456, Loss: 0.0578, Accuracy@1: 35.94%, Accuracy@5: 59.38%\n",
      "Batch 457, Loss: 0.0521, Accuracy@1: 25.00%, Accuracy@5: 51.56%\n",
      "Batch 458, Loss: 0.0464, Accuracy@1: 42.19%, Accuracy@5: 65.62%\n",
      "Batch 459, Loss: 0.0434, Accuracy@1: 48.44%, Accuracy@5: 70.31%\n",
      "Batch 460, Loss: 0.0503, Accuracy@1: 40.62%, Accuracy@5: 60.94%\n",
      "Batch 461, Loss: 0.0408, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 462, Loss: 0.0345, Accuracy@1: 64.06%, Accuracy@5: 82.81%\n",
      "Batch 463, Loss: 0.0437, Accuracy@1: 53.12%, Accuracy@5: 68.75%\n",
      "Batch 464, Loss: 0.0272, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 465, Loss: 0.0393, Accuracy@1: 45.31%, Accuracy@5: 82.81%\n",
      "Batch 466, Loss: 0.0617, Accuracy@1: 46.88%, Accuracy@5: 53.12%\n",
      "Batch 467, Loss: 0.0420, Accuracy@1: 51.56%, Accuracy@5: 71.88%\n",
      "Batch 468, Loss: 0.0572, Accuracy@1: 28.12%, Accuracy@5: 64.06%\n",
      "Batch 469, Loss: 0.0527, Accuracy@1: 32.81%, Accuracy@5: 54.69%\n",
      "Batch 470, Loss: 0.0416, Accuracy@1: 51.56%, Accuracy@5: 73.44%\n",
      "Batch 471, Loss: 0.0220, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 472, Loss: 0.0222, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 473, Loss: 0.0278, Accuracy@1: 73.44%, Accuracy@5: 87.50%\n",
      "Batch 474, Loss: 0.0189, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 475, Loss: 0.0500, Accuracy@1: 51.56%, Accuracy@5: 70.31%\n",
      "Batch 476, Loss: 0.0315, Accuracy@1: 71.88%, Accuracy@5: 84.38%\n",
      "Batch 477, Loss: 0.0288, Accuracy@1: 68.75%, Accuracy@5: 84.38%\n",
      "Batch 478, Loss: 0.0232, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 479, Loss: 0.0353, Accuracy@1: 53.12%, Accuracy@5: 81.25%\n",
      "Batch 480, Loss: 0.0393, Accuracy@1: 56.25%, Accuracy@5: 73.44%\n",
      "Batch 481, Loss: 0.0467, Accuracy@1: 54.69%, Accuracy@5: 70.31%\n",
      "Batch 482, Loss: 0.0291, Accuracy@1: 70.31%, Accuracy@5: 84.38%\n",
      "Batch 483, Loss: 0.0561, Accuracy@1: 20.31%, Accuracy@5: 57.81%\n",
      "Batch 484, Loss: 0.0416, Accuracy@1: 28.12%, Accuracy@5: 81.25%\n",
      "Batch 485, Loss: 0.0256, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 486, Loss: 0.0599, Accuracy@1: 37.50%, Accuracy@5: 56.25%\n",
      "Batch 487, Loss: 0.0628, Accuracy@1: 26.56%, Accuracy@5: 43.75%\n",
      "Batch 488, Loss: 0.0209, Accuracy@1: 78.12%, Accuracy@5: 90.62%\n",
      "Batch 489, Loss: 0.0477, Accuracy@1: 37.50%, Accuracy@5: 67.19%\n",
      "Batch 490, Loss: 0.0296, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 491, Loss: 0.0384, Accuracy@1: 50.00%, Accuracy@5: 71.88%\n",
      "Batch 492, Loss: 0.0515, Accuracy@1: 39.06%, Accuracy@5: 70.31%\n",
      "Batch 493, Loss: 0.0452, Accuracy@1: 50.00%, Accuracy@5: 75.00%\n",
      "Batch 494, Loss: 0.0633, Accuracy@1: 25.00%, Accuracy@5: 45.31%\n",
      "Batch 495, Loss: 0.0696, Accuracy@1: 14.06%, Accuracy@5: 45.31%\n",
      "Batch 496, Loss: 0.0367, Accuracy@1: 50.00%, Accuracy@5: 76.56%\n",
      "Batch 497, Loss: 0.0510, Accuracy@1: 37.50%, Accuracy@5: 68.75%\n",
      "Batch 498, Loss: 0.0374, Accuracy@1: 28.12%, Accuracy@5: 79.69%\n",
      "Batch 499, Loss: 0.0314, Accuracy@1: 39.06%, Accuracy@5: 89.06%\n",
      "Batch 500, Loss: 0.0208, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 501, Loss: 0.0264, Accuracy@1: 64.06%, Accuracy@5: 90.62%\n",
      "Batch 502, Loss: 0.0390, Accuracy@1: 53.12%, Accuracy@5: 73.44%\n",
      "Batch 503, Loss: 0.0449, Accuracy@1: 40.62%, Accuracy@5: 71.88%\n",
      "Batch 504, Loss: 0.0097, Accuracy@1: 93.75%, Accuracy@5: 98.44%\n",
      "Batch 505, Loss: 0.0351, Accuracy@1: 67.19%, Accuracy@5: 78.12%\n",
      "Batch 506, Loss: 0.0419, Accuracy@1: 51.56%, Accuracy@5: 75.00%\n",
      "Batch 507, Loss: 0.0391, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 508, Loss: 0.0589, Accuracy@1: 26.56%, Accuracy@5: 54.69%\n",
      "Batch 509, Loss: 0.0422, Accuracy@1: 50.00%, Accuracy@5: 70.31%\n",
      "Batch 510, Loss: 0.0429, Accuracy@1: 46.88%, Accuracy@5: 70.31%\n",
      "Batch 511, Loss: 0.0302, Accuracy@1: 60.94%, Accuracy@5: 85.94%\n",
      "Batch 512, Loss: 0.0478, Accuracy@1: 39.06%, Accuracy@5: 78.12%\n",
      "Batch 513, Loss: 0.0493, Accuracy@1: 21.88%, Accuracy@5: 65.62%\n",
      "Batch 514, Loss: 0.0381, Accuracy@1: 54.69%, Accuracy@5: 85.94%\n",
      "Batch 515, Loss: 0.0307, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 516, Loss: 0.0314, Accuracy@1: 65.62%, Accuracy@5: 81.25%\n",
      "Batch 517, Loss: 0.0388, Accuracy@1: 56.25%, Accuracy@5: 81.25%\n",
      "Batch 518, Loss: 0.0419, Accuracy@1: 31.25%, Accuracy@5: 81.25%\n",
      "Batch 519, Loss: 0.0277, Accuracy@1: 59.38%, Accuracy@5: 93.75%\n",
      "Batch 520, Loss: 0.0410, Accuracy@1: 48.44%, Accuracy@5: 73.44%\n",
      "Batch 521, Loss: 0.0491, Accuracy@1: 46.88%, Accuracy@5: 71.88%\n",
      "Batch 522, Loss: 0.0229, Accuracy@1: 71.88%, Accuracy@5: 96.88%\n",
      "Batch 523, Loss: 0.0251, Accuracy@1: 67.19%, Accuracy@5: 92.19%\n",
      "Batch 524, Loss: 0.0268, Accuracy@1: 56.25%, Accuracy@5: 87.50%\n",
      "Batch 525, Loss: 0.0212, Accuracy@1: 81.25%, Accuracy@5: 93.75%\n",
      "Batch 526, Loss: 0.0466, Accuracy@1: 28.12%, Accuracy@5: 75.00%\n",
      "Batch 527, Loss: 0.0606, Accuracy@1: 26.56%, Accuracy@5: 43.75%\n",
      "Batch 528, Loss: 0.0644, Accuracy@1: 20.31%, Accuracy@5: 37.50%\n",
      "Batch 529, Loss: 0.0552, Accuracy@1: 45.31%, Accuracy@5: 56.25%\n",
      "Batch 530, Loss: 0.0277, Accuracy@1: 78.12%, Accuracy@5: 87.50%\n",
      "Batch 531, Loss: 0.0275, Accuracy@1: 76.56%, Accuracy@5: 95.31%\n",
      "Batch 532, Loss: 0.0335, Accuracy@1: 48.44%, Accuracy@5: 89.06%\n",
      "Batch 533, Loss: 0.0305, Accuracy@1: 64.06%, Accuracy@5: 84.38%\n",
      "Batch 534, Loss: 0.0437, Accuracy@1: 57.81%, Accuracy@5: 76.56%\n",
      "Batch 535, Loss: 0.0198, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 536, Loss: 0.0485, Accuracy@1: 39.06%, Accuracy@5: 62.50%\n",
      "Batch 537, Loss: 0.0250, Accuracy@1: 75.00%, Accuracy@5: 89.06%\n",
      "Batch 538, Loss: 0.0492, Accuracy@1: 31.25%, Accuracy@5: 60.94%\n",
      "Batch 539, Loss: 0.0324, Accuracy@1: 48.44%, Accuracy@5: 87.50%\n",
      "Batch 540, Loss: 0.0448, Accuracy@1: 46.88%, Accuracy@5: 68.75%\n",
      "Batch 541, Loss: 0.0463, Accuracy@1: 37.50%, Accuracy@5: 65.62%\n",
      "Batch 542, Loss: 0.0375, Accuracy@1: 59.38%, Accuracy@5: 82.81%\n",
      "Batch 543, Loss: 0.0448, Accuracy@1: 48.44%, Accuracy@5: 64.06%\n",
      "Batch 544, Loss: 0.0330, Accuracy@1: 68.75%, Accuracy@5: 85.94%\n",
      "Batch 545, Loss: 0.0303, Accuracy@1: 60.94%, Accuracy@5: 92.19%\n",
      "Batch 546, Loss: 0.0443, Accuracy@1: 48.44%, Accuracy@5: 64.06%\n",
      "Batch 547, Loss: 0.0457, Accuracy@1: 43.75%, Accuracy@5: 67.19%\n",
      "Batch 548, Loss: 0.0195, Accuracy@1: 73.44%, Accuracy@5: 93.75%\n",
      "Batch 549, Loss: 0.0417, Accuracy@1: 59.38%, Accuracy@5: 79.69%\n",
      "Batch 550, Loss: 0.0292, Accuracy@1: 62.50%, Accuracy@5: 90.62%\n",
      "Batch 551, Loss: 0.0437, Accuracy@1: 40.62%, Accuracy@5: 78.12%\n",
      "Batch 552, Loss: 0.0415, Accuracy@1: 48.44%, Accuracy@5: 76.56%\n",
      "Batch 553, Loss: 0.0424, Accuracy@1: 42.19%, Accuracy@5: 81.25%\n",
      "Batch 554, Loss: 0.0571, Accuracy@1: 25.00%, Accuracy@5: 54.69%\n",
      "Batch 555, Loss: 0.0585, Accuracy@1: 28.12%, Accuracy@5: 56.25%\n",
      "Batch 556, Loss: 0.0672, Accuracy@1: 18.75%, Accuracy@5: 39.06%\n",
      "Batch 557, Loss: 0.0206, Accuracy@1: 84.38%, Accuracy@5: 96.88%\n",
      "Batch 558, Loss: 0.0371, Accuracy@1: 54.69%, Accuracy@5: 84.38%\n",
      "Batch 559, Loss: 0.0262, Accuracy@1: 68.75%, Accuracy@5: 89.06%\n",
      "Batch 560, Loss: 0.0333, Accuracy@1: 57.81%, Accuracy@5: 85.94%\n",
      "Batch 561, Loss: 0.0417, Accuracy@1: 43.75%, Accuracy@5: 78.12%\n",
      "Batch 562, Loss: 0.0440, Accuracy@1: 46.88%, Accuracy@5: 73.44%\n",
      "Batch 563, Loss: 0.0520, Accuracy@1: 40.62%, Accuracy@5: 64.06%\n",
      "Batch 564, Loss: 0.0287, Accuracy@1: 68.75%, Accuracy@5: 85.94%\n",
      "Batch 565, Loss: 0.0261, Accuracy@1: 68.75%, Accuracy@5: 90.62%\n",
      "Batch 566, Loss: 0.0330, Accuracy@1: 56.25%, Accuracy@5: 81.25%\n",
      "Batch 567, Loss: 0.0413, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 568, Loss: 0.0330, Accuracy@1: 60.94%, Accuracy@5: 79.69%\n",
      "Batch 569, Loss: 0.0615, Accuracy@1: 31.25%, Accuracy@5: 50.00%\n",
      "Batch 570, Loss: 0.0300, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 571, Loss: 0.0426, Accuracy@1: 50.00%, Accuracy@5: 70.31%\n",
      "Batch 572, Loss: 0.0383, Accuracy@1: 59.38%, Accuracy@5: 75.00%\n",
      "Batch 573, Loss: 0.0366, Accuracy@1: 54.69%, Accuracy@5: 79.69%\n",
      "Batch 574, Loss: 0.0346, Accuracy@1: 54.69%, Accuracy@5: 81.25%\n",
      "Batch 575, Loss: 0.0259, Accuracy@1: 76.56%, Accuracy@5: 84.38%\n",
      "Batch 576, Loss: 0.0247, Accuracy@1: 68.75%, Accuracy@5: 98.44%\n",
      "Batch 577, Loss: 0.0163, Accuracy@1: 85.94%, Accuracy@5: 96.88%\n",
      "Batch 578, Loss: 0.0543, Accuracy@1: 34.38%, Accuracy@5: 53.12%\n",
      "Batch 579, Loss: 0.0533, Accuracy@1: 40.62%, Accuracy@5: 65.62%\n",
      "Batch 580, Loss: 0.0384, Accuracy@1: 54.69%, Accuracy@5: 87.50%\n",
      "Batch 581, Loss: 0.0437, Accuracy@1: 45.31%, Accuracy@5: 78.12%\n",
      "Batch 582, Loss: 0.0334, Accuracy@1: 65.62%, Accuracy@5: 81.25%\n",
      "Batch 583, Loss: 0.0401, Accuracy@1: 57.81%, Accuracy@5: 65.62%\n",
      "Batch 584, Loss: 0.0623, Accuracy@1: 20.31%, Accuracy@5: 53.12%\n",
      "Batch 585, Loss: 0.0509, Accuracy@1: 43.75%, Accuracy@5: 65.62%\n",
      "Batch 586, Loss: 0.0513, Accuracy@1: 35.94%, Accuracy@5: 71.88%\n",
      "Batch 587, Loss: 0.0377, Accuracy@1: 50.00%, Accuracy@5: 79.69%\n",
      "Batch 588, Loss: 0.0352, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 589, Loss: 0.0376, Accuracy@1: 51.56%, Accuracy@5: 84.38%\n",
      "Batch 590, Loss: 0.0153, Accuracy@1: 89.06%, Accuracy@5: 96.88%\n",
      "Batch 591, Loss: 0.0285, Accuracy@1: 70.31%, Accuracy@5: 82.81%\n",
      "Batch 592, Loss: 0.0455, Accuracy@1: 48.44%, Accuracy@5: 70.31%\n",
      "Batch 593, Loss: 0.0318, Accuracy@1: 60.94%, Accuracy@5: 87.50%\n",
      "Batch 594, Loss: 0.0333, Accuracy@1: 67.19%, Accuracy@5: 81.25%\n",
      "Batch 595, Loss: 0.0415, Accuracy@1: 50.00%, Accuracy@5: 78.12%\n",
      "Batch 596, Loss: 0.0352, Accuracy@1: 54.69%, Accuracy@5: 82.81%\n",
      "Batch 597, Loss: 0.0548, Accuracy@1: 26.56%, Accuracy@5: 62.50%\n",
      "Batch 598, Loss: 0.0478, Accuracy@1: 35.94%, Accuracy@5: 73.44%\n",
      "Batch 599, Loss: 0.0496, Accuracy@1: 43.75%, Accuracy@5: 67.19%\n",
      "Batch 600, Loss: 0.0361, Accuracy@1: 67.19%, Accuracy@5: 79.69%\n",
      "Batch 601, Loss: 0.0422, Accuracy@1: 53.12%, Accuracy@5: 76.56%\n",
      "Batch 602, Loss: 0.0314, Accuracy@1: 70.31%, Accuracy@5: 85.94%\n",
      "Batch 603, Loss: 0.0406, Accuracy@1: 53.12%, Accuracy@5: 75.00%\n",
      "Batch 604, Loss: 0.0402, Accuracy@1: 51.56%, Accuracy@5: 76.56%\n",
      "Batch 605, Loss: 0.0470, Accuracy@1: 43.75%, Accuracy@5: 75.00%\n",
      "Batch 606, Loss: 0.0382, Accuracy@1: 46.88%, Accuracy@5: 71.88%\n",
      "Batch 607, Loss: 0.0487, Accuracy@1: 50.00%, Accuracy@5: 68.75%\n",
      "Batch 608, Loss: 0.0295, Accuracy@1: 67.19%, Accuracy@5: 92.19%\n",
      "Batch 609, Loss: 0.0212, Accuracy@1: 67.19%, Accuracy@5: 95.31%\n",
      "Batch 610, Loss: 0.0284, Accuracy@1: 75.00%, Accuracy@5: 85.94%\n",
      "Batch 611, Loss: 0.0342, Accuracy@1: 32.81%, Accuracy@5: 76.56%\n",
      "Batch 612, Loss: 0.0389, Accuracy@1: 54.69%, Accuracy@5: 71.88%\n",
      "Batch 613, Loss: 0.0440, Accuracy@1: 53.12%, Accuracy@5: 71.88%\n",
      "Batch 614, Loss: 0.0342, Accuracy@1: 64.06%, Accuracy@5: 81.25%\n",
      "Batch 615, Loss: 0.0462, Accuracy@1: 54.69%, Accuracy@5: 75.00%\n",
      "Batch 616, Loss: 0.0281, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 617, Loss: 0.0318, Accuracy@1: 56.25%, Accuracy@5: 76.56%\n",
      "Batch 618, Loss: 0.0316, Accuracy@1: 57.81%, Accuracy@5: 85.94%\n",
      "Batch 619, Loss: 0.0365, Accuracy@1: 65.62%, Accuracy@5: 81.25%\n",
      "Batch 620, Loss: 0.0329, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 621, Loss: 0.0283, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 622, Loss: 0.0361, Accuracy@1: 60.94%, Accuracy@5: 73.44%\n",
      "Batch 623, Loss: 0.0460, Accuracy@1: 48.44%, Accuracy@5: 71.88%\n",
      "Batch 624, Loss: 0.0445, Accuracy@1: 56.25%, Accuracy@5: 71.88%\n",
      "Batch 625, Loss: 0.0179, Accuracy@1: 82.81%, Accuracy@5: 95.31%\n",
      "Batch 626, Loss: 0.0137, Accuracy@1: 82.81%, Accuracy@5: 98.44%\n",
      "Batch 627, Loss: 0.0223, Accuracy@1: 76.56%, Accuracy@5: 90.62%\n",
      "Batch 628, Loss: 0.0430, Accuracy@1: 37.50%, Accuracy@5: 71.88%\n",
      "Batch 629, Loss: 0.0414, Accuracy@1: 59.38%, Accuracy@5: 70.31%\n",
      "Batch 630, Loss: 0.0366, Accuracy@1: 59.38%, Accuracy@5: 81.25%\n",
      "Batch 631, Loss: 0.0498, Accuracy@1: 45.31%, Accuracy@5: 65.62%\n",
      "Batch 632, Loss: 0.0345, Accuracy@1: 57.81%, Accuracy@5: 85.94%\n",
      "Batch 633, Loss: 0.0429, Accuracy@1: 35.94%, Accuracy@5: 71.88%\n",
      "Batch 634, Loss: 0.0381, Accuracy@1: 53.12%, Accuracy@5: 78.12%\n",
      "Batch 635, Loss: 0.0625, Accuracy@1: 28.12%, Accuracy@5: 43.75%\n",
      "Batch 636, Loss: 0.0267, Accuracy@1: 65.62%, Accuracy@5: 92.19%\n",
      "Batch 637, Loss: 0.0277, Accuracy@1: 75.00%, Accuracy@5: 96.88%\n",
      "Batch 638, Loss: 0.0329, Accuracy@1: 56.25%, Accuracy@5: 85.94%\n",
      "Batch 639, Loss: 0.0476, Accuracy@1: 48.44%, Accuracy@5: 67.19%\n",
      "Batch 640, Loss: 0.0319, Accuracy@1: 65.62%, Accuracy@5: 87.50%\n",
      "Batch 641, Loss: 0.0247, Accuracy@1: 71.88%, Accuracy@5: 93.75%\n",
      "Batch 642, Loss: 0.0138, Accuracy@1: 85.94%, Accuracy@5: 93.75%\n",
      "Batch 643, Loss: 0.0395, Accuracy@1: 65.62%, Accuracy@5: 82.81%\n",
      "Batch 644, Loss: 0.0428, Accuracy@1: 64.06%, Accuracy@5: 85.94%\n",
      "Batch 645, Loss: 0.0362, Accuracy@1: 59.38%, Accuracy@5: 90.62%\n",
      "Batch 646, Loss: 0.0479, Accuracy@1: 37.50%, Accuracy@5: 68.75%\n",
      "Batch 647, Loss: 0.0372, Accuracy@1: 56.25%, Accuracy@5: 75.00%\n",
      "Batch 648, Loss: 0.0316, Accuracy@1: 64.06%, Accuracy@5: 81.25%\n",
      "Batch 649, Loss: 0.0539, Accuracy@1: 35.94%, Accuracy@5: 67.19%\n",
      "Batch 650, Loss: 0.0342, Accuracy@1: 57.81%, Accuracy@5: 81.25%\n",
      "Batch 651, Loss: 0.0537, Accuracy@1: 25.00%, Accuracy@5: 60.94%\n",
      "Batch 652, Loss: 0.0471, Accuracy@1: 43.75%, Accuracy@5: 70.31%\n",
      "Batch 653, Loss: 0.0579, Accuracy@1: 23.44%, Accuracy@5: 54.69%\n",
      "Batch 654, Loss: 0.0619, Accuracy@1: 12.50%, Accuracy@5: 53.12%\n",
      "Batch 655, Loss: 0.0491, Accuracy@1: 34.38%, Accuracy@5: 67.19%\n",
      "Batch 656, Loss: 0.0302, Accuracy@1: 60.94%, Accuracy@5: 90.62%\n",
      "Batch 657, Loss: 0.0537, Accuracy@1: 34.38%, Accuracy@5: 50.00%\n",
      "Batch 658, Loss: 0.0283, Accuracy@1: 62.50%, Accuracy@5: 87.50%\n",
      "Batch 659, Loss: 0.0374, Accuracy@1: 60.94%, Accuracy@5: 82.81%\n",
      "Batch 660, Loss: 0.0480, Accuracy@1: 45.31%, Accuracy@5: 65.62%\n",
      "Batch 661, Loss: 0.0359, Accuracy@1: 54.69%, Accuracy@5: 79.69%\n",
      "Batch 662, Loss: 0.0369, Accuracy@1: 37.50%, Accuracy@5: 75.00%\n",
      "Batch 663, Loss: 0.0337, Accuracy@1: 51.56%, Accuracy@5: 78.12%\n",
      "Batch 664, Loss: 0.0407, Accuracy@1: 56.25%, Accuracy@5: 85.94%\n",
      "Batch 665, Loss: 0.0397, Accuracy@1: 50.00%, Accuracy@5: 71.88%\n",
      "Batch 666, Loss: 0.0406, Accuracy@1: 54.69%, Accuracy@5: 78.12%\n",
      "Batch 667, Loss: 0.0459, Accuracy@1: 50.00%, Accuracy@5: 70.31%\n",
      "Batch 668, Loss: 0.0393, Accuracy@1: 51.56%, Accuracy@5: 73.44%\n",
      "Batch 669, Loss: 0.0502, Accuracy@1: 28.12%, Accuracy@5: 67.19%\n",
      "Batch 670, Loss: 0.0583, Accuracy@1: 29.69%, Accuracy@5: 60.94%\n",
      "Batch 671, Loss: 0.0460, Accuracy@1: 53.12%, Accuracy@5: 68.75%\n",
      "Batch 672, Loss: 0.0459, Accuracy@1: 37.50%, Accuracy@5: 68.75%\n",
      "Batch 673, Loss: 0.0318, Accuracy@1: 64.06%, Accuracy@5: 89.06%\n",
      "Batch 674, Loss: 0.0328, Accuracy@1: 67.19%, Accuracy@5: 78.12%\n",
      "Batch 675, Loss: 0.0456, Accuracy@1: 32.81%, Accuracy@5: 67.19%\n",
      "Batch 676, Loss: 0.0313, Accuracy@1: 70.31%, Accuracy@5: 89.06%\n",
      "Batch 677, Loss: 0.0333, Accuracy@1: 59.38%, Accuracy@5: 82.81%\n",
      "Batch 678, Loss: 0.0537, Accuracy@1: 37.50%, Accuracy@5: 62.50%\n",
      "Batch 679, Loss: 0.0365, Accuracy@1: 64.06%, Accuracy@5: 78.12%\n",
      "Batch 680, Loss: 0.0250, Accuracy@1: 45.31%, Accuracy@5: 96.88%\n",
      "Batch 681, Loss: 0.0377, Accuracy@1: 40.62%, Accuracy@5: 82.81%\n",
      "Batch 682, Loss: 0.0194, Accuracy@1: 82.81%, Accuracy@5: 92.19%\n",
      "Batch 683, Loss: 0.0150, Accuracy@1: 82.81%, Accuracy@5: 98.44%\n",
      "Batch 684, Loss: 0.0225, Accuracy@1: 53.12%, Accuracy@5: 93.75%\n",
      "Batch 685, Loss: 0.0326, Accuracy@1: 62.50%, Accuracy@5: 84.38%\n",
      "Batch 686, Loss: 0.0282, Accuracy@1: 54.69%, Accuracy@5: 87.50%\n",
      "Batch 687, Loss: 0.0331, Accuracy@1: 64.06%, Accuracy@5: 82.81%\n",
      "Batch 688, Loss: 0.0415, Accuracy@1: 48.44%, Accuracy@5: 79.69%\n",
      "Batch 689, Loss: 0.0422, Accuracy@1: 54.69%, Accuracy@5: 78.12%\n",
      "Batch 690, Loss: 0.0392, Accuracy@1: 54.69%, Accuracy@5: 79.69%\n",
      "Batch 691, Loss: 0.0630, Accuracy@1: 34.38%, Accuracy@5: 45.31%\n",
      "Batch 692, Loss: 0.0333, Accuracy@1: 65.62%, Accuracy@5: 81.25%\n",
      "Batch 693, Loss: 0.0461, Accuracy@1: 50.00%, Accuracy@5: 62.50%\n",
      "Batch 694, Loss: 0.0314, Accuracy@1: 64.06%, Accuracy@5: 79.69%\n",
      "Batch 695, Loss: 0.0224, Accuracy@1: 71.88%, Accuracy@5: 92.19%\n",
      "Batch 696, Loss: 0.0496, Accuracy@1: 46.88%, Accuracy@5: 62.50%\n",
      "Batch 697, Loss: 0.0440, Accuracy@1: 42.19%, Accuracy@5: 68.75%\n",
      "Batch 698, Loss: 0.0509, Accuracy@1: 37.50%, Accuracy@5: 65.62%\n",
      "Batch 699, Loss: 0.0292, Accuracy@1: 71.88%, Accuracy@5: 90.62%\n",
      "Batch 700, Loss: 0.0266, Accuracy@1: 54.69%, Accuracy@5: 95.31%\n",
      "Batch 701, Loss: 0.0275, Accuracy@1: 70.31%, Accuracy@5: 92.19%\n",
      "Batch 702, Loss: 0.0466, Accuracy@1: 28.12%, Accuracy@5: 73.44%\n",
      "Batch 703, Loss: 0.0210, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 704, Loss: 0.0312, Accuracy@1: 54.69%, Accuracy@5: 82.81%\n",
      "Batch 705, Loss: 0.0334, Accuracy@1: 71.88%, Accuracy@5: 85.94%\n",
      "Batch 706, Loss: 0.0362, Accuracy@1: 64.06%, Accuracy@5: 76.56%\n",
      "Batch 707, Loss: 0.0616, Accuracy@1: 26.56%, Accuracy@5: 59.38%\n",
      "Batch 708, Loss: 0.0596, Accuracy@1: 18.75%, Accuracy@5: 54.69%\n",
      "Batch 709, Loss: 0.0428, Accuracy@1: 42.19%, Accuracy@5: 71.88%\n",
      "Batch 710, Loss: 0.0436, Accuracy@1: 31.25%, Accuracy@5: 78.12%\n",
      "Batch 711, Loss: 0.0571, Accuracy@1: 21.88%, Accuracy@5: 60.94%\n",
      "Batch 712, Loss: 0.0381, Accuracy@1: 51.56%, Accuracy@5: 82.81%\n",
      "Batch 713, Loss: 0.0265, Accuracy@1: 67.19%, Accuracy@5: 90.62%\n",
      "Batch 714, Loss: 0.0164, Accuracy@1: 84.38%, Accuracy@5: 98.44%\n",
      "Batch 715, Loss: 0.0273, Accuracy@1: 71.88%, Accuracy@5: 87.50%\n",
      "Batch 716, Loss: 0.0278, Accuracy@1: 75.00%, Accuracy@5: 87.50%\n",
      "Batch 717, Loss: 0.0191, Accuracy@1: 84.38%, Accuracy@5: 93.75%\n",
      "Batch 718, Loss: 0.0374, Accuracy@1: 65.62%, Accuracy@5: 84.38%\n",
      "Batch 719, Loss: 0.0242, Accuracy@1: 79.69%, Accuracy@5: 92.19%\n",
      "Batch 720, Loss: 0.0323, Accuracy@1: 70.31%, Accuracy@5: 93.75%\n",
      "Batch 721, Loss: 0.0424, Accuracy@1: 50.00%, Accuracy@5: 85.94%\n",
      "Batch 722, Loss: 0.0326, Accuracy@1: 59.38%, Accuracy@5: 87.50%\n",
      "Batch 723, Loss: 0.0358, Accuracy@1: 51.56%, Accuracy@5: 85.94%\n",
      "Batch 724, Loss: 0.0226, Accuracy@1: 82.81%, Accuracy@5: 93.75%\n",
      "Batch 725, Loss: 0.0555, Accuracy@1: 17.19%, Accuracy@5: 62.50%\n",
      "Batch 726, Loss: 0.0597, Accuracy@1: 35.94%, Accuracy@5: 50.00%\n",
      "Batch 727, Loss: 0.0619, Accuracy@1: 18.75%, Accuracy@5: 54.69%\n",
      "Batch 728, Loss: 0.0432, Accuracy@1: 53.12%, Accuracy@5: 75.00%\n",
      "Batch 729, Loss: 0.0501, Accuracy@1: 45.31%, Accuracy@5: 71.88%\n",
      "Batch 730, Loss: 0.0453, Accuracy@1: 50.00%, Accuracy@5: 65.62%\n",
      "Batch 731, Loss: 0.0345, Accuracy@1: 65.62%, Accuracy@5: 90.62%\n",
      "Batch 732, Loss: 0.0160, Accuracy@1: 87.50%, Accuracy@5: 98.44%\n",
      "Batch 733, Loss: 0.0258, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 734, Loss: 0.0418, Accuracy@1: 46.88%, Accuracy@5: 79.69%\n",
      "Batch 735, Loss: 0.0492, Accuracy@1: 37.50%, Accuracy@5: 62.50%\n",
      "Batch 736, Loss: 0.0542, Accuracy@1: 35.94%, Accuracy@5: 67.19%\n",
      "Batch 737, Loss: 0.0458, Accuracy@1: 46.88%, Accuracy@5: 75.00%\n",
      "Batch 738, Loss: 0.0394, Accuracy@1: 60.94%, Accuracy@5: 84.38%\n",
      "Batch 739, Loss: 0.0288, Accuracy@1: 73.44%, Accuracy@5: 90.62%\n",
      "Batch 740, Loss: 0.0321, Accuracy@1: 51.56%, Accuracy@5: 90.62%\n",
      "Batch 741, Loss: 0.0530, Accuracy@1: 45.31%, Accuracy@5: 62.50%\n",
      "Batch 742, Loss: 0.0448, Accuracy@1: 48.44%, Accuracy@5: 79.69%\n",
      "Batch 743, Loss: 0.0341, Accuracy@1: 73.44%, Accuracy@5: 89.06%\n",
      "Batch 744, Loss: 0.0421, Accuracy@1: 51.56%, Accuracy@5: 75.00%\n",
      "Batch 745, Loss: 0.0461, Accuracy@1: 54.69%, Accuracy@5: 79.69%\n",
      "Batch 746, Loss: 0.0275, Accuracy@1: 84.38%, Accuracy@5: 92.19%\n",
      "Batch 747, Loss: 0.0314, Accuracy@1: 75.00%, Accuracy@5: 90.62%\n",
      "Batch 748, Loss: 0.0391, Accuracy@1: 65.62%, Accuracy@5: 89.06%\n",
      "Batch 749, Loss: 0.0347, Accuracy@1: 67.19%, Accuracy@5: 84.38%\n",
      "Batch 750, Loss: 0.0558, Accuracy@1: 37.50%, Accuracy@5: 62.50%\n",
      "Batch 751, Loss: 0.0601, Accuracy@1: 14.06%, Accuracy@5: 56.25%\n",
      "Batch 752, Loss: 0.0477, Accuracy@1: 56.25%, Accuracy@5: 75.00%\n",
      "Batch 753, Loss: 0.0511, Accuracy@1: 37.50%, Accuracy@5: 73.44%\n",
      "Batch 754, Loss: 0.0565, Accuracy@1: 31.25%, Accuracy@5: 64.06%\n",
      "Batch 755, Loss: 0.0504, Accuracy@1: 35.94%, Accuracy@5: 62.50%\n",
      "Batch 756, Loss: 0.0511, Accuracy@1: 28.12%, Accuracy@5: 64.06%\n",
      "Batch 757, Loss: 0.0515, Accuracy@1: 39.06%, Accuracy@5: 64.06%\n",
      "Batch 758, Loss: 0.0300, Accuracy@1: 60.94%, Accuracy@5: 87.50%\n",
      "Batch 759, Loss: 0.0355, Accuracy@1: 53.12%, Accuracy@5: 85.94%\n",
      "Batch 760, Loss: 0.0241, Accuracy@1: 71.88%, Accuracy@5: 98.44%\n",
      "Batch 761, Loss: 0.0214, Accuracy@1: 71.88%, Accuracy@5: 89.06%\n",
      "Batch 762, Loss: 0.0423, Accuracy@1: 35.94%, Accuracy@5: 81.25%\n",
      "Batch 763, Loss: 0.0287, Accuracy@1: 56.25%, Accuracy@5: 96.88%\n",
      "Batch 764, Loss: 0.0323, Accuracy@1: 62.50%, Accuracy@5: 89.06%\n",
      "Batch 765, Loss: 0.0279, Accuracy@1: 76.56%, Accuracy@5: 93.75%\n",
      "Batch 766, Loss: 0.0358, Accuracy@1: 60.94%, Accuracy@5: 81.25%\n",
      "Batch 767, Loss: 0.0377, Accuracy@1: 62.50%, Accuracy@5: 81.25%\n",
      "Batch 768, Loss: 0.0144, Accuracy@1: 84.38%, Accuracy@5: 98.44%\n",
      "Batch 769, Loss: 0.0238, Accuracy@1: 82.81%, Accuracy@5: 90.62%\n",
      "Batch 770, Loss: 0.0201, Accuracy@1: 90.62%, Accuracy@5: 98.44%\n",
      "Batch 771, Loss: 0.0464, Accuracy@1: 31.25%, Accuracy@5: 67.19%\n",
      "Batch 772, Loss: 0.0254, Accuracy@1: 81.25%, Accuracy@5: 92.19%\n",
      "Batch 773, Loss: 0.0308, Accuracy@1: 75.00%, Accuracy@5: 93.75%\n",
      "Batch 774, Loss: 0.0214, Accuracy@1: 87.50%, Accuracy@5: 95.31%\n",
      "Batch 775, Loss: 0.0142, Accuracy@1: 87.50%, Accuracy@5: 96.88%\n",
      "Batch 776, Loss: 0.0132, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 777, Loss: 0.0159, Accuracy@1: 89.06%, Accuracy@5: 98.44%\n",
      "Batch 778, Loss: 0.0265, Accuracy@1: 81.25%, Accuracy@5: 95.31%\n",
      "Batch 779, Loss: 0.0449, Accuracy@1: 37.50%, Accuracy@5: 75.00%\n",
      "Batch 780, Loss: 0.0554, Accuracy@1: 31.25%, Accuracy@5: 57.81%\n",
      "Batch 781, Loss: 0.2736, Accuracy@1: 12.50%, Accuracy@5: 43.75%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m accuracy_top1, accuracy_top5, losses \u001B[38;5;241m=\u001B[39m evaluate(model, DEVICE, test_dataloader)\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43macc\u001B[49m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'acc' is not defined"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T01:20:49.683222Z",
     "start_time": "2024-12-04T01:20:49.662925Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"acc@1: {accuracy_top1*100}%, acc@5: {accuracy_top5*100}%, loss: {losses}\")",
   "id": "b9b655a0cfe00a3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc@1: 59.16400000000001%, acc@5: 82.32199999999999%, loss: 26.93688239902258\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T12:31:46.828146Z",
     "start_time": "2024-12-27T12:31:46.812667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "test = {\"model\": {}, \"hardware\": {}}\n",
    "with open(\"./metrics.json\", \"w\") as f:\n",
    "    json.dump(test, f, indent=2)"
   ],
   "id": "2dedd0dc720650e7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling-based Pruning (Channel-Level Pruning)",
   "id": "bd56797a602ef8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:26:56.961562Z",
     "start_time": "2024-12-03T16:26:56.955728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for parameter in model.named_parameters():\n",
    "    # print(parameter)\n",
    "    # print(parameter.size())\n",
    "    print(parameter[0], parameter[1].shape)"
   ],
   "id": "23e4e7ca8ad4e42f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight torch.Size([32, 3, 3, 3])\n",
      "features.0.1.weight torch.Size([32])\n",
      "features.0.1.bias torch.Size([32])\n",
      "features.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
      "features.1.conv.0.1.weight torch.Size([32])\n",
      "features.1.conv.0.1.bias torch.Size([32])\n",
      "features.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
      "features.1.conv.2.weight torch.Size([16])\n",
      "features.1.conv.2.bias torch.Size([16])\n",
      "features.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
      "features.2.conv.0.1.weight torch.Size([96])\n",
      "features.2.conv.0.1.bias torch.Size([96])\n",
      "features.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
      "features.2.conv.1.1.weight torch.Size([96])\n",
      "features.2.conv.1.1.bias torch.Size([96])\n",
      "features.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
      "features.2.conv.3.weight torch.Size([24])\n",
      "features.2.conv.3.bias torch.Size([24])\n",
      "features.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "features.3.conv.0.1.weight torch.Size([144])\n",
      "features.3.conv.0.1.bias torch.Size([144])\n",
      "features.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "features.3.conv.1.1.weight torch.Size([144])\n",
      "features.3.conv.1.1.bias torch.Size([144])\n",
      "features.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
      "features.3.conv.3.weight torch.Size([24])\n",
      "features.3.conv.3.bias torch.Size([24])\n",
      "features.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "features.4.conv.0.1.weight torch.Size([144])\n",
      "features.4.conv.0.1.bias torch.Size([144])\n",
      "features.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "features.4.conv.1.1.weight torch.Size([144])\n",
      "features.4.conv.1.1.bias torch.Size([144])\n",
      "features.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
      "features.4.conv.3.weight torch.Size([32])\n",
      "features.4.conv.3.bias torch.Size([32])\n",
      "features.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "features.5.conv.0.1.weight torch.Size([192])\n",
      "features.5.conv.0.1.bias torch.Size([192])\n",
      "features.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "features.5.conv.1.1.weight torch.Size([192])\n",
      "features.5.conv.1.1.bias torch.Size([192])\n",
      "features.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "features.5.conv.3.weight torch.Size([32])\n",
      "features.5.conv.3.bias torch.Size([32])\n",
      "features.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "features.6.conv.0.1.weight torch.Size([192])\n",
      "features.6.conv.0.1.bias torch.Size([192])\n",
      "features.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "features.6.conv.1.1.weight torch.Size([192])\n",
      "features.6.conv.1.1.bias torch.Size([192])\n",
      "features.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "features.6.conv.3.weight torch.Size([32])\n",
      "features.6.conv.3.bias torch.Size([32])\n",
      "features.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "features.7.conv.0.1.weight torch.Size([192])\n",
      "features.7.conv.0.1.bias torch.Size([192])\n",
      "features.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "features.7.conv.1.1.weight torch.Size([192])\n",
      "features.7.conv.1.1.bias torch.Size([192])\n",
      "features.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
      "features.7.conv.3.weight torch.Size([64])\n",
      "features.7.conv.3.bias torch.Size([64])\n",
      "features.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "features.8.conv.0.1.weight torch.Size([384])\n",
      "features.8.conv.0.1.bias torch.Size([384])\n",
      "features.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "features.8.conv.1.1.weight torch.Size([384])\n",
      "features.8.conv.1.1.bias torch.Size([384])\n",
      "features.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "features.8.conv.3.weight torch.Size([64])\n",
      "features.8.conv.3.bias torch.Size([64])\n",
      "features.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "features.9.conv.0.1.weight torch.Size([384])\n",
      "features.9.conv.0.1.bias torch.Size([384])\n",
      "features.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "features.9.conv.1.1.weight torch.Size([384])\n",
      "features.9.conv.1.1.bias torch.Size([384])\n",
      "features.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "features.9.conv.3.weight torch.Size([64])\n",
      "features.9.conv.3.bias torch.Size([64])\n",
      "features.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "features.10.conv.0.1.weight torch.Size([384])\n",
      "features.10.conv.0.1.bias torch.Size([384])\n",
      "features.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "features.10.conv.1.1.weight torch.Size([384])\n",
      "features.10.conv.1.1.bias torch.Size([384])\n",
      "features.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "features.10.conv.3.weight torch.Size([64])\n",
      "features.10.conv.3.bias torch.Size([64])\n",
      "features.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "features.11.conv.0.1.weight torch.Size([384])\n",
      "features.11.conv.0.1.bias torch.Size([384])\n",
      "features.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "features.11.conv.1.1.weight torch.Size([384])\n",
      "features.11.conv.1.1.bias torch.Size([384])\n",
      "features.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
      "features.11.conv.3.weight torch.Size([96])\n",
      "features.11.conv.3.bias torch.Size([96])\n",
      "features.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "features.12.conv.0.1.weight torch.Size([576])\n",
      "features.12.conv.0.1.bias torch.Size([576])\n",
      "features.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "features.12.conv.1.1.weight torch.Size([576])\n",
      "features.12.conv.1.1.bias torch.Size([576])\n",
      "features.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "features.12.conv.3.weight torch.Size([96])\n",
      "features.12.conv.3.bias torch.Size([96])\n",
      "features.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "features.13.conv.0.1.weight torch.Size([576])\n",
      "features.13.conv.0.1.bias torch.Size([576])\n",
      "features.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "features.13.conv.1.1.weight torch.Size([576])\n",
      "features.13.conv.1.1.bias torch.Size([576])\n",
      "features.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "features.13.conv.3.weight torch.Size([96])\n",
      "features.13.conv.3.bias torch.Size([96])\n",
      "features.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "features.14.conv.0.1.weight torch.Size([576])\n",
      "features.14.conv.0.1.bias torch.Size([576])\n",
      "features.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "features.14.conv.1.1.weight torch.Size([576])\n",
      "features.14.conv.1.1.bias torch.Size([576])\n",
      "features.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
      "features.14.conv.3.weight torch.Size([160])\n",
      "features.14.conv.3.bias torch.Size([160])\n",
      "features.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "features.15.conv.0.1.weight torch.Size([960])\n",
      "features.15.conv.0.1.bias torch.Size([960])\n",
      "features.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "features.15.conv.1.1.weight torch.Size([960])\n",
      "features.15.conv.1.1.bias torch.Size([960])\n",
      "features.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "features.15.conv.3.weight torch.Size([160])\n",
      "features.15.conv.3.bias torch.Size([160])\n",
      "features.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "features.16.conv.0.1.weight torch.Size([960])\n",
      "features.16.conv.0.1.bias torch.Size([960])\n",
      "features.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "features.16.conv.1.1.weight torch.Size([960])\n",
      "features.16.conv.1.1.bias torch.Size([960])\n",
      "features.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "features.16.conv.3.weight torch.Size([160])\n",
      "features.16.conv.3.bias torch.Size([160])\n",
      "features.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "features.17.conv.0.1.weight torch.Size([960])\n",
      "features.17.conv.0.1.bias torch.Size([960])\n",
      "features.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "features.17.conv.1.1.weight torch.Size([960])\n",
      "features.17.conv.1.1.bias torch.Size([960])\n",
      "features.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
      "features.17.conv.3.weight torch.Size([320])\n",
      "features.17.conv.3.bias torch.Size([320])\n",
      "features.18.0.weight torch.Size([1280, 320, 1, 1])\n",
      "features.18.1.weight torch.Size([1280])\n",
      "features.18.1.bias torch.Size([1280])\n",
      "classifier.1.weight torch.Size([1000, 1280])\n",
      "classifier.1.bias torch.Size([1000])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T16:27:48.841551Z",
     "start_time": "2024-12-03T16:27:48.829849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter[0], parameter[1].shape)\n",
    "    break"
   ],
   "id": "7148465dac3c3c81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0132, -0.0043,  0.0148],\n",
      "         [ 0.0328, -0.0254,  0.0069],\n",
      "         [ 0.0105, -0.0373, -0.0147]],\n",
      "\n",
      "        [[ 0.0080, -0.0059,  0.0151],\n",
      "         [ 0.0200, -0.0329, -0.0021],\n",
      "         [ 0.0114, -0.0330, -0.0079]],\n",
      "\n",
      "        [[-0.0252, -0.0202, -0.0100],\n",
      "         [-0.0112, -0.0293, -0.0152],\n",
      "         [-0.0265, -0.0334, -0.0242]]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T17:20:50.479902Z",
     "start_time": "2024-12-03T17:20:50.440098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Iterate through all layers of the model\n",
    "batchnorm_params = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        batchnorm_params.append((name, module))\n",
    "\n",
    "# Print the names and parameters of BatchNorm layers\n",
    "count = 0\n",
    "for name, module in batchnorm_params:\n",
    "    print(f\"Layer: {name}\")\n",
    "    print(f\"Weight: {module.weight.data}\")\n",
    "    print(f\"Bias: {module.bias.data}\")\n",
    "    print()\n",
    "    \n",
    "    count += 1\n",
    "    if count==1:\n",
    "        break   \n"
   ],
   "id": "a583f6d85a33070f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.1\n",
      "Weight: tensor([0.0381, 0.1872, 0.1975, 0.2451, 0.1313, 0.1590, 0.0881, 0.2552, 0.0870,\n",
      "        0.0119, 0.4129, 0.1137, 0.2245, 0.3014, 0.0114, 0.0104, 0.0128, 0.0043,\n",
      "        0.0668, 0.4108, 0.3578, 0.1278, 0.5250, 0.0039, 0.4444, 0.1298, 0.3284,\n",
      "        0.2453, 0.3565, 0.3066, 0.4146, 0.1419], device='cuda:0')\n",
      "Bias: tensor([-8.4354e-02,  5.6023e-01,  3.5002e-01,  2.8363e-01,  9.7327e-01,\n",
      "         6.4774e-01,  4.9481e-01,  5.5817e-01,  6.1756e-01, -4.2980e-04,\n",
      "        -3.0858e-01,  9.5334e-01,  4.4609e-01, -3.8414e-01, -9.3045e-04,\n",
      "         5.7470e-03, -4.2064e-02, -1.7965e-02,  3.3821e-01,  1.1017e-01,\n",
      "        -2.5284e-01,  5.0251e-01,  3.7990e-01, -1.5532e-02, -4.6869e-01,\n",
      "         5.1056e-01, -2.8880e-01,  6.4006e-01, -1.0935e-01, -5.9483e-02,\n",
      "         3.7479e-01,  2.6511e-01], device='cuda:0')\n",
      "\n",
      "Layer: features.1.conv.0.1\n",
      "Weight: tensor([0.0258, 0.5501, 0.7154, 0.4909, 1.1534, 0.4905, 0.5998, 0.8480, 0.5139,\n",
      "        0.0061, 0.2110, 1.0502, 0.6857, 0.2376, 0.0059, 0.0049, 0.0085, 0.0050,\n",
      "        0.5734, 0.1952, 0.2612, 0.2812, 0.2269, 0.0084, 0.0887, 1.0206, 0.2726,\n",
      "        0.2608, 0.1285, 0.1769, 0.2043, 0.4029], device='cuda:0')\n",
      "Bias: tensor([-1.1896e-02,  9.3495e-01,  1.4576e+00,  4.8383e-01,  2.3977e-01,\n",
      "         1.0311e+00,  9.5301e-01,  4.0770e-01,  5.6844e-01, -4.8085e-03,\n",
      "         3.6246e-01,  2.0216e-01,  1.0718e+00, -3.6395e-01, -5.7587e-03,\n",
      "        -5.8622e-03,  8.3305e-03,  1.3609e-02, -6.2070e-02,  1.7487e-01,\n",
      "        -1.9538e-01,  8.5177e-01,  1.0148e+00,  1.2245e-03,  7.4337e-01,\n",
      "         1.7794e+00, -2.7268e-01,  1.2048e+00,  6.1641e-01,  1.8691e-01,\n",
      "         1.0905e+00, -1.9219e-01], device='cuda:0')\n",
      "\n",
      "Layer: features.1.conv.2\n",
      "Weight: tensor([0.5126, 0.5345, 0.5721, 0.4572, 0.5695, 0.6116, 0.6057, 0.5404, 0.4620,\n",
      "        0.5511, 0.4132, 0.5557, 0.6912, 0.5953, 0.6496, 0.5285],\n",
      "       device='cuda:0')\n",
      "Bias: tensor([ 7.9770e-07, -1.2473e-07, -3.3166e-07,  1.7660e-07,  5.2644e-07,\n",
      "        -1.1789e-07, -6.9996e-08,  9.6487e-07, -6.4538e-07, -7.6193e-08,\n",
      "        -6.1555e-07,  3.6105e-08,  7.8038e-08, -1.5054e-06, -8.3980e-07,\n",
      "         3.4930e-08], device='cuda:0')\n",
      "\n",
      "Layer: features.2.conv.0.1\n",
      "Weight: tensor([0.4781, 0.3555, 0.1179, 0.2020, 0.1545, 0.1932, 0.2850, 0.1157, 0.4600,\n",
      "        0.1388, 0.2101, 0.3857, 0.0740, 0.0939, 0.1435, 0.4553, 0.1762, 0.1291,\n",
      "        0.3302, 0.2201, 0.1488, 0.1329, 0.1036, 0.0965, 0.2142, 0.4674, 0.1710,\n",
      "        0.2755, 0.1164, 0.0880, 0.5032, 0.0862, 0.1530, 0.3188, 0.4420, 0.2030,\n",
      "        0.2694, 0.3427, 0.0959, 0.3549, 0.4764, 0.1192, 0.1446, 0.0982, 0.1568,\n",
      "        0.4687, 0.3017, 0.1512, 0.1095, 0.0791, 0.2689, 0.4144, 0.4867, 0.2503,\n",
      "        0.2136, 0.1161, 0.4364, 0.1640, 0.1549, 0.3209, 0.4129, 0.4301, 0.0811,\n",
      "        0.1398, 0.5203, 0.1206, 0.1202, 0.3221, 0.2256, 0.1752, 0.1461, 0.5356,\n",
      "        0.0292, 0.2363, 0.1280, 0.2230, 0.2935, 0.2700, 0.1223, 0.2562, 0.5363,\n",
      "        0.2602, 0.2524, 0.4126, 0.5834, 0.5720, 0.0983, 0.4992, 0.2753, 0.1621,\n",
      "        0.0913, 0.5380, 0.4304, 0.3548, 0.1390, 0.1423], device='cuda:0')\n",
      "Bias: tensor([ 1.5737e-02, -1.2232e-02,  2.4170e-01,  3.6565e-01,  2.7892e-01,\n",
      "         1.6961e-01,  8.5801e-02,  6.8653e-02, -4.7721e-02,  1.4869e-01,\n",
      "        -5.9781e-02,  5.1204e-02,  2.0294e-01,  2.2977e-01,  2.0617e-01,\n",
      "         1.2868e-02, -4.7105e-02,  3.0441e-01,  8.2332e-02,  3.4854e-01,\n",
      "         4.1094e-01,  1.4778e-01,  2.7136e-01,  2.9415e-01,  8.0138e-02,\n",
      "        -1.3986e-03,  9.6997e-02, -3.3425e-01,  2.7630e-01,  2.4107e-01,\n",
      "        -8.9781e-03,  2.3318e-01,  1.6361e-01,  2.1910e-02, -7.5195e-03,\n",
      "         1.7180e-01,  1.1713e-01,  9.4551e-02,  2.7588e-01,  7.8572e-02,\n",
      "         2.7519e-02,  2.1859e-01,  2.7045e-01,  2.3398e-01,  1.6900e-01,\n",
      "         3.5872e-03,  2.2214e-01,  1.7671e-01,  2.2882e-01,  1.9106e-01,\n",
      "        -4.5695e-01, -6.7571e-02,  6.9704e-02,  7.1846e-04, -7.1883e-02,\n",
      "         2.5025e-01, -5.7695e-03,  7.2786e-01,  2.5297e-01, -1.8486e-03,\n",
      "         2.3194e-04, -1.2452e-02,  2.4255e-01,  2.5975e-01, -2.7696e-02,\n",
      "         2.3329e-01,  3.4012e-01, -4.1562e-03, -5.9648e-03,  9.6649e-02,\n",
      "         1.5315e-01,  1.7621e-01,  3.5270e-01,  3.4143e-02,  2.0475e-01,\n",
      "         1.7198e-01,  5.7062e-02,  5.5225e-02,  3.7398e-01,  1.1676e-01,\n",
      "        -4.1456e-02,  1.1347e-01,  3.8942e-02,  2.2273e-02,  7.7098e-02,\n",
      "        -5.5118e-02,  2.1786e-01,  2.7153e-02,  7.1733e-02,  1.5787e-01,\n",
      "         2.4351e-01, -7.4056e-02,  1.2476e-02, -6.7238e-02,  2.9533e-01,\n",
      "         1.3465e-01], device='cuda:0')\n",
      "\n",
      "Layer: features.2.conv.1.1\n",
      "Weight: tensor([0.5391, 0.3177, 0.3604, 0.2582, 0.3349, 0.3266, 0.1795, 0.3357, 0.2887,\n",
      "        0.2787, 0.1519, 0.4542, 0.3939, 0.2721, 0.2452, 0.4422, 0.2300, 0.2140,\n",
      "        0.4919, 0.4352, 0.2535, 0.2655, 0.4116, 0.4524, 0.3137, 0.3486, 0.3223,\n",
      "        0.1529, 0.2158, 0.3583, 0.5199, 0.2295, 0.3345, 0.2113, 0.2147, 0.3630,\n",
      "        0.1833, 0.3233, 0.4100, 0.2948, 0.4323, 0.2317, 0.3387, 0.3594, 0.3378,\n",
      "        0.2930, 0.3511, 0.2573, 0.2544, 0.3818, 0.1223, 0.2180, 0.4859, 0.2145,\n",
      "        0.1048, 0.4749, 0.3160, 0.2324, 0.3841, 0.3785, 0.3022, 0.2806, 0.3982,\n",
      "        0.2398, 0.5745, 0.4191, 0.2017, 0.1983, 0.2757, 0.3292, 0.2023, 0.8416,\n",
      "        0.4368, 0.1372, 0.3202, 0.2254, 0.3342, 0.1385, 0.4564, 0.3375, 0.4981,\n",
      "        0.1533, 0.2968, 0.3208, 0.6602, 0.3149, 0.2399, 0.3461, 0.1877, 0.3553,\n",
      "        0.4089, 0.4260, 0.2203, 0.1754, 0.2150, 0.3698], device='cuda:0')\n",
      "Bias: tensor([-5.5673e-02,  1.2691e-01,  1.6497e-02,  5.8970e-01,  1.2762e-01,\n",
      "         1.0478e-01,  7.2812e-01, -4.3023e-01,  2.8668e-01, -3.5511e-03,\n",
      "         6.5929e-02,  9.3570e-02,  1.6963e-04, -2.8846e-02, -1.9435e-02,\n",
      "         3.8691e-02,  6.2011e-02,  2.6625e-02,  3.5447e-03,  1.3653e-02,\n",
      "         6.6875e-01,  6.7837e-03, -6.9556e-04,  9.4138e-04, -9.0930e-03,\n",
      "         2.5367e-01,  3.7075e-02,  3.5492e-02,  9.1358e-02, -1.3217e-02,\n",
      "        -1.4852e-02, -9.0147e-04,  2.3020e-02,  6.4781e-01,  1.0797e+00,\n",
      "         2.0785e-02,  3.4754e-01,  1.6867e-01, -2.7832e-03,  2.1875e-01,\n",
      "         1.4448e-01,  3.7185e-02,  2.0235e-02,  1.8366e-02, -3.3985e-03,\n",
      "         2.9642e-01,  1.8472e-02,  5.9267e-03, -1.8872e-03, -1.3961e-02,\n",
      "         4.2895e-02,  7.7515e-01, -6.3451e-02,  1.0331e-01,  6.8941e-01,\n",
      "        -1.8466e-02,  2.6880e-01,  3.3144e-01,  2.0752e-02,  1.0653e-01,\n",
      "         2.0541e-01,  2.2809e-01, -2.6332e-03,  7.0316e-02, -8.5167e-02,\n",
      "        -3.4192e-03,  1.6008e-02,  2.4576e-01,  1.5463e-01,  4.0969e-03,\n",
      "         9.7721e-02, -2.2108e-01, -9.4132e-03,  4.8434e-01,  5.5085e-03,\n",
      "         4.1522e-01,  8.0863e-02,  3.1019e-01, -6.9981e-03,  1.3238e-01,\n",
      "        -7.3964e-02,  2.2874e-01,  3.2319e-01,  2.5844e-01, -1.0985e-01,\n",
      "         2.9519e-01, -2.0062e-03,  2.9009e-01,  1.3362e-01,  2.7286e-03,\n",
      "        -1.8566e-03,  6.9576e-02,  1.3792e+00,  1.3417e-01,  1.2911e-01,\n",
      "        -2.3786e-02], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T18:07:04.269147Z",
     "start_time": "2024-12-03T18:07:04.252342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "pruning_ratio = 0.3\n",
    "# Print the names and parameters of BatchNorm layers\n",
    "count = 0\n",
    "for name, module in batchnorm_params:\n",
    "    print(f\"Layer: {name}\")\n",
    "    print(f\"Weight: {module.weight.data}\")\n",
    "    print(f\"Bias: {module.bias.data}\")\n",
    "    print()\n",
    "    batchnorm_scale =  module.weight.data\n",
    "    num_channels = batchnorm_scale.size(0)\n",
    "    num_pruned_channels = int(num_channels * pruning_ratio)\n",
    "    \n",
    "    weights = {k: v for k, v in enumerate(batchnorm_scale.cpu().numpy())}\n",
    "    sorted_weights = {k: v for k, v in sorted(weights.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    keep_idx = list(sorted_weights.keys())[num_pruned_channels:]\n",
    "    pruned_weights = batchnorm_scale[keep_idx]\n",
    "    pruned_bias = module.bias.data[keep_idx]\n",
    "    \n",
    "    count += 1\n",
    "    if count==1:\n",
    "        break   "
   ],
   "id": "ab1cfd61eaf90576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.1\n",
      "Weight: tensor([0.0381, 0.1872, 0.1975, 0.2451, 0.1313, 0.1590, 0.0881, 0.2552, 0.0870,\n",
      "        0.0119, 0.4129, 0.1137, 0.2245, 0.3014, 0.0114, 0.0104, 0.0128, 0.0043,\n",
      "        0.0668, 0.4108, 0.3578, 0.1278, 0.5250, 0.0039, 0.4444, 0.1298, 0.3284,\n",
      "        0.2453, 0.3565, 0.3066, 0.4146, 0.1419], device='cuda:0')\n",
      "Bias: tensor([-8.4354e-02,  5.6023e-01,  3.5002e-01,  2.8363e-01,  9.7327e-01,\n",
      "         6.4774e-01,  4.9481e-01,  5.5817e-01,  6.1756e-01, -4.2980e-04,\n",
      "        -3.0858e-01,  9.5334e-01,  4.4609e-01, -3.8414e-01, -9.3045e-04,\n",
      "         5.7470e-03, -4.2064e-02, -1.7965e-02,  3.3821e-01,  1.1017e-01,\n",
      "        -2.5284e-01,  5.0251e-01,  3.7990e-01, -1.5532e-02, -4.6869e-01,\n",
      "         5.1056e-01, -2.8880e-01,  6.4006e-01, -1.0935e-01, -5.9483e-02,\n",
      "         3.7479e-01,  2.6511e-01], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0881, 0.1137, 0.1278, 0.1298, 0.1313, 0.1419, 0.1590, 0.1872, 0.1975,\n",
       "        0.2245, 0.2451, 0.2453, 0.2552, 0.3014, 0.3066, 0.3284, 0.3565, 0.3578,\n",
       "        0.4108, 0.4129, 0.4146, 0.4444, 0.5250], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T19:22:25.752364Z",
     "start_time": "2024-12-03T19:22:25.742945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruning_ratio = 0.3\n",
    "pruned_channels = {}  # To store pruned indices for each layer\n",
    "    \n",
    "# Step 1: Analyze BatchNorm scaling factors\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        # Get the scale (\\gamma) values\n",
    "        gamma = module.weight.detach().cpu().numpy()\n",
    "        num_channels = len(gamma)\n",
    "        \n",
    "        # Determine the number of channels to prune\n",
    "        num_prune = int(pruning_ratio * num_channels)\n",
    "        \n",
    "        # Identify the indices of the smallest \\gamma values\n",
    "        prune_indices = gamma.argsort()[:num_prune]\n",
    "        pruned_channels[name] = prune_indices"
   ],
   "id": "9314d6310adb1b59",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T19:22:26.348473Z",
     "start_time": "2024-12-03T19:22:26.332948Z"
    }
   },
   "cell_type": "code",
   "source": "pruned_channels",
   "id": "c52a56363f57ea54",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features.0.1': array([23, 17, 15, 14,  9, 16,  0, 18,  8], dtype=int64),\n",
       " 'features.1.conv.0.1': array([15, 17, 14,  9, 23, 16,  0, 24, 28], dtype=int64),\n",
       " 'features.1.conv.2': array([10,  3,  8,  0], dtype=int64),\n",
       " 'features.2.conv.0.1': array([72, 12, 49, 62, 31, 29, 90, 13, 38, 23, 43, 86, 22, 48,  7, 55, 28,\n",
       "         2, 41, 66, 65, 78, 74, 17, 21,  9, 94, 63], dtype=int64),\n",
       " 'features.2.conv.1.1': array([54, 50, 73, 77, 10, 27, 81, 93,  6, 36, 88, 67, 66, 70, 33, 17, 53,\n",
       "        34, 94, 28, 51, 92, 75, 31, 16, 41, 57, 63], dtype=int64),\n",
       " 'features.2.conv.3': array([ 6,  4, 23,  3, 21, 20,  1], dtype=int64),\n",
       " 'features.3.conv.0.1': array([110,  44, 115,  87,  90, 120,  79,  97, 126, 114,   1, 142,  64,\n",
       "         46, 138,  29,  30, 139,  73, 128, 140,  88,  83,  17,  95,  70,\n",
       "        118, 106, 130,  45,  42, 125,  12,  54,  35,   8,  81,  41, 137,\n",
       "        135,  43, 107, 116], dtype=int64),\n",
       " 'features.3.conv.1.1': array([ 75,  24, 111,  13,  82,  99,  15, 116,   0,  14,  85,  55, 131,\n",
       "        104,  16,  50,   4,   2,  62,  12,  77,   7,  22,  58, 101, 129,\n",
       "         61,  83,   6,  33,  94,  40,  38, 124,  48,  91,  57, 108,  89,\n",
       "         28,  32,  34,  66], dtype=int64),\n",
       " 'features.3.conv.3': array([18,  7, 16,  9, 15,  8, 19], dtype=int64),\n",
       " 'features.4.conv.0.1': array([ 62, 115, 107,  63, 110,  82,  70, 135,  95, 101,   3,  59,  26,\n",
       "         81,   1, 120, 122,  32, 131,  79,  46, 116,  48, 106, 134,  98,\n",
       "        108, 140,  97,  86,  92, 100,   5, 133,  78, 114,  55,  25, 124,\n",
       "        118, 127,  85, 119], dtype=int64),\n",
       " 'features.4.conv.1.1': array([ 33,  97,  45,  74,  90, 112,  43,  27,  22, 105,  65,  10,  50,\n",
       "         87, 127,  48,  85,  83,  35,  68,  64,  30,  13,  42, 103,  29,\n",
       "        132,  55, 140,  89,  24,   6,  44, 100, 104,   7,  66,  19, 139,\n",
       "         78,  94,  49,  88], dtype=int64),\n",
       " 'features.4.conv.3': array([ 4, 31, 19, 23, 10, 26,  9, 15,  8], dtype=int64),\n",
       " 'features.5.conv.0.1': array([186, 136,  54,  65,  67, 133, 134, 187, 161,  76, 113, 142, 101,\n",
       "         25,  10, 188,  48,  60,  97,  38,   2, 147, 109,   5,  35,  42,\n",
       "         58,  70,  46,  63,  55, 137,  75, 118, 176,  36,  86,  20,  56,\n",
       "        117, 150,  12, 182, 149, 127,  24,  43, 121, 184, 125, 120, 154,\n",
       "         91, 148,  47,   6, 173], dtype=int64),\n",
       " 'features.5.conv.1.1': array([ 64, 170,   8,  32, 140,   4,  29, 116, 174,  68, 126,  95,  15,\n",
       "         73,  72,  59, 180, 115,  58, 124, 102,  34,  26,  53,  79,   9,\n",
       "        179,  30,  88, 135,  78,  33,  56,  69, 166, 114, 177,  18,  71,\n",
       "         87, 141,  14, 178, 191,   1, 104,  22, 120,  90,  82, 122, 142,\n",
       "         80, 152,  27,  92, 168], dtype=int64),\n",
       " 'features.5.conv.3': array([ 5, 21, 14, 12, 18,  1, 13, 11, 22], dtype=int64),\n",
       " 'features.6.conv.0.1': array([  6,  62,   3,  76,  61,   1,   2, 117,  56,  27, 180, 178,  75,\n",
       "          4,  28,  31, 131, 102,  60,  16, 159,  57, 115,  99, 158,  91,\n",
       "        120, 135,  43,  87, 123,  23,  46, 138, 171, 108, 104, 148, 101,\n",
       "        129,  80, 116, 166,  42,  71, 145,  13,  54,  17, 105, 140,  97,\n",
       "          0,  82, 182,   9,  32], dtype=int64),\n",
       " 'features.6.conv.1.1': array([  6,  85, 179, 130,  49, 107,  52,  10, 141,  33, 139, 186, 185,\n",
       "        100, 134,  12, 160, 155,  44, 110,  96, 183,  40, 163,  93,  94,\n",
       "        126,   7,  50,  35, 181, 188, 182, 118,  19,  59,  51,  11,  86,\n",
       "        122, 176,  95, 153,  83, 154,  66, 177, 162,  92, 190, 113, 150,\n",
       "        101, 170,  63, 102, 145], dtype=int64),\n",
       " 'features.6.conv.3': array([18, 11, 22, 13, 14,  1, 24,  5, 21], dtype=int64),\n",
       " 'features.7.conv.0.1': array([ 22,  76, 118,  61,  85,  78, 110, 107, 101, 158,  83, 171,  14,\n",
       "         12,   2, 111,  41,  95, 149, 139,  13,   5, 126, 113,   9, 187,\n",
       "         25, 172, 142, 141,   4, 184,  75, 156,  18, 140,  45,  40, 188,\n",
       "        170, 185,  16,  39,  55, 169,  35, 123, 173,  49,  10,  77, 131,\n",
       "         69,  54,  60,  80, 127], dtype=int64),\n",
       " 'features.7.conv.1.1': array([ 69,  90, 172, 162,  28, 131,   1,  88,   8, 176,  62,  27,  36,\n",
       "        191,  38, 183, 143, 123,  10, 169, 181, 173, 125,  53, 159,  96,\n",
       "        102, 119,  77,  71, 151, 112, 114, 168,  42, 104, 137, 153,  57,\n",
       "        175, 142,  67,  12,   6,  20, 163,  32,  34, 116, 117,  30, 148,\n",
       "         94, 184,  54,  93,  15], dtype=int64),\n",
       " 'features.7.conv.3': array([51, 44, 28,  8, 18, 60, 59, 56, 50, 46, 26, 30,  7, 23, 34,  0, 24,\n",
       "        43, 58], dtype=int64),\n",
       " 'features.8.conv.0.1': array([131,  51, 193, 369, 136,  11, 341, 116, 350, 219, 150, 174, 143,\n",
       "        278,  57, 206, 312, 283, 149, 305, 222, 132,  84,   0, 167,  24,\n",
       "        293, 186, 296,  94, 113, 266,  41,  60, 183, 330, 265, 162, 297,\n",
       "        258, 169, 263, 229, 307, 168,  52, 241, 226, 331, 294, 301,   1,\n",
       "        103, 289, 354, 231, 114, 204, 187, 249, 110, 292, 234, 165,  19,\n",
       "        196, 308, 197, 366, 291, 286,  75, 375, 158, 334, 277, 180,  29,\n",
       "        199, 376, 221, 182,  67, 200, 313,  96, 267,  59, 203,  25, 242,\n",
       "        339,  45, 190,  71, 328, 323, 129, 372,  80, 107,  20,  58, 207,\n",
       "        195,  91, 189, 157, 276, 244,  37, 248,  61, 351,   2], dtype=int64),\n",
       " 'features.8.conv.1.1': array([ 88,  72, 321, 280, 382,  39, 217, 257, 220, 349, 304, 315,  94,\n",
       "         48, 185,  56, 214,   7,  35, 177, 371, 230,  17,  86, 359, 148,\n",
       "        353,  92, 383, 120,  53, 212, 256, 264, 324, 311, 284, 268, 106,\n",
       "        171, 344, 236, 341, 129, 239,  93,  33,  68, 110, 137, 326, 224,\n",
       "         82, 108,  26, 147, 293, 170, 275, 376, 151, 119, 197,   9, 125,\n",
       "        366, 323, 160, 367, 285, 157, 312, 123, 253,  15, 358,  46, 292,\n",
       "        357, 373, 107, 208, 310, 211, 288, 198, 213, 329, 302, 247, 207,\n",
       "         36, 254, 127, 218, 255, 100, 240, 314, 346, 281,  78,  29, 251,\n",
       "        348, 179,  62,  21,  24,  80, 306, 145,  25, 273,  81], dtype=int64),\n",
       " 'features.8.conv.3': array([33, 17, 20, 62, 19, 12, 27, 39, 10, 32, 49,  9, 16,  1, 21, 43, 48,\n",
       "        37, 42], dtype=int64),\n",
       " 'features.9.conv.0.1': array([266, 316,  92, 218, 114,  80,  97, 297, 104, 118,  69, 216, 288,\n",
       "        197, 344, 111, 148,  54, 169, 254,  32, 178, 315,  87, 346, 205,\n",
       "          6,  56, 333,  26, 246,  14, 214, 353, 334,  35,  76, 357,  62,\n",
       "        263, 311,  15, 185,  93,  20, 110, 327,  55, 354, 369, 130, 122,\n",
       "        322,  21,  71, 158,  91,  44, 286, 238,   1, 168, 125, 336,  77,\n",
       "        273,  11, 382,  89, 121, 281, 144, 134,  48, 298, 182, 290, 211,\n",
       "         73, 116, 186,  78, 331, 294, 301, 177, 201, 249, 250, 220, 222,\n",
       "        305, 348, 272, 202, 109,  72, 343, 188,  58,   2, 277, 152, 295,\n",
       "        235, 166, 239, 212, 253, 146, 131,  79, 261, 342,  65], dtype=int64),\n",
       " 'features.9.conv.1.1': array([266, 316,  92, 224, 172, 340,   3,  13, 222,  66, 326, 167, 261,\n",
       "        291,  68, 313, 163,  24, 337, 191, 105, 332,  52, 330, 182, 147,\n",
       "        304, 252,  49, 204, 161,  86, 156, 108, 324, 341,  90, 123, 228,\n",
       "        177, 146, 319, 184,  43, 383, 136, 159, 198, 154, 168,  28, 347,\n",
       "        230, 103, 245, 259, 211, 225, 150, 317, 364, 292, 251, 275, 143,\n",
       "        270, 382, 134,  38, 264, 359, 109, 302, 284, 178, 271,  94,  96,\n",
       "        318,  85, 351, 193,  41, 278, 349, 325, 247, 145, 362, 203,   0,\n",
       "        379, 217, 223, 378, 381, 298,  10, 180, 371,  62,  75, 175,  37,\n",
       "        256, 215,  20, 295,  78, 307,  42, 138,  59,  82,  88], dtype=int64),\n",
       " 'features.9.conv.3': array([62, 19, 35, 42, 17, 15,  1, 39, 33, 20, 45, 32, 10, 14, 49, 37,  4,\n",
       "         5, 36], dtype=int64),\n",
       " 'features.10.conv.0.1': array([149,   0, 271, 182, 156, 367,  23, 267, 200,  96,  91,  33,  52,\n",
       "        344, 319, 177, 381, 361, 356, 231, 261, 256, 178,  51, 123, 286,\n",
       "        203, 334,  84, 348, 376, 144,  97, 304, 283,  10, 353,  87, 287,\n",
       "        129, 103,  66, 107, 207, 246, 140, 293, 162, 125, 206, 235, 358,\n",
       "        312, 335, 268, 120, 239, 237, 301, 360, 234, 352,  45, 241, 221,\n",
       "         79, 323, 254, 272, 337, 117, 137, 100, 151, 163, 223, 104, 188,\n",
       "        193,  28, 165, 152,  63, 260, 324, 197, 173, 172, 341,   5, 339,\n",
       "        379,   2,  85,  27, 217, 196,  17, 180, 329, 330,  92,  56,   9,\n",
       "        146, 127,  76,  48, 208, 158, 363, 138, 153,  55,  57], dtype=int64),\n",
       " 'features.10.conv.1.1': array([149,   0, 156, 182, 271,  89,  73,  58, 281, 253, 325, 316,  47,\n",
       "        136, 315, 100, 215,  24, 330, 111,  22, 276,  36, 327,  41, 124,\n",
       "         78, 145, 351, 377, 233, 214, 227, 212, 232, 216, 189,  76, 132,\n",
       "        127, 373, 169,  16, 180, 278, 313, 249,  69, 186,  30, 284, 270,\n",
       "        338, 134, 229, 345, 264, 185,  48,   7,  94,  92, 366,  15, 255,\n",
       "        165, 355, 163, 120,  25,  26, 209,  62,  63,  86, 273, 141, 326,\n",
       "        306, 265,  45, 236, 171, 357, 210,  40,   9, 359, 382, 197, 112,\n",
       "        208, 307, 282, 285, 219,  82, 323,  46, 230, 102,  54,  70, 289,\n",
       "         13,  95,  53, 290, 160,  97,  74,  93,  75, 258,  50], dtype=int64),\n",
       " 'features.10.conv.3': array([62, 19, 13, 16, 17, 12, 20, 33, 37, 14, 21, 27,  1, 15, 10, 29,  5,\n",
       "        55, 49], dtype=int64),\n",
       " 'features.11.conv.0.1': array([ 10,  72, 313,  42, 342, 318, 112, 159, 232, 259, 353,  50, 327,\n",
       "        165, 199,  69,  59,  62, 317, 268,   9, 238, 376,  35, 208, 343,\n",
       "        235, 152, 224,  31, 118, 336, 289, 181,  58, 163, 161, 273,  55,\n",
       "        338, 160, 170, 382, 137, 218, 121, 254,  66, 250, 171, 354,  33,\n",
       "        312, 379, 173, 125,  67, 157, 364,  37,  54, 241, 114,   3, 245,\n",
       "        177, 262, 350, 261,  25,  26,  30, 368, 335, 375, 172, 270, 148,\n",
       "        362,  17, 124,  47, 154, 110,  53, 282, 352,   1, 297, 207, 307,\n",
       "         65, 287, 234, 372, 146, 105,  75, 175, 190, 283, 167,   7,  73,\n",
       "        301,  60, 130, 133,  24, 186,  95, 102, 150, 367,  56], dtype=int64),\n",
       " 'features.11.conv.1.1': array([293, 202,  43, 264, 257, 243, 136, 117, 287, 119, 158,  49, 305,\n",
       "        265, 113,  61,  19,   8, 291, 145,  70, 212, 217,  78, 375, 248,\n",
       "        192, 298, 316, 227, 196, 266, 104, 244,  96, 233, 270, 256, 133,\n",
       "        267,  52, 175, 186, 151, 344, 294,  41,  12, 269,   2, 126, 147,\n",
       "        223,  63, 130, 254, 225, 162, 137, 383,  62,  34, 127, 129, 205,\n",
       "        110,  97, 306, 333, 271, 194,  74, 246, 330, 370, 181, 352,  85,\n",
       "         29, 228,  21, 242, 189, 188,  18,  84, 128, 378, 111, 275,  11,\n",
       "        363, 152, 261, 200, 197, 140, 274, 131, 331, 138, 176, 226, 106,\n",
       "         32, 286, 216,   3,  79,  46, 209, 367,   0, 324, 357], dtype=int64),\n",
       " 'features.11.conv.3': array([21, 42, 54, 82, 94, 25,  9, 86, 95, 59, 23, 37, 71, 39, 18, 67, 70,\n",
       "        90, 15, 85, 30, 66, 92, 69, 76, 81, 27, 14], dtype=int64),\n",
       " 'features.12.conv.0.1': array([235,  89,  90, 369, 232, 163, 117, 461, 386, 416, 298, 464, 312,\n",
       "        504, 377,  34, 287,  80, 138, 526, 547, 345, 535,  49, 233, 252,\n",
       "         11, 296, 554, 403, 462, 173, 337, 263, 541, 324, 285, 124,  78,\n",
       "        466, 308, 550, 187, 450, 490, 430, 363, 282, 223,  97, 208, 509,\n",
       "         29,  30, 469,  99, 371, 538, 301, 506, 328, 413, 498, 171,  36,\n",
       "        257, 428, 113,  67, 529, 238,  43,  33, 127, 121, 168, 151, 264,\n",
       "        406, 411, 125, 126, 565, 200, 351, 134, 481, 104, 437, 340, 225,\n",
       "        110, 452, 489, 205, 132, 194, 448, 278, 179, 357, 180, 108, 532,\n",
       "        230, 295, 181,  27, 453,  20,  72, 130, 495, 539, 480, 260,  60,\n",
       "        476, 176,  58, 408,  40, 370, 503, 555, 491,  63,   4,  64, 118,\n",
       "        373, 115, 400, 299,  10, 303, 459, 512, 228, 143, 254,  37, 537,\n",
       "        505,  24, 280, 240,   2,  45, 217, 156, 320,  23, 190, 206, 531,\n",
       "        268, 410, 261,  73, 398, 286, 161, 271, 177, 245, 220, 405, 553,\n",
       "          0, 289,  15], dtype=int64),\n",
       " 'features.12.conv.1.1': array([235, 163, 232,  89, 117, 369,  90, 184,  17, 256,  23, 570, 477,\n",
       "         19, 212, 473, 575, 294, 445, 274, 185, 496,   6, 565,  31, 364,\n",
       "        344, 234, 546, 402, 436, 545,  74, 199, 178, 484, 533,  82, 392,\n",
       "        134, 486, 342, 560, 103, 446, 112, 458, 381, 557,  70, 169, 353,\n",
       "        141,  91, 149, 170,  45, 165, 419, 315, 245,  79, 362, 449, 465,\n",
       "        426, 360, 388,  52, 242, 319, 227, 259,  41,  75, 530, 508, 311,\n",
       "        270, 292, 542, 204, 273, 398,  87, 265, 472, 299, 338, 475, 437,\n",
       "        115,  59, 498, 326, 427, 154, 221, 375, 440, 136, 291, 205, 483,\n",
       "        389, 321, 238, 230, 423, 494, 101, 404, 459, 198,  43, 572, 317,\n",
       "        293, 333, 516, 384, 258,  10, 507, 391, 329, 523, 182, 336, 501,\n",
       "        571, 519,   7, 176, 255, 172, 122,  26, 271, 372, 468, 467, 142,\n",
       "        327,  64, 511, 532, 474, 540,  28,  98, 197, 137, 502, 303, 209,\n",
       "         77,  37,  57,   5, 295,   4,  35, 175,  83, 556, 168, 551, 335,\n",
       "         39, 228, 574], dtype=int64),\n",
       " 'features.12.conv.3': array([ 3, 28, 40, 64, 13, 57, 79, 52,  8, 50, 31, 62, 26, 10, 45, 65, 78,\n",
       "        56, 44, 33, 91, 29, 22, 46, 51, 75, 68,  2], dtype=int64),\n",
       " 'features.13.conv.0.1': array([534,  25, 280, 544, 132, 122, 281, 441, 164, 147, 353, 465, 151,\n",
       "        199, 336, 402,  75,   6, 416, 225, 510,   8, 403, 231, 170, 412,\n",
       "        470,  13, 396, 507,  81,  29, 264, 308, 390,  95, 168, 237,  77,\n",
       "        287, 538,  56, 259, 505, 529, 188, 360, 404, 158, 130, 190,  96,\n",
       "        409, 562, 106,  83, 528,   0, 474, 279, 500, 141, 251,  47, 148,\n",
       "        185,  38,  40,  59, 567,  72, 388, 133, 573, 413,  41, 120, 537,\n",
       "        568,  62, 204, 261, 443, 137, 524, 104, 272, 548, 320, 422, 407,\n",
       "         35, 267, 126,  42, 187, 128, 297, 490,  22, 208,  17, 152, 317,\n",
       "        427, 218, 256, 356,  92, 495, 285, 460, 109, 399, 522, 380, 432,\n",
       "        370, 535, 209, 454, 429, 318, 252, 247,  21, 315, 478, 262, 313,\n",
       "        408,  36, 545, 157, 437,   5, 428,  69, 373, 112, 445, 144, 499,\n",
       "        394, 447, 116, 431, 181, 108, 359, 175, 138,  37, 405, 426, 469,\n",
       "        294, 496, 543, 377, 497, 515, 565, 149, 291, 509,  63, 201, 570,\n",
       "        560, 369, 521], dtype=int64),\n",
       " 'features.13.conv.1.1': array([ 25, 280, 544, 534, 281, 122, 132, 512, 105, 462,  97, 238, 102,\n",
       "        330, 388, 425,  99, 266, 253, 520, 506, 484, 172, 344, 123, 358,\n",
       "        557, 469, 306, 224, 207,  89, 211, 309, 393, 391, 517, 566, 367,\n",
       "         39, 564, 261, 455, 454, 213, 327, 189, 542, 561, 550, 369, 486,\n",
       "         58, 538, 305, 456, 335, 342,  48, 119, 125, 410, 468, 408, 166,\n",
       "        533, 414, 453, 359, 476, 149,  54, 165, 273,  78,  12, 269,  49,\n",
       "        435, 395, 157,  84,  23, 401,  71,  34, 252, 568,  26,  32, 197,\n",
       "        177, 296,  93, 248, 389, 111, 136, 154, 422, 575, 278, 222, 420,\n",
       "         52,   3, 438, 303, 569, 182, 452, 571, 377, 547, 226, 183, 343,\n",
       "        345, 383, 293, 490, 546,  92,  60, 184, 511, 249, 458, 233, 480,\n",
       "        356, 180, 130, 201, 217, 501,  24, 526, 351, 434, 347, 301,  18,\n",
       "         27, 543, 492, 193, 255,  94, 481, 121, 513,  38, 432, 232, 488,\n",
       "        433, 563, 560, 529, 290, 540, 530, 221, 413, 313, 440, 444, 556,\n",
       "        155, 332,  66], dtype=int64),\n",
       " 'features.13.conv.3': array([ 3, 57, 64, 79, 65, 52, 40, 78, 50,  8, 10, 13, 89,  2, 68, 45, 28,\n",
       "        46, 39, 12, 26,  6, 88, 34, 62, 29, 91, 27], dtype=int64),\n",
       " 'features.14.conv.0.1': array([304, 543, 423, 455, 402,  25, 267, 370, 147, 464,  40, 508, 135,\n",
       "        180, 456, 432,  16, 264, 259, 121, 255, 445, 439,  81, 297, 238,\n",
       "        142,  78, 213, 333, 416, 134, 250,  64,  96, 566, 438, 547, 512,\n",
       "        299, 545,  97, 334,   0, 225, 301, 484, 129, 113, 298, 352, 197,\n",
       "         79, 563, 257, 295, 411, 191, 198, 181, 575, 220, 163,   5, 246,\n",
       "        193, 215, 429, 362, 434, 261,  47, 375, 461, 123, 319, 475, 195,\n",
       "         24, 460, 205, 235, 440, 192, 573, 405, 360, 337,  13, 539, 130,\n",
       "        431, 441, 167, 357, 303, 412, 467, 287, 500, 313, 144, 392, 364,\n",
       "         73, 212, 473, 314, 280, 359, 351,  63, 229, 127, 284, 340, 382,\n",
       "        419, 406, 361, 221, 358, 172, 166, 125, 308, 442,  31, 480,  85,\n",
       "        569, 494, 415, 349, 251, 410,  53, 248,  27,  70, 184, 339, 174,\n",
       "         68, 151,  26, 462, 529, 418, 350, 482, 294, 207, 468, 240, 567,\n",
       "         99, 401, 514, 149, 175, 330, 414,  17, 451, 249, 465, 302, 338,\n",
       "        367, 556, 239], dtype=int64),\n",
       " 'features.14.conv.1.1': array([304, 543, 129,  86, 431, 303,  13, 349, 358, 311,  24, 308,  27,\n",
       "        558, 221, 467, 483, 419, 482, 215, 522, 475, 334,  65, 401,  79,\n",
       "        429, 234, 284, 412, 460, 181, 314, 556,  73,  69, 177, 114, 362,\n",
       "        144,  90, 529, 340, 262,  39, 205, 239, 213, 416,  47,  63, 421,\n",
       "        127, 397, 435, 347, 406, 357, 477, 494, 392, 447, 376,  11, 462,\n",
       "        457, 480, 174, 493, 367, 235, 500, 382, 189, 319, 442, 510, 451,\n",
       "        410,  70, 459, 337, 225, 360, 488,  53, 248, 139, 104, 452, 166,\n",
       "        355, 455, 113, 246, 249, 255, 123,  17, 473,  36, 484, 151, 279,\n",
       "        207, 342, 496,   5, 250, 553, 514, 199, 361, 315,  30, 575, 170,\n",
       "        218, 313, 517, 346, 209, 454, 434, 290, 295, 375, 172, 531,   4,\n",
       "        542,   6, 125, 464,  77, 515, 507, 258, 364,  35, 280,  67,  44,\n",
       "        229, 270, 359, 528, 445, 420, 157, 564, 519, 232, 257, 539, 323,\n",
       "        393,  76, 468, 339, 291, 495, 533, 351, 440, 301, 569, 184,   0,\n",
       "        147, 296, 550], dtype=int64),\n",
       " 'features.14.conv.3': array([ 31,   7,  34, 148,  84, 118,  25,  50,  59,  21,   1,  10,  82,\n",
       "        129,   0,  14,   5, 131,  76,  29,  16,  23,  67, 120,   8,  74,\n",
       "        149, 137,  89, 107,  52,  93,  56,  81,  64, 153,  51,  38,  83,\n",
       "         97,  32,  53,  65,  91, 155,  54,   3,  24], dtype=int64),\n",
       " 'features.15.conv.0.1': array([946, 201, 554, 604, 386, 280, 188, 675,  92, 355, 785,   7, 392,\n",
       "        175, 114, 922, 273, 368,  56, 211, 809, 358, 735, 207, 401, 658,\n",
       "        958, 820, 505, 616,   2, 466, 603, 579, 486, 472, 235, 478, 396,\n",
       "        205, 490, 800, 769, 932, 112,   6, 685, 487, 508, 425,  10,  89,\n",
       "        762, 340, 329, 780, 255, 617, 787, 332, 291, 317, 364, 314, 776,\n",
       "        232,  98, 365, 566, 108, 520, 465, 605, 692, 477, 836, 896, 204,\n",
       "        238, 911, 716, 671, 877, 546, 756, 359, 287, 316,  30, 804,  66,\n",
       "        858, 136, 243, 485, 259, 104, 198, 611, 322, 883, 529, 574, 691,\n",
       "        511, 192, 366, 474, 840, 677, 153, 707, 930,  73, 718, 428,  57,\n",
       "        810, 538, 102, 275, 326, 323, 397, 610, 412, 634, 770, 531, 327,\n",
       "        806, 146, 626, 673, 129, 171, 236, 377,  44, 440,   1, 738, 134,\n",
       "         23, 391, 944, 556, 476, 957, 302, 349, 827, 484, 947, 461, 266,\n",
       "         28, 216, 902, 449, 884, 637, 398, 771, 172, 797, 299, 450, 521,\n",
       "        150, 224, 580, 654, 525, 385, 727, 564, 351, 479, 807, 763, 183,\n",
       "        764, 518,  38, 512, 218,  54, 583, 407, 559, 569, 489, 765, 717,\n",
       "        917, 700, 638, 415, 503, 406, 116, 904, 842, 221, 684, 798,  47,\n",
       "        142, 832, 681, 816, 627, 741, 715, 778, 568, 516, 699, 239, 307,\n",
       "        252, 278, 888, 457,  27, 721, 470, 455, 107, 635, 805, 608, 821,\n",
       "        918, 533, 261, 524, 405, 881, 870, 504, 845, 126, 530, 296, 301,\n",
       "         63, 101, 636, 282, 851, 649, 754, 181, 682, 305, 248, 951,  33,\n",
       "        934, 177,  15, 213, 597, 620, 325, 589, 774, 889,  20, 446, 777,\n",
       "        864, 509, 614, 647, 811, 483, 694, 933, 402, 125, 866, 740,  95,\n",
       "        893, 731], dtype=int64),\n",
       " 'features.15.conv.1.1': array([199, 693, 688, 315,  77, 855, 826, 399, 491, 852,  22,  37, 643,\n",
       "         98, 281, 874, 320, 364, 648, 609, 645, 325, 208, 480, 878, 876,\n",
       "        873, 788, 145, 507, 856,  51, 676, 446, 584, 894, 284,  40, 123,\n",
       "        535,  50,  21,   3, 477, 619, 596, 247, 680, 595, 245, 464, 136,\n",
       "        314, 420, 808, 553, 871, 575, 450, 880, 110, 120, 468, 692, 865,\n",
       "        345, 851, 527, 666, 312, 765, 410, 165, 582, 456, 754, 499, 335,\n",
       "        318,  72, 279, 280, 459, 573, 739, 231, 401, 933, 847, 753, 341,\n",
       "        683, 756, 344, 424, 793, 892, 329, 517, 109,  61, 321, 404, 923,\n",
       "        599, 179, 363, 378, 522, 538, 925, 322, 817, 844, 339, 777, 267,\n",
       "        308, 534, 415, 414, 350, 903, 353, 542,  29, 774, 725, 303, 702,\n",
       "        572, 361,  16,  42, 909, 953, 438, 482, 206, 612,  65, 254, 389,\n",
       "        198, 510, 580, 759, 888,  34, 585, 651, 597, 195, 822, 423,   9,\n",
       "        265, 714, 543, 426, 413, 182, 223, 338, 388,  80, 422, 532, 347,\n",
       "        362, 629, 881, 622, 642, 723, 773, 742, 872, 802, 838, 429, 504,\n",
       "        421, 394, 443, 112,  90, 537, 395, 147, 672, 860, 116, 578, 250,\n",
       "        898, 412, 895, 225, 457, 705, 271, 301, 783, 383, 202, 633, 377,\n",
       "        706, 563, 166, 324, 270, 590, 430, 549, 139, 118, 743, 917,  91,\n",
       "        952, 900,  19,  68, 138, 639, 290, 936, 122, 781, 837, 712,  70,\n",
       "         35, 740, 638, 926, 623, 747, 346, 904, 641,  24, 539, 805, 506,\n",
       "        859, 177, 201, 474, 411, 897, 348, 167, 649,  74, 617, 958, 751,\n",
       "        500, 950, 577, 621, 907, 406,  88,  81, 509,  32, 196,   6, 148,\n",
       "        173, 846, 951, 663, 408, 957, 529, 550,  85, 831, 352, 178, 244,\n",
       "        748,  95], dtype=int64),\n",
       " 'features.15.conv.3': array([ 58,  71, 152,  15,  87,  24,   2,  62,  47,  73, 134, 143,   8,\n",
       "         41,  18,  35,   4, 120,  27,  60, 142,  89,  86,  94, 106,  91,\n",
       "         78,  99, 141,  20, 156,  32, 136, 119,  74,  48,  19,  40,  64,\n",
       "         33,  54, 158, 115,  57,  53,  69, 101,  84], dtype=int64),\n",
       " 'features.16.conv.0.1': array([814, 404,  80, 348, 868, 775, 179, 585, 438, 395, 406, 812, 413,\n",
       "         34, 811, 643, 114, 888, 805,  58, 603, 431, 503, 389,   7, 808,\n",
       "        258, 161, 252, 299, 599, 692, 773, 244, 552,  16, 109, 443, 239,\n",
       "        364, 285, 740, 302, 914, 874, 570, 707, 208, 498, 799, 146, 743,\n",
       "         55, 559, 945, 260, 876, 835, 259, 311, 472, 717, 291, 573, 864,\n",
       "        725, 720, 332,  59, 685, 832, 826, 113, 900, 652, 598, 127, 433,\n",
       "        426, 145, 879, 221, 455, 264, 800, 904, 657, 349, 894, 939, 701,\n",
       "        809, 479, 751, 293,  30, 662, 706, 853, 639, 421, 675, 767, 397,\n",
       "        190, 851, 306, 715,  65, 620, 341,   8, 744, 320, 391, 737, 596,\n",
       "          2, 628, 322, 187, 220, 222, 567, 490, 354, 204, 232,  99,  27,\n",
       "        780, 644, 891, 763, 356, 440, 836, 870, 935, 251,  74,  18, 412,\n",
       "        273, 419, 821, 702, 564,  35,  56, 936, 597, 731, 765, 887, 350,\n",
       "        197, 804, 582, 626, 683, 713, 451, 147,   0, 745, 462, 316, 677,\n",
       "        512, 130, 794, 693, 712, 446, 255, 548, 198, 326,  32, 371, 545,\n",
       "        687, 294, 950, 246, 108, 121, 308, 718, 686, 924, 761,  87, 663,\n",
       "        508, 366, 277, 129, 569, 660, 344, 577, 422, 321, 646, 591, 196,\n",
       "        297, 930, 764, 563, 199, 213, 343, 379, 143, 162, 240, 200, 467,\n",
       "         38, 228, 818, 535, 523, 284, 456, 926,  97, 106, 766, 941, 735,\n",
       "        398, 556, 133, 281, 654,  21, 656, 650, 757, 681, 589, 648,  13,\n",
       "        202, 423, 840, 841, 131, 627,  14, 500, 542, 495, 209, 911, 169,\n",
       "        640, 753, 795,  89,  71, 651, 381, 792, 758, 307, 710, 571, 609,\n",
       "         23, 330, 923, 324, 587,  90, 242, 485, 856, 132,  15, 517, 534,\n",
       "        810, 912], dtype=int64),\n",
       " 'features.16.conv.1.1': array([ 80, 404, 814, 348, 770, 956,  97, 580, 955, 641, 473, 350, 494,\n",
       "         39, 903, 934, 220, 834, 773, 360, 375, 595, 115,   9, 378, 711,\n",
       "        169, 505, 152, 642, 464, 538, 528, 769, 270, 774, 897, 272, 543,\n",
       "        286, 300, 508, 486, 236, 793, 191, 684,  10, 424, 804, 426, 482,\n",
       "        526, 565, 704, 400, 890, 790, 601, 588, 155, 607, 629, 929,  75,\n",
       "        506, 710, 617, 476,  72, 522, 193, 854, 731, 923, 133, 863, 276,\n",
       "        874, 816, 188, 906, 386, 823, 802, 466, 316, 340, 583, 708, 866,\n",
       "        423,  30, 556, 723, 461, 786, 638, 743, 529,  99, 401, 940, 907,\n",
       "        178, 277, 384,  48, 208, 186, 787,   7, 873, 403, 746, 363, 269,\n",
       "        706, 330, 241, 748, 323, 365, 280, 435, 911, 351, 492, 762, 511,\n",
       "         59, 551,  51, 209, 134, 776, 355, 288, 477, 405, 515, 237, 105,\n",
       "        301, 310, 542, 334, 732, 819, 293, 949, 128, 371, 591, 870, 176,\n",
       "        490, 806,  69, 173, 394, 651, 928, 752, 428, 120, 825, 698, 346,\n",
       "        142, 724, 216, 103,  61, 885,   0, 843, 157, 862, 662, 156, 122,\n",
       "        253, 168, 304, 192, 951, 336, 872, 135, 665, 781, 660, 373, 889,\n",
       "        438, 174, 345, 474,  87, 223, 416, 107, 910, 333, 110, 881, 395,\n",
       "         83, 444, 813, 225, 381, 739, 884,  14, 210, 838, 161, 563, 702,\n",
       "        848, 513, 388,  24, 672, 856, 938, 753, 705, 882, 547, 760, 926,\n",
       "        246,  93, 199, 912, 432, 309, 353, 279, 425, 445, 470, 250, 857,\n",
       "        102, 602, 489, 318, 810, 213, 878, 585, 839, 686, 387, 745, 211,\n",
       "        235,  19, 239, 267, 577, 254,  11, 298, 791, 761, 669,  57, 163,\n",
       "        507, 144, 379, 320, 117, 231, 113, 530, 296, 189, 151, 212, 616,\n",
       "        136,  26], dtype=int64),\n",
       " 'features.16.conv.3': array([  9, 149, 110,  60, 114,  47,  95,  84,  33, 144,  12,  28,  69,\n",
       "          0,  22, 126, 116,  92, 125, 139,  32, 135, 106,  18,  54,  44,\n",
       "         86, 109,  20, 133,   7,   4,  75,  88,  94,  45, 113, 100,  37,\n",
       "         21, 107,  70,  91, 140, 124,  74,  13,  35], dtype=int64),\n",
       " 'features.17.conv.0.1': array([ 33, 923, 370, 395, 508, 328, 708, 465, 369, 562, 518, 523,  70,\n",
       "        542, 449,  50, 904, 104, 897, 280, 496, 261, 867, 507, 134, 362,\n",
       "        441,  10, 799, 883, 384, 848, 333, 807, 910, 292,  47, 906, 733,\n",
       "        155, 471, 874, 233, 616, 870, 174, 769, 113, 109, 941, 815, 415,\n",
       "        558, 592, 954, 861, 177, 669, 511, 111, 440, 547, 944, 937, 296,\n",
       "        530, 300, 559, 501,  38, 250, 341, 621, 887, 811, 546, 379, 598,\n",
       "        550, 631, 545, 489, 539, 670, 106, 663, 219, 266, 703, 865, 503,\n",
       "        796, 216, 873, 677, 915, 657, 637, 446, 123, 306, 125, 615, 202,\n",
       "        713, 263, 184, 862, 931,  94, 367, 497, 685, 524, 918,  23, 228,\n",
       "         57, 773, 614, 834, 920, 462, 878, 417, 721,  54, 493, 726, 921,\n",
       "        679, 454, 141, 295, 291, 121, 421, 913, 345, 809, 531, 404,   1,\n",
       "        330, 605, 429, 686, 629, 407, 313, 482, 120,   0,  58, 783, 298,\n",
       "        325, 294, 455, 167, 738, 806, 578, 952, 334, 272, 930, 277, 780,\n",
       "        785, 509,  34, 882, 641, 665,  24, 842, 434, 940, 378, 724, 400,\n",
       "        595, 365, 850, 892, 264, 335, 357, 911,  11, 552, 888, 580, 352,\n",
       "        577, 772, 492, 436, 821, 354, 401, 327, 802, 612, 683, 484, 439,\n",
       "        226, 139,  91, 690, 691, 411, 624, 881, 803, 565, 260, 833, 176,\n",
       "        391, 477, 640, 472, 452, 680, 855,  87, 722,  95, 789,  69, 363,\n",
       "        301,  56, 189, 754, 461,  39, 314, 372, 543, 727, 645, 652, 635,\n",
       "        303, 529, 160, 681, 196, 406,  98, 165, 221, 688,  45, 664, 222,\n",
       "         66, 231, 351, 186, 383,  29, 312, 936, 795, 381, 289, 800, 323,\n",
       "        812, 610, 464,  42, 896, 416, 797, 227, 279, 244, 348, 377, 220,\n",
       "        249, 494], dtype=int64),\n",
       " 'features.17.conv.1.1': array([ 33, 605, 862, 874, 878, 395,  95, 261,  70, 850, 125,  11, 730,\n",
       "        910, 325, 379, 471, 867,  69, 724, 679, 865,  21, 295, 113, 509,\n",
       "         32, 407, 404, 436, 313, 799, 489, 921, 663, 362, 219, 123,  56,\n",
       "        660, 369, 189, 691, 410, 378, 954, 637, 228, 882, 831,  66, 580,\n",
       "        280,  34, 417, 312,   0, 665, 197, 524, 263, 202, 796, 365, 492,\n",
       "        411, 773, 816, 186, 221, 277, 196, 815, 421, 842, 291, 833, 560,\n",
       "        735, 547, 134, 669, 614, 621, 888, 401, 783, 683, 616, 641, 324,\n",
       "        806, 578, 629, 301, 931, 334, 940, 685, 615, 155, 220, 484, 454,\n",
       "        533, 595, 264, 562, 430, 628, 824, 511, 104,  10, 287, 772, 686,\n",
       "        351, 439, 464, 895, 327, 883, 565, 106, 870, 306, 444, 754, 384,\n",
       "        363, 892, 881, 250, 915, 482, 793, 249, 266, 809, 357, 167, 262,\n",
       "        434, 415, 244, 341, 548, 703, 494, 788,  94, 420,  68,  50, 294,\n",
       "        323, 160, 105, 210, 690, 141, 610, 800,  28, 292, 936,  87,  38,\n",
       "        452, 391, 869, 938, 635, 222, 721,  91, 748, 449, 224, 175, 529,\n",
       "        550, 559, 518,  23,  57, 792, 348,   1, 624, 812, 139, 477, 381,\n",
       "        406, 612, 702, 530, 631, 606, 713, 289, 640, 192, 627, 937, 372,\n",
       "        367, 780, 260, 232, 920, 504,  77, 926, 957, 834, 808, 144, 233,\n",
       "        807,  44, 672, 535, 527, 558, 762, 944, 508, 896,  89, 688, 176,\n",
       "        592, 461, 319, 493, 496, 821, 657, 120,  92, 523, 886, 472, 501,\n",
       "        585, 785, 345, 296, 314, 171,  29, 109,  80,  47, 598, 321,  42,\n",
       "        769, 917, 752, 507,  24, 218, 577, 300, 146, 243, 932, 465,  39,\n",
       "        343, 539, 140, 542, 429, 165, 279, 455, 586, 446, 913, 339, 121,\n",
       "        333, 127], dtype=int64),\n",
       " 'features.17.conv.3': array([193, 314,  52,  49,  53,  66, 205,  51, 149, 191, 231, 283,   4,\n",
       "        238,  68, 176, 200, 303, 266,  28, 137, 132, 109, 143,  50, 153,\n",
       "         10, 308,  44, 220, 302,  45, 117, 190, 102,  48, 208, 154, 247,\n",
       "          5, 316, 107, 260, 170, 103, 318, 198, 133, 230, 179,  67, 113,\n",
       "        212, 215,  70,  65, 313, 211, 239, 122,  57, 234, 166,  81, 237,\n",
       "        292, 221, 148, 112, 256, 236, 305, 269, 295, 262, 105, 286,  33,\n",
       "        173, 276, 110,  71, 134, 252,  26,  60, 182, 121,  29, 183,  11,\n",
       "         42, 294, 181,  74,  22], dtype=int64),\n",
       " 'features.18.1': array([ 652,  381,  144,  484,  517,  797, 1083,   77,  401,  168, 1162,\n",
       "         717,  845, 1220,  101,  932, 1052,  923,  216,  295,  635,  943,\n",
       "         570,  841, 1108,  163, 1236,  613, 1088,  699, 1268,  416, 1278,\n",
       "         518, 1082,  388,  968,  522,  136,  915,  351,  467,  746,  961,\n",
       "        1113,   12,  950,  441,  649,  998, 1192,  494,  432,  150,  703,\n",
       "         389,  141,  988,  222,  619,   81,  298, 1062,  811,  588,  197,\n",
       "         191, 1177,  871,  376,  792,  477,  656, 1263,  576,  643,  331,\n",
       "         166,  489, 1045,  369,  816,  156,   54,   22,  958,  904, 1042,\n",
       "        1041,  695,  363, 1245,  556, 1193,  219,   92,  638,  133,  821,\n",
       "         548, 1259,  311,  660,  492,  879, 1203,  847,  954,   43,  236,\n",
       "         240,  763,  419,  937,  909,  460,  869, 1010, 1265,  538,  308,\n",
       "         798,  292,  987,  565,  553,  443,  515,  846,  119,  806,  394,\n",
       "        1184, 1243,  725,  938,  365,  650,  976,  734,  560,  826,  712,\n",
       "         785,  963,  794,  539,  218, 1160,  705,  736,   63,  750, 1110,\n",
       "         540,   73,   57,  620, 1165, 1122,   53, 1075, 1196,  735,  176,\n",
       "        1007,  900,  409,  178,   50,  762,  445,  655,  343,  205,  713,\n",
       "         789,  326,  447,   90, 1016,  266,  260,  378, 1149,  720, 1013,\n",
       "         640,  289,  528,   69,   79,  284, 1065,  688,  764,  221,    6,\n",
       "         609,  245,  297,  335,  303,  929,  478,  583,  942, 1254,  524,\n",
       "        1089,  426,  415,  480,  526,  529,  137,  709,  562,  888,  153,\n",
       "         213,  374, 1081,  546, 1195,  580,  310, 1047,   59,   88,  413,\n",
       "         788,  338,  488,   83, 1179,  506, 1257,  499, 1211,  851,   17,\n",
       "         410,  314, 1276,  418,  820,  672,  500,  886,  766,  423,   49,\n",
       "           5,  340,   32,  644, 1261,   33, 1190, 1248,  745,  280,   40,\n",
       "         715,  641, 1021,  983,  530,  405,  392,   85,  189,  765,   68,\n",
       "         683, 1168, 1272, 1032, 1036,  349, 1115,  603,  910,  668,  252,\n",
       "         906,  164, 1058, 1070,   23,  711,  636,  167,  452, 1076,  673,\n",
       "        1014, 1143,  741, 1201,  321,  272,  282,  978,  142,  171,  903,\n",
       "         867,  860,  657,  893,  199,  884,  495,  680,  913,  634,  157,\n",
       "         920,  323,  383,  578,  702,  819,  107,  852, 1252, 1118, 1214,\n",
       "         995, 1264,  391, 1194, 1187,  206, 1131,  783,  367,  448,  784,\n",
       "         200,  305,  400,  242,  585,  647, 1223,  318,   11, 1242,  870,\n",
       "         759,  424,  379, 1274, 1255,  805,  175,    3,  215,  404,  224,\n",
       "         466,  557, 1050,  666,    1,  377,  622,  501,  209, 1095,  312,\n",
       "         824,  462,  659,  967,  122,  581,  706,  891,  777, 1173],\n",
       "       dtype=int64)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T19:22:26.995203Z",
     "start_time": "2024-12-03T19:22:26.932377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune_layer(layer, prune_indices):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # Prune output channels\n",
    "        weight = layer.weight.detach().cpu()\n",
    "        new_weight = weight[~torch.tensor(prune_indices)].clone()\n",
    "        \n",
    "        # Update Conv2d weights\n",
    "        layer.out_channels = new_weight.size(0)\n",
    "        layer.weight = nn.Parameter(new_weight)\n",
    "        \n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        # Prune BatchNorm parameters\n",
    "        layer.weight = nn.Parameter(layer.weight.detach()[~torch.tensor(prune_indices)].clone())\n",
    "        layer.bias = nn.Parameter(layer.bias.detach()[~torch.tensor(prune_indices)].clone())\n",
    "        layer.running_mean = layer.running_mean.detach()[~torch.tensor(prune_indices)].clone()\n",
    "        layer.running_var = layer.running_var.detach()[~torch.tensor(prune_indices)].clone()\n",
    "\n",
    "        layer.num_features = layer.weight.size(0)\n",
    "    \n",
    "for name, layer in model.named_modules():\n",
    "    if name in pruned_channels:\n",
    "        print(name)\n",
    "        prune_layer(layer, pruned_channels[name])"
   ],
   "id": "a8df93b853c3dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.1\n",
      "features.1.conv.0.1\n",
      "features.1.conv.2\n",
      "features.2.conv.0.1\n",
      "features.2.conv.1.1\n",
      "features.2.conv.3\n",
      "features.3.conv.0.1\n",
      "features.3.conv.1.1\n",
      "features.3.conv.3\n",
      "features.4.conv.0.1\n",
      "features.4.conv.1.1\n",
      "features.4.conv.3\n",
      "features.5.conv.0.1\n",
      "features.5.conv.1.1\n",
      "features.5.conv.3\n",
      "features.6.conv.0.1\n",
      "features.6.conv.1.1\n",
      "features.6.conv.3\n",
      "features.7.conv.0.1\n",
      "features.7.conv.1.1\n",
      "features.7.conv.3\n",
      "features.8.conv.0.1\n",
      "features.8.conv.1.1\n",
      "features.8.conv.3\n",
      "features.9.conv.0.1\n",
      "features.9.conv.1.1\n",
      "features.9.conv.3\n",
      "features.10.conv.0.1\n",
      "features.10.conv.1.1\n",
      "features.10.conv.3\n",
      "features.11.conv.0.1\n",
      "features.11.conv.1.1\n",
      "features.11.conv.3\n",
      "features.12.conv.0.1\n",
      "features.12.conv.1.1\n",
      "features.12.conv.3\n",
      "features.13.conv.0.1\n",
      "features.13.conv.1.1\n",
      "features.13.conv.3\n",
      "features.14.conv.0.1\n",
      "features.14.conv.1.1\n",
      "features.14.conv.3\n",
      "features.15.conv.0.1\n",
      "features.15.conv.1.1\n",
      "features.15.conv.3\n",
      "features.16.conv.0.1\n",
      "features.16.conv.1.1\n",
      "features.16.conv.3\n",
      "features.17.conv.0.1\n",
      "features.17.conv.1.1\n",
      "features.17.conv.3\n",
      "features.18.1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T18:53:26.230475Z",
     "start_time": "2024-12-03T18:53:26.220928Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "e7194af5f832e65d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:08:22.660702Z",
     "start_time": "2024-12-27T19:08:22.261184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "model = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ],
   "id": "da614320e5a33925",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:08:37.271789Z",
     "start_time": "2024-12-27T19:08:37.265296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_weights_ori = 0\n",
    "for i in model._modules['features']._modules['16']._modules['conv']._modules['0']:\n",
    "    # if isinstance(i, nn.BatchNorm2d):\n",
    "    #     print(i.weight.flatten().numel())\n",
    "    #     total_weights_pruned += i.weight.flatten().numel()\n",
    "    if isinstance(i, nn.Conv2d):\n",
    "        print(i.weight.flatten().numel())\n",
    "        total_weights_ori += i.weight.flatten().numel()\n",
    "    # print(i)\n",
    "    # print(i.weight.shape)\n",
    "for i in model._modules['features']._modules['16']._modules['conv']._modules['1']:\n",
    "    # if isinstance(i, nn.BatchNorm2d):\n",
    "    #     print(i.weight.flatten().numel())\n",
    "    #     total_weights_pruned += i.weight.flatten().numel()\n",
    "    if isinstance(i, nn.Conv2d):\n",
    "        print(i.weight.flatten().numel())\n",
    "        total_weights_ori += i.weight.flatten().numel()"
   ],
   "id": "bc2bccfc639a2b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153600\n",
      "8640\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:23:59.518465Z",
     "start_time": "2024-12-27T13:23:59.498419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_norms = []\n",
    "for i in range(1, 18):\n",
    "    if i == 1:\n",
    "        batch_norms.append(f'features.{i}.conv.0.1')\n",
    "        continue    \n",
    "    batch_norms.append(f'features.{i}.conv.1.1')"
   ],
   "id": "f32de3d10a6b8ded",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T13:24:07.754135Z",
     "start_time": "2024-12-27T13:24:07.742790Z"
    }
   },
   "cell_type": "code",
   "source": "batch_norms",
   "id": "27fdb16100d3c119",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.1.conv.0.1',\n",
       " 'features.2.conv.1.1',\n",
       " 'features.3.conv.1.1',\n",
       " 'features.4.conv.1.1',\n",
       " 'features.5.conv.1.1',\n",
       " 'features.6.conv.1.1',\n",
       " 'features.7.conv.1.1',\n",
       " 'features.8.conv.1.1',\n",
       " 'features.9.conv.1.1',\n",
       " 'features.10.conv.1.1',\n",
       " 'features.11.conv.1.1',\n",
       " 'features.12.conv.1.1',\n",
       " 'features.13.conv.1.1',\n",
       " 'features.14.conv.1.1',\n",
       " 'features.15.conv.1.1',\n",
       " 'features.16.conv.1.1',\n",
       " 'features.17.conv.1.1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:46:08.291249Z",
     "start_time": "2024-12-27T11:46:08.283808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Only prune 1 layer\n",
    "batch_norms = [batch_norms[15]]\n",
    "batch_norms"
   ],
   "id": "23f6875b5f2b0414",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.16.conv.1.1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:46:11.219304Z",
     "start_time": "2024-12-27T11:46:11.013448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prune_percentage = 0.1\n",
    "pruned_channels = {}  # To store pruned indices for each layer\n",
    "layer_list = list(model.named_modules())  # Flatten the model's layers for traversal\n",
    "\n",
    "# Step 1: Analyze BatchNorm scaling factors and determine channels to prune\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d) and name in batch_norms:\n",
    "        # Get the scale (\\gamma) values\n",
    "        gamma = module.weight.detach().cpu().numpy()\n",
    "        num_channels = len(gamma)\n",
    "        \n",
    "        # Determine the number of channels to prune\n",
    "        num_prune = int(prune_percentage * num_channels)\n",
    "        \n",
    "        # Identify the indices of the smallest \\gamma values\n",
    "        keep_indices = gamma.argsort()[num_prune:]\n",
    "        pruned_channels[name] = keep_indices\n",
    "\n",
    "# Step 2: Prune layers\n",
    "def prune_layer(layer, prune_indices, is_input=False):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        if is_input:\n",
    "            # Prune input channels\n",
    "            weight = layer.weight.detach().cpu()\n",
    "            new_weight = weight[:, torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "            \n",
    "            layer.in_channels = new_weight.size(1)\n",
    "            layer.weight = nn.Parameter(new_weight).to(device=DEVICE)\n",
    "        else:\n",
    "            # Prune output channels\n",
    "            weight = layer.weight.detach().cpu()\n",
    "            new_weight = weight[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "            \n",
    "            layer.out_channels = new_weight.size(0)\n",
    "            layer.weight = nn.Parameter(new_weight).to(device=DEVICE)\n",
    "            \n",
    "            # Adjust the 'groups' parameter if it's a depthwise convolution\n",
    "            if layer.groups == layer.in_channels:\n",
    "                layer.groups = new_weight.size(0)\n",
    "                layer.in_channels = new_weight.size(0)\n",
    "                \n",
    "    \n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        # Prune BatchNorm parameters\n",
    "        layer.weight = nn.Parameter(layer.weight.detach()[torch.tensor(prune_indices)].clone()).to(device=DEVICE)\n",
    "        layer.bias = nn.Parameter(layer.bias.detach()[torch.tensor(prune_indices)].clone()).to(device=DEVICE)\n",
    "        layer.running_mean = layer.running_mean.detach()[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "        layer.running_var = layer.running_var.detach()[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "        \n",
    "        layer.num_features = layer.weight.size(0)\n",
    "\n",
    "# Traverse the model and prune connected layers\n",
    "for i, (name, module) in enumerate(layer_list):\n",
    "    if name in pruned_channels:\n",
    "        prune_indices = pruned_channels[name]\n",
    "        \n",
    "        # Prune the current BatchNorm layer\n",
    "        prune_layer(module, prune_indices)\n",
    "        \n",
    "        # Prune the preceding Conv2d (output channels)\n",
    "        if i > 0:\n",
    "            prev_name, prev_module = layer_list[i - 1]\n",
    "            if isinstance(prev_module, nn.Conv2d):\n",
    "                prune_layer(prev_module, prune_indices, is_input=False)\n",
    "                if prev_module.groups == prev_module.in_channels:\n",
    "                    j = i - 2\n",
    "                    while j > 0:\n",
    "                        prev_name, prev_module = layer_list[j]\n",
    "                        if isinstance(prev_module, nn.BatchNorm2d):\n",
    "                            prune_layer(prev_module, prune_indices)\n",
    "                            prev_name, prev_module = layer_list[j-1]\n",
    "                            if isinstance(prev_module, nn.Conv2d):\n",
    "                                prune_layer(prev_module, prune_indices, is_input=False)\n",
    "                                break\n",
    "                        j -= 1\n",
    "                        \n",
    "\n",
    "        # Prune the following Conv2d (input channels)\n",
    "        j = i\n",
    "        # if i < len(layer_list) - 1:\n",
    "        #     next_name, next_module = layer_list[i + 2]      # Next conv2d comes after ReLU6 activation layer\n",
    "        #     if isinstance(next_module, nn.Conv2d):\n",
    "        #         prune_layer(next_module, prune_indices, is_input=True)\n",
    "        while j < len(layer_list) - 1:\n",
    "            next_name, next_module = layer_list[j + 1]\n",
    "            if isinstance(next_module, nn.Conv2d):\n",
    "                prune_layer(next_module, prune_indices, is_input=True)\n",
    "                break\n",
    "            j += 1\n"
   ],
   "id": "aa8ed0340e89af60",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:47:00.837476Z",
     "start_time": "2024-12-27T11:47:00.825597Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "affb75a843617553",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(864, 864, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=864, bias=False)\n",
       "          (1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(864, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:19:17.272465Z",
     "start_time": "2024-12-27T21:19:17.266040Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "557e3dab5d4acb40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.models.mobilenetv2.MobileNetV2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0653e-02,  1.3092e+00,  2.5752e+00,  1.4306e+00,  2.3660e+00,\n",
       "          1.6254e+00,  2.0543e+00,  1.8195e-01, -2.5556e-01, -1.5043e+00,\n",
       "         -7.2094e-01,  4.9831e-01,  6.4751e-02,  7.8800e-02, -1.1369e+00,\n",
       "         -1.2337e-01, -2.3843e-01, -4.3499e-01,  1.3113e+00, -8.5748e-01,\n",
       "         -5.2804e-01,  7.2479e-01,  2.0583e-02, -3.9940e-02, -3.0934e-01,\n",
       "         -6.5113e-01, -6.9464e-01, -2.8221e-01, -3.9663e-01,  8.4183e-02,\n",
       "          3.7517e-01,  1.2465e-01, -1.4552e+00,  1.7520e+00,  2.2929e+00,\n",
       "         -4.3805e-01, -3.3361e-02,  3.6526e-01, -1.3277e+00,  1.0576e-01,\n",
       "          2.6324e-01, -1.8242e+00, -7.4608e-01, -3.9004e-01, -1.3254e+00,\n",
       "         -9.3254e-01,  7.4371e-01, -9.4542e-01, -8.2086e-01, -9.7253e-01,\n",
       "          8.0435e-01, -8.6156e-01, -3.3841e-01, -5.0917e-01,  1.3228e-02,\n",
       "          8.0233e-01, -1.0509e+00, -1.2024e+00,  4.5280e-01,  1.5692e-01,\n",
       "          6.1297e-01, -7.4407e-01, -1.4191e+00, -7.2231e-01,  6.0577e-01,\n",
       "          1.2504e+00, -1.1276e+00,  1.9972e-01, -1.2892e+00,  1.9804e-01,\n",
       "         -1.0092e+00, -2.3941e-01, -1.2901e+00, -1.1600e+00, -8.1316e-01,\n",
       "         -3.1598e-01,  2.0041e-01,  1.6921e-01,  2.7424e-01,  1.2900e+00,\n",
       "         -8.0540e-01,  5.0571e-01, -1.8139e-01, -1.0474e+00,  8.6836e-01,\n",
       "         -4.7925e-01, -3.9363e-01,  8.6210e-01,  7.5350e-01,  2.6809e+00,\n",
       "          8.1492e-01, -8.0167e-01,  7.5518e-01,  1.4765e-01,  4.6336e-01,\n",
       "         -1.6263e+00,  9.3574e-01,  1.2700e+00,  6.9699e-01,  1.7234e+00,\n",
       "          1.3369e+00, -2.3607e+00, -1.1716e-01,  2.1760e-01, -2.0038e-01,\n",
       "          3.2699e-01, -3.6704e-01,  2.3326e+00,  7.7515e-01,  1.8129e+00,\n",
       "          2.6303e-01,  1.1484e+00,  9.6162e-01,  1.6232e-01,  4.2539e-01,\n",
       "          4.3519e-01,  8.7531e-02,  2.8257e-01,  8.1507e-01, -7.0103e-01,\n",
       "         -5.1049e-01, -3.6189e-01, -3.4850e-01, -5.8942e-01,  1.4894e-01,\n",
       "          2.4995e-01, -2.8293e-01,  4.9376e-01,  8.3172e-01,  1.7058e+00,\n",
       "          3.5204e-01,  1.3337e-01,  1.4353e+00,  6.0813e-01,  6.2927e-01,\n",
       "         -1.6544e-01,  1.3349e-01,  7.7783e-01, -2.7034e-01, -1.0408e-01,\n",
       "          4.8346e-01,  9.8118e-02, -2.5456e-01,  2.1340e-01,  9.7693e-01,\n",
       "          9.8979e-01,  1.7581e+00,  1.8076e-01,  2.5423e-02,  1.4870e+00,\n",
       "          1.9561e-01,  4.5605e-01, -1.6498e+00,  3.8055e-01,  3.8110e-01,\n",
       "          8.0372e-01, -5.9535e-01, -8.6490e-01, -3.3889e-01, -8.3988e-01,\n",
       "         -7.1946e-01,  7.0433e-01,  9.3622e-01, -2.0559e-01, -8.1673e-01,\n",
       "         -8.6565e-01, -3.1633e-01, -1.7368e+00, -5.9576e-01, -9.7167e-01,\n",
       "         -6.6245e-01,  2.6456e-01, -1.3157e-01, -5.9929e-02, -6.2588e-01,\n",
       "         -1.8142e+00, -9.0786e-01, -1.7629e+00,  5.0640e-01, -1.8544e-01,\n",
       "          2.2418e-01, -1.4251e-01, -3.1731e-01, -3.9076e-01, -4.8013e-01,\n",
       "         -4.3182e-01, -9.6701e-01, -4.2513e-01, -1.1645e+00, -3.8260e-01,\n",
       "         -1.0680e+00, -4.1366e-01, -1.5298e-01, -6.3169e-01, -1.4997e+00,\n",
       "          9.8659e-01,  9.6536e-02, -1.1041e+00, -8.7039e-01,  2.7119e-01,\n",
       "         -9.6758e-01, -1.7318e-01, -5.9593e-02,  1.2711e+00,  2.1713e-01,\n",
       "         -8.8359e-01, -1.0236e+00,  2.4485e-01,  6.8382e-01,  2.4023e-01,\n",
       "         -5.2032e-01,  4.3482e-02,  3.7042e-01, -1.3526e-01, -8.2859e-01,\n",
       "          6.9102e-01,  3.0539e-01,  1.9772e+00,  3.3942e-01,  6.8943e-01,\n",
       "         -1.1909e+00, -7.7524e-02, -1.2022e+00, -3.4129e-01, -1.3133e+00,\n",
       "         -7.0944e-01, -1.3086e+00, -1.0318e-01, -9.0390e-01,  3.1149e-01,\n",
       "          1.8164e-01, -9.0695e-01, -4.5533e-01, -1.9916e+00, -5.2184e-01,\n",
       "         -6.3341e-01, -5.4633e-01, -2.6314e-01, -1.8688e-01, -3.7553e-01,\n",
       "         -1.0388e+00, -5.1664e-01,  3.8540e-01, -2.7942e-02, -1.5767e+00,\n",
       "          1.0370e+00, -1.1781e+00, -1.3940e-02, -1.4515e-01,  2.9385e-01,\n",
       "          7.8003e-01,  4.8088e-01, -1.3202e+00,  2.7906e-01,  1.2213e+00,\n",
       "         -1.7113e+00, -2.1736e-01,  4.9485e-01,  6.1379e-01, -2.2637e-01,\n",
       "         -3.6654e-01, -1.3479e+00, -1.7136e+00,  8.4794e-02,  3.0487e-01,\n",
       "          7.3448e-01,  8.0818e-01, -1.5937e-01, -1.8524e+00, -1.9457e+00,\n",
       "         -8.6986e-01, -2.4728e+00, -2.2322e+00, -2.7216e-02, -2.5268e+00,\n",
       "         -1.9184e+00, -1.5435e+00, -9.5636e-01, -1.4765e+00,  1.4974e-02,\n",
       "         -1.6746e+00,  4.9922e-01,  7.2130e-01,  5.2643e-01,  2.2718e+00,\n",
       "          8.5636e-01, -6.7776e-01, -4.6633e-01, -1.1242e+00, -1.7109e+00,\n",
       "         -1.5934e+00, -1.6580e+00, -1.6058e+00, -5.5417e-01, -1.4200e+00,\n",
       "         -1.7085e+00, -3.6851e-01, -2.7793e+00, -1.2036e+00, -8.8963e-01,\n",
       "         -1.3056e+00,  6.3700e-01, -9.1337e-01, -7.5679e-01, -7.5009e-01,\n",
       "         -2.8571e-01,  6.4155e-01, -2.2181e-01,  5.1408e-01,  4.4347e-01,\n",
       "          1.6923e+00, -6.3504e-01, -2.8519e-01,  1.7339e+00,  1.7224e-01,\n",
       "          6.7070e-01,  4.3908e-02, -9.4588e-01,  7.2392e-01,  5.1990e-01,\n",
       "         -1.2602e-01, -1.2519e+00, -1.3985e+00, -2.3948e-01, -3.2174e-01,\n",
       "         -4.2052e-01, -5.9858e-01,  1.0395e+00,  1.0753e+00,  5.2310e-01,\n",
       "         -4.0362e-01,  5.2839e-02,  6.3205e-01,  7.4666e-01, -1.6089e-01,\n",
       "         -1.8191e+00, -1.6816e+00, -1.0452e-01,  3.4347e-01, -2.0086e+00,\n",
       "         -1.5008e+00, -1.7947e+00, -2.2034e+00, -1.5232e+00, -9.7885e-01,\n",
       "         -1.1958e+00, -1.3521e+00, -1.7429e+00, -5.4779e-01, -2.4283e+00,\n",
       "         -9.7300e-01, -1.6773e+00, -9.5153e-01, -8.0158e-01, -5.2979e-01,\n",
       "         -8.3669e-01,  2.5436e-01, -8.9155e-01, -6.7007e-01,  5.2093e-01,\n",
       "         -6.8039e-01,  2.9748e-01, -9.7041e-01,  8.2261e-01,  2.3728e-01,\n",
       "         -1.0251e+00, -1.9640e+00, -2.0759e+00, -1.0735e+00, -1.5348e+00,\n",
       "         -1.4990e+00, -2.4650e+00, -2.3554e+00, -4.8140e-01, -1.3944e+00,\n",
       "         -1.0234e+00, -7.3856e-01, -1.0879e+00, -5.6658e-01, -1.4065e+00,\n",
       "         -6.3811e-01, -8.2560e-01, -5.6066e-01, -4.7253e-01, -1.1093e+00,\n",
       "         -2.1285e+00, -1.0353e+00, -2.0884e+00, -5.5933e-01, -1.3784e-01,\n",
       "          4.0824e-01,  3.2178e-01,  2.1623e-01,  1.5916e+00, -1.4641e-01,\n",
       "         -2.1729e-01,  4.2984e-01,  7.1772e-01,  4.2052e-01, -1.1128e+00,\n",
       "         -1.7201e+00,  2.3141e-01, -2.1985e-01, -1.5765e+00,  3.9703e-01,\n",
       "          6.1155e-01, -1.0945e+00, -9.1123e-01, -1.9212e+00,  1.8771e+00,\n",
       "          2.5321e-01,  4.2966e-01,  4.6551e-01, -6.0254e-01,  1.1259e-01,\n",
       "         -6.3415e-01, -2.7672e-01,  8.0477e-01,  6.6415e-01,  2.8811e-01,\n",
       "          3.9038e-01,  2.0330e-01, -9.4744e-01, -5.1421e-01, -2.9664e-01,\n",
       "         -5.3108e-01,  5.0724e-01, -4.5270e-01, -8.9397e-01,  9.0540e-01,\n",
       "         -8.0721e-01, -3.8135e-01,  8.1225e-01,  4.9728e-01, -4.6721e-01,\n",
       "          8.4301e-01,  4.0143e-01,  1.3518e+00, -8.8547e-02, -2.6278e+00,\n",
       "          3.3340e-01,  1.4700e-01, -1.1559e-01,  1.0039e+00, -4.1021e-01,\n",
       "          1.6023e-01,  2.8252e+00,  3.5306e-01, -6.8570e-01, -7.0313e-01,\n",
       "         -1.7625e+00, -2.8393e-01,  3.9483e-01,  6.6230e-01, -3.9753e-01,\n",
       "          8.3875e-01,  1.2713e-01,  1.3470e+00, -1.0293e-01,  5.3138e-01,\n",
       "         -8.9400e-02, -1.3730e-01,  1.0958e+00,  9.4529e-01, -2.5630e-01,\n",
       "         -2.3991e+00, -6.7889e-01, -6.1784e-01, -4.4860e-01,  3.7358e-01,\n",
       "          1.8499e+00, -1.2024e+00, -1.5800e-01,  7.2081e-01,  4.0079e-01,\n",
       "         -4.8217e-01, -4.9339e-01, -1.3180e+00,  3.6891e-01, -2.4957e-01,\n",
       "          1.0085e+00, -3.8921e-01,  5.7599e-01, -1.9139e-01,  6.8622e-01,\n",
       "          1.0713e+00, -2.9317e-01,  8.0169e-01, -4.3488e-01, -9.8026e-01,\n",
       "          1.6614e-01, -3.3914e-01,  3.3184e-01, -1.6973e-01,  2.3800e-01,\n",
       "          5.1988e-01,  9.0682e-01,  6.7968e-01,  9.0711e-01, -2.0574e-02,\n",
       "         -1.8594e+00, -1.9636e-01, -6.0242e-01, -9.0307e-02,  8.3944e-01,\n",
       "         -5.3309e-02,  7.2381e-02, -6.8384e-01,  1.9331e+00, -6.4599e-01,\n",
       "         -4.9105e-01, -1.0906e-01,  7.8815e-01,  3.6156e-01, -5.0187e-02,\n",
       "          2.9659e-01, -3.7917e-01,  2.3582e-01, -3.0059e-01, -8.3661e-01,\n",
       "          1.8401e-01,  6.0415e-01, -2.0827e-01,  5.3104e-01, -6.7575e-01,\n",
       "         -6.7928e-01,  5.4476e-01,  1.4870e+00,  9.2918e-01,  3.4708e-01,\n",
       "          1.5476e+00,  4.0033e-01, -9.8013e-02, -1.2698e-01,  1.2250e+00,\n",
       "         -2.4371e+00,  2.2643e-01, -1.7069e+00,  2.0303e+00,  1.2465e+00,\n",
       "          9.0075e-02,  7.2286e-01,  1.0007e+00, -7.0959e-01, -5.9652e-02,\n",
       "          1.3925e+00, -1.9063e-02, -2.1025e+00, -7.0615e-02,  1.8701e+00,\n",
       "         -6.4120e-01, -1.5395e-01,  2.2955e+00,  7.5336e-01, -4.7018e-01,\n",
       "         -3.4057e-02,  3.9132e-01,  1.5255e+00, -2.2386e-02,  5.3109e-01,\n",
       "          1.7659e-01, -1.1575e+00,  5.6880e-01,  4.3522e-01, -1.0140e+00,\n",
       "         -1.6170e+00,  5.0175e-01,  5.7939e-01, -3.2337e-01, -1.3816e+00,\n",
       "          2.0276e-01, -5.4734e-01,  7.6455e-01, -2.4664e+00,  1.9431e+00,\n",
       "         -1.7527e-01, -9.5630e-01,  3.7968e-01,  6.7550e-01,  7.6923e-01,\n",
       "          1.1143e+00,  6.9048e-01, -1.3182e-01, -1.3091e-01, -7.5805e-02,\n",
       "          3.7208e-01, -2.2759e+00,  8.2905e-01,  4.5831e-01,  8.7209e-01,\n",
       "         -3.1808e-02,  6.2364e-01,  2.7729e-01,  1.3078e-01, -1.0386e+00,\n",
       "         -6.5701e-01,  1.8899e-02, -9.1875e-01,  1.3548e+00,  9.9587e-01,\n",
       "          4.7458e-01,  6.2585e-01, -4.4249e-01, -2.9462e+00, -8.1579e-02,\n",
       "          8.1673e-01,  1.4134e+00,  5.7850e-01,  6.1188e-01,  3.4374e-01,\n",
       "         -7.5871e-01,  8.2372e-01, -1.6416e+00,  9.8158e-01, -8.7953e-01,\n",
       "         -1.2594e+00,  6.2500e-01,  1.2558e+00,  1.4420e+00,  1.6410e+00,\n",
       "          2.9583e+00,  5.5229e-02,  6.2354e-01,  9.1797e-01,  1.3692e-01,\n",
       "         -1.1446e+00,  2.4987e-01, -5.8084e-01,  3.3620e-01,  2.0773e-01,\n",
       "         -2.1327e-01,  6.0812e-01,  1.3394e+00,  1.4377e+00, -2.3698e+00,\n",
       "          5.6665e-01,  6.6027e-01,  6.9406e-02,  6.4324e-01,  7.4386e-01,\n",
       "         -3.7416e-01,  4.5503e-01,  1.1295e+00,  6.8426e-02,  9.1897e-01,\n",
       "          6.0717e-01,  1.6818e-01, -1.5600e-02,  5.7047e-03, -1.3981e-01,\n",
       "          1.4840e+00,  4.5259e-01, -1.7479e+00, -1.0590e+00, -9.2659e-01,\n",
       "          1.0952e+00, -4.1999e-01,  8.0851e-01, -9.6052e-02,  2.2949e-01,\n",
       "         -5.3218e-01, -9.0448e-01,  1.3739e+00, -1.3036e+00,  2.6558e+00,\n",
       "         -5.2937e-01, -3.0612e-01, -3.4966e-01,  1.1199e+00,  8.0371e-01,\n",
       "         -5.6922e-01, -1.9468e+00,  7.1381e-01,  1.9191e+00, -2.1600e-01,\n",
       "         -2.4352e+00, -1.4480e+00,  3.6633e-02, -1.4281e-01,  2.3435e-03,\n",
       "          5.5374e-02,  2.7086e+00,  9.1488e-01,  8.2019e-01,  2.8198e-01,\n",
       "         -7.8610e-01, -1.4532e+00, -1.3745e+00,  1.3722e+00, -7.1725e-01,\n",
       "         -2.4780e+00,  1.9123e-01,  6.4122e-01,  4.7236e-01, -1.0983e+00,\n",
       "         -1.3148e+00,  1.4801e+00,  6.5366e-02,  5.5937e-02, -5.8381e-01,\n",
       "          3.9222e-01,  1.2692e+00, -1.7222e-01, -2.3650e-01,  8.5531e-01,\n",
       "         -1.1129e+00,  5.4779e-01,  1.1376e+00, -4.6109e-01,  4.8265e-01,\n",
       "         -1.4504e-01, -1.1864e-01,  6.3911e-01,  1.2931e+00, -1.1473e-01,\n",
       "         -1.9545e+00,  1.0150e+00, -7.4652e-01,  6.3795e-01,  8.3667e-01,\n",
       "          4.6045e-01,  1.5112e+00,  2.1932e+00,  8.6394e-01, -8.1621e-01,\n",
       "          9.1931e-01, -9.7783e-01,  2.0155e+00,  6.0534e-01, -1.4213e+00,\n",
       "         -1.7397e-01,  7.1493e-01, -2.0375e-01,  7.9163e-01, -1.6943e+00,\n",
       "          5.4325e-01,  2.0091e+00,  1.2809e+00,  4.9471e-01, -1.6144e+00,\n",
       "          1.0518e+00,  5.7264e-01,  1.5768e+00, -1.1138e+00,  8.0406e-01,\n",
       "          1.0549e+00, -8.7098e-01,  6.1892e-02, -2.5379e-01,  2.4351e+00,\n",
       "          1.0393e+00, -1.8568e+00,  8.2360e-01,  6.8379e-01,  1.5943e+00,\n",
       "          4.2742e-01,  7.6949e-01, -1.5317e+00,  4.1934e-01,  2.3664e-01,\n",
       "          7.9575e-01,  1.2047e+00,  3.5431e-01, -2.2937e-01, -2.9022e-01,\n",
       "          1.7555e-01, -1.3672e+00,  2.7489e-01,  5.2672e-02,  1.2394e+00,\n",
       "         -3.0729e-01,  2.0352e-01,  2.7157e-01,  8.8598e-01,  8.0372e-02,\n",
       "          5.9499e-01,  6.1088e-01,  2.7930e-01,  5.9336e-01, -3.9390e-02,\n",
       "          7.6039e-02, -2.5832e-01,  2.4337e+00,  7.0088e-01,  1.1573e+00,\n",
       "         -7.7285e-02,  6.3480e-01, -4.5089e-01, -4.5618e-01,  9.8183e-01,\n",
       "         -1.0114e-01,  5.6138e-01,  8.7832e-01,  1.0826e+00,  1.2496e+00,\n",
       "          1.1098e+00,  1.2487e-01,  1.0669e+00, -3.6233e-01,  1.5581e+00,\n",
       "          9.9515e-01,  2.6531e+00, -1.3226e+00, -9.8924e-01, -1.9148e-01,\n",
       "          7.8205e-01,  6.7389e-01,  1.9196e-01, -4.9724e-01,  1.0815e+00,\n",
       "          5.1709e-02,  1.2061e+00,  9.5149e-01,  6.6461e-01, -2.8875e-01,\n",
       "          2.5616e-01, -1.0545e-01, -1.2084e+00,  1.9597e+00,  5.2667e-01,\n",
       "         -2.2467e+00,  4.3238e-01,  8.3938e-01,  1.2908e+00,  8.9568e-01,\n",
       "         -9.7782e-01,  8.0320e-01, -8.7736e-02,  1.3641e+00,  2.3913e-01,\n",
       "         -6.3942e-01,  6.1855e-03, -3.4621e-01, -5.7120e-01,  4.0813e-01,\n",
       "         -2.3407e-01,  3.9241e-01,  6.6897e-01,  3.9831e-01,  2.6117e-01,\n",
       "          5.5809e-01, -2.0337e-01,  9.7649e-01, -6.4894e-01,  3.1800e-01,\n",
       "          1.7665e+00,  1.5449e+00, -2.2339e+00,  3.7137e-01,  8.3771e-01,\n",
       "          1.9496e-01,  1.8801e+00,  1.8514e+00, -7.6686e-01, -8.2459e-01,\n",
       "          3.2398e-01, -1.8530e+00, -1.6902e+00, -6.0058e-01,  4.3603e-01,\n",
       "         -1.2811e+00,  1.0775e+00,  2.4971e-01, -1.0873e+00, -1.6588e+00,\n",
       "         -1.0717e+00, -3.6789e-01, -1.4232e+00,  6.6479e-01,  3.7888e-01,\n",
       "         -2.2381e-01, -2.8937e-01,  7.5141e-01, -1.0910e-01, -1.3011e+00,\n",
       "          4.0586e-01,  1.0239e+00, -5.3214e-02, -2.9754e-01,  1.7894e+00,\n",
       "          3.3262e-02, -6.5668e-01,  1.2013e+00,  2.7072e-01, -3.1714e-01,\n",
       "          1.3023e-01,  9.2654e-01, -7.7509e-01, -8.1329e-02,  3.5173e-02,\n",
       "         -1.3438e+00, -4.1291e-01,  2.0113e+00, -1.9628e-01,  2.4852e-01,\n",
       "         -2.5200e-01,  2.9033e-01,  1.1667e+00,  1.3851e+00,  1.0922e+00,\n",
       "          1.3413e-01, -8.3449e-01,  4.2135e-01,  1.2182e+00,  1.5975e+00,\n",
       "          1.1118e+00,  5.1458e-01,  5.6460e-01,  1.7733e+00,  5.5363e-01,\n",
       "          6.8720e-01,  1.1482e-01, -1.6465e+00, -3.2696e-01, -8.3088e-01,\n",
       "         -1.2556e+00,  1.4806e+00,  6.1642e-01,  7.3102e-01, -8.8221e-02,\n",
       "          5.4616e-01,  1.2945e+00,  7.1031e-01, -3.5202e-01,  1.7766e-01,\n",
       "          4.5973e-01, -5.0501e-01, -1.2720e-01,  5.0761e-01,  8.3118e-01,\n",
       "         -1.0572e+00, -7.7051e-01, -1.6740e-01,  1.9400e-01, -1.2655e-02,\n",
       "          8.4932e-01, -2.6912e-01,  7.8619e-01,  4.4455e-01,  7.6589e-02,\n",
       "         -4.2157e-01, -1.1454e+00, -7.4930e-01,  3.8911e-01, -9.7364e-01,\n",
       "          5.0642e-01, -6.6018e-01,  1.0835e+00,  1.4607e+00,  5.3515e-01,\n",
       "          1.5309e+00,  1.2956e+00,  9.7621e-01,  1.2607e+00,  1.5468e+00,\n",
       "          5.8378e-01,  5.2449e-01,  2.1855e-01,  3.9257e-01, -3.4344e-01,\n",
       "         -5.0764e-01,  6.5972e-01, -3.0653e-01,  3.9609e-01,  1.5314e-01,\n",
       "         -5.4851e-01,  3.4786e-01,  3.1084e-01,  1.2307e+00,  1.2100e-01,\n",
       "         -3.4267e-01,  2.2178e+00, -3.7341e-01,  1.3958e+00, -5.7073e-01,\n",
       "          7.0722e-01,  8.3154e-01,  2.1662e+00,  1.9146e+00, -8.0693e-01,\n",
       "          7.7351e-01, -3.7391e-01,  1.1335e+00,  1.8212e+00,  2.0195e-01,\n",
       "          5.0875e-01, -2.3973e-01,  2.8334e-01, -2.8647e-01, -6.0853e-01,\n",
       "         -6.0355e-02, -2.3849e-01,  5.4223e-01, -1.1345e+00, -8.8847e-01,\n",
       "         -6.2733e-01, -7.0919e-01,  3.4273e-01,  4.3829e-01,  7.5316e-01]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3,
   "source": [
    "input = torch.randn((1,3,224,224)).to(device=\"cuda\")\n",
    "net(input)"
   ],
   "id": "83fa190074f476ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T21:40:06.829782Z",
     "start_time": "2024-12-03T21:40:06.822815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "module_name = 'features.0'\n",
    "module = model\n",
    "for name in module_name.split('.'):\n",
    "    print(name)\n",
    "    module = module._modules[name]\n",
    "    print(module)"
   ],
   "id": "1ca5c2e7fb56db37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      "  (1): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (3): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (4): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (9): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (10): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (11): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (12): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (13): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (14): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (15): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (16): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (17): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (18): Conv2dNormActivation(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "Conv2dNormActivation(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU6(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:57:31.223264Z",
     "start_time": "2024-12-27T21:57:31.213743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        # Get the scale values\n",
    "        gamma = module.weight.detach().cpu().numpy()\n",
    "        print(gamma.shape)\n",
    "        num_channels = len(gamma)\n",
    "        print(num_channels)"
   ],
   "id": "a11e297418b2d7e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "32\n",
      "(32,)\n",
      "32\n",
      "(16,)\n",
      "16\n",
      "(96,)\n",
      "96\n",
      "(96,)\n",
      "96\n",
      "(24,)\n",
      "24\n",
      "(144,)\n",
      "144\n",
      "(144,)\n",
      "144\n",
      "(24,)\n",
      "24\n",
      "(144,)\n",
      "144\n",
      "(144,)\n",
      "144\n",
      "(32,)\n",
      "32\n",
      "(192,)\n",
      "192\n",
      "(192,)\n",
      "192\n",
      "(32,)\n",
      "32\n",
      "(192,)\n",
      "192\n",
      "(192,)\n",
      "192\n",
      "(32,)\n",
      "32\n",
      "(192,)\n",
      "192\n",
      "(192,)\n",
      "192\n",
      "(64,)\n",
      "64\n",
      "(384,)\n",
      "384\n",
      "(384,)\n",
      "384\n",
      "(64,)\n",
      "64\n",
      "(384,)\n",
      "384\n",
      "(384,)\n",
      "384\n",
      "(64,)\n",
      "64\n",
      "(384,)\n",
      "384\n",
      "(384,)\n",
      "384\n",
      "(64,)\n",
      "64\n",
      "(384,)\n",
      "384\n",
      "(384,)\n",
      "384\n",
      "(96,)\n",
      "96\n",
      "(576,)\n",
      "576\n",
      "(576,)\n",
      "576\n",
      "(96,)\n",
      "96\n",
      "(576,)\n",
      "576\n",
      "(576,)\n",
      "576\n",
      "(96,)\n",
      "96\n",
      "(576,)\n",
      "576\n",
      "(576,)\n",
      "576\n",
      "(160,)\n",
      "160\n",
      "(960,)\n",
      "960\n",
      "(960,)\n",
      "960\n",
      "(160,)\n",
      "160\n",
      "(960,)\n",
      "960\n",
      "(960,)\n",
      "960\n",
      "(160,)\n",
      "160\n",
      "(960,)\n",
      "960\n",
      "(960,)\n",
      "960\n",
      "(320,)\n",
      "320\n",
      "(1280,)\n",
      "1280\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:19:55.284692Z",
     "start_time": "2024-12-27T21:19:55.205630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in model._modules['features']._modules['5']._modules['conv']._modules['1']:\n",
    "    print(i)\n",
    "    print(i.weight.shape)\n",
    "    print(type(i))"
   ],
   "id": "1b022d8acc187737",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "torch.Size([192, 1, 3, 3])\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([192])\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "ReLU6(inplace=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU6' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m5\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconv\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(i))\n",
      "File \u001B[1;32mD:\\dev-workspace\\model-compression\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1727\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1728\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1729\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ReLU6' object has no attribute 'weight'"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:08:02.502731Z",
     "start_time": "2024-12-27T19:08:02.498076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_weights_pruned = 0\n",
    "for i in model._modules['features']._modules['16']._modules['conv']._modules['0']:\n",
    "    # if isinstance(i, nn.BatchNorm2d):\n",
    "    #     print(i.weight.flatten().numel())\n",
    "    #     total_weights_pruned += i.weight.flatten().numel()\n",
    "    if isinstance(i, nn.Conv2d):\n",
    "        print(i.weight.flatten().numel())\n",
    "        total_weights_pruned += i.weight.flatten().numel()\n",
    "    # print(i)\n",
    "    # print(i.weight.shape)\n",
    "for i in model._modules['features']._modules['16']._modules['conv']._modules['1']:\n",
    "    # if isinstance(i, nn.BatchNorm2d):\n",
    "    #     print(i.weight.flatten().numel())\n",
    "    #     total_weights_pruned += i.weight.flatten().numel()\n",
    "    if isinstance(i, nn.Conv2d):\n",
    "        print(i.weight.flatten().numel())\n",
    "        total_weights_pruned += i.weight.flatten().numel()"
   ],
   "id": "24fd035050b5403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138240\n",
      "7776\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:08:56.950562Z",
     "start_time": "2024-12-27T19:08:56.945060Z"
    }
   },
   "cell_type": "code",
   "source": "total_weights_ori",
   "id": "3266f88ac0bb929b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162240"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:09:22.755880Z",
     "start_time": "2024-12-27T19:09:22.750973Z"
    }
   },
   "cell_type": "code",
   "source": "total_weights_pruned / total_weights_ori ",
   "id": "123fdf576db64154",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T19:07:12.821906Z",
     "start_time": "2024-12-27T19:07:12.803731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in model._modules['features']._modules['5']._modules['conv']._modules['0']:\n",
    "    print(i)\n",
    "    print(i.weight.shape)"
   ],
   "id": "a15be1e89010675d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "torch.Size([192, 32, 1, 1])\n",
      "BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "torch.Size([192])\n",
      "ReLU6(inplace=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU6' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m5\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconv\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39m_modules[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mD:\\dev-workspace\\model-compression\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1727\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1728\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1729\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ReLU6' object has no attribute 'weight'"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:01:05.630618Z",
     "start_time": "2024-12-27T21:01:05.622023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prune_percentage = 0.9\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        # Get the scale (\\gamma) values\n",
    "        gamma = module.weight.detach().cpu().numpy()\n",
    "        num_channels = len(gamma)\n",
    "        \n",
    "        # Determine the number of channels to prune\n",
    "        num_prune = int(prune_percentage * num_channels)\n",
    "        \n",
    "        # Identify the indices of the smallest \\gamma values\n",
    "        keep_indices = gamma.argsort()[num_prune:]\n",
    "        pruned_channels[name] = keep_indices"
   ],
   "id": "a3d91806c7dee691",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:02:38.751422Z",
     "start_time": "2024-12-27T21:02:38.746129Z"
    }
   },
   "cell_type": "code",
   "source": "gamma",
   "id": "d0a0e247b2f564a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4253117, 1.4295772, 1.4305606, ..., 1.6586254, 1.7704152,\n",
       "       1.7989348], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:21:46.879717Z",
     "start_time": "2024-12-27T11:21:46.866381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"sensivity_analysis.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "data"
   ],
   "id": "33b84599409b5524",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'yolov7': {'inference_time_gpu': [1, 2, 3],\n",
       "   'inference_time_cpu': 0.8197850699994887,\n",
       "   'num_parameter': 6033930,\n",
       "   'memory_usage': 12.06786}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:42:05.335335Z",
     "start_time": "2024-12-27T11:42:05.328631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {}\n",
    "for bn in batch_norms:\n",
    "    data[bn] = []"
   ],
   "id": "6270fbd00306d1a5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T11:43:09.976451Z",
     "start_time": "2024-12-27T11:43:09.966704Z"
    }
   },
   "cell_type": "code",
   "source": "data[bn]",
   "id": "5a6a444d1a3d2fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Pruning",
   "id": "e5818d5df09d1049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:58:33.319136Z",
     "start_time": "2024-12-28T18:58:33.224635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.mobilenet_v2(weights='MobileNet_V2_Weights.IMAGENET1K_V1')\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ],
   "id": "f5328c54af35ea64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:58:35.591794Z",
     "start_time": "2024-12-28T18:58:35.587424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_norms = []\n",
    "for i in range(1, 18):\n",
    "    if i == 1:\n",
    "        batch_norms.append(f'features.{i}.conv.0.1')\n",
    "        continue    \n",
    "    batch_norms.append(f'features.{i}.conv.1.1')"
   ],
   "id": "58d8d5bde5eb18f1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:57:02.380818Z",
     "start_time": "2024-12-28T19:57:01.221278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruning_ratio = 0.1\n",
    "\n",
    "all_gammas = torch.cat([module.weight.flatten() for name, module in model.named_modules() if isinstance(module, nn.BatchNorm2d) and name in batch_norms])\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, nn.BatchNorm2d):\n",
    "#         # Get the scale values\n",
    "#         gamma = module.weight\n",
    "        # batch_norms[name] = list(gamma)\n",
    "        \n",
    "        # total_channels += len(gamma)\n",
    " \n",
    "prune_target = int(all_gammas.size(0) * pruning_ratio)  \n",
    "threshold = torch.topk(all_gammas, prune_target, largest=False).values[-1]"
   ],
   "id": "1f99394cbcd6b0f5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:57:07.102470Z",
     "start_time": "2024-12-28T19:57:07.089457Z"
    }
   },
   "cell_type": "code",
   "source": "threshold",
   "id": "7d309fd4581f109",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1308, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:58:49.408131Z",
     "start_time": "2024-12-28T18:58:49.399132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "std_dev = np.array([np.std(module.weight.detach().cpu().numpy()) for name, module in model.named_modules() if isinstance(module, nn.BatchNorm2d) and name in batch_norms])\n",
    "std_dev"
   ],
   "id": "458f5c38d845d956",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3246427 , 0.11976993, 0.06118749, 0.07926096, 0.0611471 ,\n",
       "       0.05634503, 0.04216883, 0.0448107 , 0.04583936, 0.04520043,\n",
       "       0.05498647, 0.04713114, 0.04968606, 0.05315563, 0.05127232,\n",
       "       0.05739376, 0.04744095], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:58:55.719507Z",
     "start_time": "2024-12-28T18:58:55.703597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open('../../notebooks/sensivity_analysis.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data"
   ],
   "id": "52fdfe85354c069e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features.1.conv.0.1': {'top1': [0.71096, 0.5757, 0.03474, 0.0022, 0.00114],\n",
       "  'top5': [0.89948, 0.813, 0.09788, 0.00736, 0.00558],\n",
       "  'loss': [14.403162048663944,\n",
       "   22.223612068686634,\n",
       "   103.64576007798314,\n",
       "   131.74684715643525,\n",
       "   138.54842003434896]},\n",
       " 'features.2.conv.1.1': {'top1': [0.57006, 0.02824, 0.00234, 0.00102, 0.00162],\n",
       "  'top5': [0.80558, 0.09398, 0.0128, 0.00766, 0.00534],\n",
       "  'loss': [22.71068325964734,\n",
       "   109.75838568434119,\n",
       "   157.76405614987016,\n",
       "   167.2277132384479,\n",
       "   113.75423611328006]},\n",
       " 'features.3.conv.1.1': {'top1': [0.3765, 0.00972, 0.00236, 0.00208, 0.00204],\n",
       "  'top5': [0.61622, 0.03884, 0.01028, 0.0079, 0.0078],\n",
       "  'loss': [38.63741221651435,\n",
       "   106.57205509021878,\n",
       "   131.6942581795156,\n",
       "   130.46310277096927,\n",
       "   131.80611641332507]},\n",
       " 'features.4.conv.1.1': {'top1': [0.38188, 0.0451, 0.00462, 0.00156, 0.00108],\n",
       "  'top5': [0.61692, 0.11048, 0.01654, 0.00704, 0.00516],\n",
       "  'loss': [38.688165878877044,\n",
       "   90.7589758131653,\n",
       "   114.93547889590263,\n",
       "   107.02099776268005,\n",
       "   109.59588756784797]},\n",
       " 'features.5.conv.1.1': {'top1': [0.6069, 0.62694, 0.48994, 0.38124, 0.19686],\n",
       "  'top5': [0.83262, 0.84834, 0.7356, 0.61546, 0.37608],\n",
       "  'loss': [20.447602433618158,\n",
       "   19.119747966295108,\n",
       "   28.76720022317022,\n",
       "   39.07511836849153,\n",
       "   60.880392072722316]},\n",
       " 'features.6.conv.1.1': {'top1': [0.61492, 0.23106, 0.18622, 0.28926, 0.3051],\n",
       "  'top5': [0.83726, 0.43712, 0.36168, 0.5079, 0.53326],\n",
       "  'loss': [20.01523323962465,\n",
       "   59.9209018656984,\n",
       "   67.78178645856678,\n",
       "   49.83770241867751,\n",
       "   47.46645291894674]},\n",
       " 'features.7.conv.1.1': {'top1': [0.54134, 0.22984, 0.00826, 0.0013, 0.001],\n",
       "  'top5': [0.77294, 0.42662, 0.0265, 0.00762, 0.00518],\n",
       "  'loss': [25.470998293720186,\n",
       "   56.500871101394296,\n",
       "   107.62498717010021,\n",
       "   110.88355255126953,\n",
       "   107.20022105798125]},\n",
       " 'features.8.conv.1.1': {'top1': [0.6854, 0.66142, 0.62424, 0.32742, 0.06948],\n",
       "  'top5': [0.88574, 0.8692, 0.8417, 0.55288, 0.16748],\n",
       "  'loss': [15.842723487992771,\n",
       "   17.202552509494126,\n",
       "   19.620376507751644,\n",
       "   44.18108316138387,\n",
       "   90.33383758366108]},\n",
       " 'features.9.conv.1.1': {'top1': [0.6898, 0.6697, 0.66226, 0.62982, 0.61246],\n",
       "  'top5': [0.8877, 0.87602, 0.87038, 0.8512, 0.83834],\n",
       "  'loss': [15.629996660689358,\n",
       "   16.68985240231268,\n",
       "   17.231718708528206,\n",
       "   19.108421202981845,\n",
       "   20.384089114842936]},\n",
       " 'features.10.conv.1.1': {'top1': [0.6878, 0.63686, 0.59266, 0.56456, 0.53012],\n",
       "  'top5': [0.88706, 0.85544, 0.82604, 0.8044, 0.77394],\n",
       "  'loss': [15.692610385362059,\n",
       "   18.53071971400641,\n",
       "   21.33095110184513,\n",
       "   23.10674373502843,\n",
       "   25.75267687626183]},\n",
       " 'features.11.conv.1.1': {'top1': [0.59084, 0.1408, 0.01428, 0.00234, 0.00112],\n",
       "  'top5': [0.821, 0.3056, 0.04548, 0.01046, 0.00626],\n",
       "  'loss': [21.376359459245577,\n",
       "   66.13404728472233,\n",
       "   94.96281173080206,\n",
       "   103.78863148391247,\n",
       "   99.8527332097292]},\n",
       " 'features.12.conv.1.1': {'top1': [0.65336,\n",
       "   0.59428,\n",
       "   0.51166,\n",
       "   0.41676,\n",
       "   0.26124],\n",
       "  'top5': [0.86686, 0.82788, 0.76342, 0.67856, 0.49868],\n",
       "  'loss': [17.45897646236699,\n",
       "   20.840168295428157,\n",
       "   26.16386583633721,\n",
       "   32.946845727041364,\n",
       "   47.80971038714051]},\n",
       " 'features.13.conv.1.1': {'top1': [0.53336, 0.36828, 0.22414, 0.13282, 0.0723],\n",
       "  'top5': [0.77098, 0.6211, 0.44282, 0.30098, 0.18802],\n",
       "  'loss': [25.66058163717389,\n",
       "   38.06431659217924,\n",
       "   52.00654382817447,\n",
       "   63.18616998940706,\n",
       "   76.91957684978843]},\n",
       " 'features.14.conv.1.1': {'top1': [0.40792,\n",
       "   0.09206,\n",
       "   0.01972,\n",
       "   0.00408,\n",
       "   0.00092],\n",
       "  'top5': [0.64698, 0.21644, 0.05972, 0.01464, 0.00666],\n",
       "  'loss': [36.70771576068364,\n",
       "   98.96778907068074,\n",
       "   167.7936252579093,\n",
       "   243.05842252075672,\n",
       "   359.27527437359095]},\n",
       " 'features.15.conv.1.1': {'top1': [0.63466,\n",
       "   0.53418,\n",
       "   0.33662,\n",
       "   0.26974,\n",
       "   0.17388],\n",
       "  'top5': [0.85596, 0.77438, 0.58126, 0.50012, 0.372],\n",
       "  'loss': [18.924303645035252,\n",
       "   27.412310465006158,\n",
       "   53.56805336102843,\n",
       "   61.64406030345708,\n",
       "   74.3588215392083]},\n",
       " 'features.16.conv.1.1': {'top1': [0.5826, 0.43266, 0.31082, 0.28866, 0.21762],\n",
       "  'top5': [0.81756, 0.67786, 0.5549, 0.53562, 0.43536],\n",
       "  'loss': [27.756535032764077,\n",
       "   38.68137038871646,\n",
       "   48.781712766736746,\n",
       "   45.28970911446959,\n",
       "   51.217913798987865]},\n",
       " 'features.17.conv.1.1': {'top1': [0.50698, 0.13684, 0.08666, 0.03252, 0.0041],\n",
       "  'top5': [0.74682, 0.31034, 0.18544, 0.07778, 0.01812],\n",
       "  'loss': [28.622359328903258,\n",
       "   73.74094871524721,\n",
       "   106.46136314142495,\n",
       "   156.95356149598956,\n",
       "   209.1782298721373]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:55:25.630543Z",
     "start_time": "2024-12-28T19:55:25.622155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate layer sensivity\n",
    "top1 = 71.1\n",
    "top5 = 89.956\n",
    "scales = {}\n",
    "for i, bn in enumerate(data.keys()):\n",
    "    layer_sensivity = data[bn][\"top5\"][0] - data[bn][\"top5\"][-1] + (top5 / 100 - data[bn][\"top5\"][0])\n",
    "    scales[bn] = np.array(layer_sensivity) * std_dev[i]"
   ],
   "id": "c53c236f862034a4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:55:32.656923Z",
     "start_time": "2024-12-28T19:55:32.643533Z"
    }
   },
   "cell_type": "code",
   "source": "scales",
   "id": "8a047136cb16a467",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features.1.conv.0.1': 0.290224070250392,\n",
       " 'features.2.conv.1.1': 0.10710066755533218,\n",
       " 'features.3.conv.1.1': 0.05456456013649702,\n",
       " 'features.4.conv.1.1': 0.0708910028219223,\n",
       " 'features.5.conv.1.1': 0.0320092864985764,\n",
       " 'features.6.conv.1.1': 0.020639183368161326,\n",
       " 'features.7.conv.1.1': 0.037714954476952556,\n",
       " 'features.8.conv.1.1': 0.03280501537919045,\n",
       " 'features.9.conv.1.1': 0.0028062857322394873,\n",
       " 'features.10.conv.1.1': 0.005678077998608354,\n",
       " 'features.11.conv.1.1': 0.04911940982975065,\n",
       " 'features.12.conv.1.1': 0.01889392982363701,\n",
       " 'features.13.conv.1.1': 0.03535361602328718,\n",
       " 'features.14.conv.1.1': 0.047462662765383724,\n",
       " 'features.15.conv.1.1': 0.027049223961234093,\n",
       " 'features.16.conv.1.1': 0.02664218142554164,\n",
       " 'features.17.conv.1.1': 0.0418163540995121}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:58:18.090954Z",
     "start_time": "2024-12-28T19:58:16.998910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_gammas_scaled = torch.cat([module.weight.flatten() / scales[name] for name, module in model.named_modules() if\n",
    "                                isinstance(module, nn.BatchNorm2d) and name in batch_norms])\n",
    "\n",
    "prune_target = int(all_gammas.size(0) * pruning_ratio)  \n",
    "threshold_scaled = torch.topk(all_gammas_scaled, prune_target, largest=False).values[-1]\n",
    "threshold_scaled"
   ],
   "id": "d6d3b53db8396002",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3956, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T19:56:16.742084Z",
     "start_time": "2024-12-28T19:56:16.722319Z"
    }
   },
   "cell_type": "code",
   "source": "print(all_gammas_scaled)",
   "id": "d8e742497682f7c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0075, 0.1597, 0.2076,  ..., 0.0059, 0.0096, 0.0089], device='cuda:0',\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "thresholds = {}\n",
    "for i, bn in enumerate(batch_norms):\n",
    "    thresholds[bn] = threshold * scale[i]"
   ],
   "id": "8b01a80c393b0503"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T11:01:49.842665Z",
     "start_time": "2024-12-28T11:01:49.818892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d) and name in batch_norms:\n",
    "        print(torch.std(module.weight))\n",
    "        print(np.std(module.weight.detach().cpu().numpy()))\n",
    "        "
   ],
   "id": "9dd066f9ec9298ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3298, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.3246427\n",
      "tensor(0.1204, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.11976993\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.061187495\n",
      "tensor(0.0795, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.07926096\n",
      "tensor(0.0613, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.061147105\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.056345027\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.042168826\n",
      "tensor(0.0449, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.044810697\n",
      "tensor(0.0459, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.04583936\n",
      "tensor(0.0453, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.04520043\n",
      "tensor(0.0551, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.054986466\n",
      "tensor(0.0472, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.047131136\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.049686056\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.05315563\n",
      "tensor(0.0513, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.051272318\n",
      "tensor(0.0574, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.057393756\n",
      "tensor(0.0475, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "0.047440954\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:18:11.530199Z",
     "start_time": "2024-12-28T01:18:11.522701Z"
    }
   },
   "cell_type": "code",
   "source": "prune_target",
   "id": "c075dbe6137d68ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T22:54:57.288609Z",
     "start_time": "2024-12-27T22:54:57.226538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open( \"./test.txt\", \"w\") as f:\n",
    "    for v in torch.topk(all_gammas, prune_target, largest=False).values:\n",
    "        f.writelines(f\"{str(v.item())}\\n\")"
   ],
   "id": "8f3334d4538a8ab9",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:17:49.364809Z",
     "start_time": "2024-12-28T01:17:49.357886Z"
    }
   },
   "cell_type": "code",
   "source": "threshold_.item()",
   "id": "51cb13e9145a50ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1308184266090393"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 358
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 252,
   "source": [
    "batch_norms = []\n",
    "for i in range(1, 18):\n",
    "    if i == 1:\n",
    "        batch_norms.append(f'features.{i}.conv.0.1')\n",
    "        continue    \n",
    "    batch_norms.append(f'features.{i}.conv.1.1')"
   ],
   "id": "301d6775bc6c728f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T22:47:36.515805Z",
     "start_time": "2024-12-27T22:47:36.511943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_norms = [batch_norms[16]]\n",
    "batch_norms"
   ],
   "id": "965f38416283090e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.17.conv.1.1']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T20:05:34.742476Z",
     "start_time": "2024-12-28T20:05:33.661187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "layer_list = list(model.named_modules())  # Flatten the model's layers for traversal\n",
    "num_gamma = {}\n",
    "num_gamma_pruned = {}\n",
    "pruned_channels = {}\n",
    "\n",
    "# scale = 10**7  # Scaling factor for 7 decimals\n",
    "# print(threshold.item())\n",
    "# threshold = np.ceil(threshold.detach().cpu().numpy() * scale) / scale\n",
    "# print(threshold.item())\n",
    "# threshold = threshold_.detach().cpu().numpy()\n",
    "# print(threshold + 1e-5)\n",
    "\n",
    "# Step 1: Analyze BatchNorm scaling factors and determine channels to prune\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.BatchNorm2d) and name in batch_norms:\n",
    "        # Get the scale (\\gamma) values\n",
    "        # gamma = module.weight.detach().cpu().numpy()\n",
    "        gamma = module.weight.data\n",
    "        \n",
    "        # print(module.weight.detach().cpu().numpy().dtype)\n",
    "        # print(threshold.dtype)\n",
    "\n",
    "    \n",
    "        # keep_indices = np.array(np.where(gamma > threshold + 1e-2)[0])\n",
    "        # keep_indices = torch.where(gamma > threshold_)[0]\n",
    "        keep_indices = torch.where(gamma / scales[name] > threshold_scaled)[0]\n",
    "        pruned_channels[name] = keep_indices\n",
    "        \n",
    "        num_gamma[name] = len(gamma)\n",
    "        num_gamma_pruned[name] = len(keep_indices)\n",
    "        \n"
   ],
   "id": "dfc49c415657f3cd",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aa3ffe6d37c81db6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:38:46.010543Z",
     "start_time": "2024-12-27T23:38:46.002539Z"
    }
   },
   "cell_type": "code",
   "source": [
    " x = torch.randn(3).to(device=\"cuda\")\n",
    "x"
   ],
   "id": "dcafd66a2ecb758d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6179, -1.2895, -0.0481], device='cuda:0')"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 347
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:39:23.847862Z",
     "start_time": "2024-12-27T23:39:23.841717Z"
    }
   },
   "cell_type": "code",
   "source": "torch.where(x > -1)",
   "id": "9f5c91dd90d58b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2], device='cuda:0'),)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 350
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:25:04.412848Z",
     "start_time": "2024-12-27T23:25:04.361824Z"
    }
   },
   "cell_type": "code",
   "source": "gamma.argsort()",
   "id": "b43ae969514bab52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 33, 605, 862, 874, 878, 395,  95, 261,  70, 850, 125,  11, 730, 910,\n",
       "        325, 379, 471, 867,  69, 724, 679, 865,  21, 295, 113, 509,  32, 407,\n",
       "        404, 436, 313, 799, 489, 921, 663, 362, 219, 123,  56, 660, 369, 189,\n",
       "        691, 410, 378, 954, 637, 228, 882, 831,  66, 580, 280,  34, 417, 312,\n",
       "          0, 665, 197, 524, 263, 202, 796, 365, 492, 411, 773, 816, 186, 221,\n",
       "        277, 196, 815, 421, 842, 291, 833, 560, 735, 547, 134, 669, 614, 621,\n",
       "        888, 401, 783, 683, 616, 641, 324, 806, 578, 629, 301, 931, 334, 940,\n",
       "        685, 615, 155, 220, 484, 454, 533, 595, 264, 562, 430, 628, 824, 511,\n",
       "        104,  10, 287, 772, 686, 351, 439, 464, 895, 327, 883, 565, 106, 870,\n",
       "        306, 444, 754, 384, 363, 892, 881, 250, 915, 482, 793, 249, 266, 809,\n",
       "        357, 167, 262, 434, 415, 244, 341, 548, 703, 494, 788,  94, 420,  68,\n",
       "         50, 294, 323, 160, 105, 210, 690, 141, 610, 800,  28, 292, 936,  87,\n",
       "         38, 452, 391, 869, 938, 635, 222, 721,  91, 748, 449, 224, 175, 529,\n",
       "        550, 559, 518,  23,  57, 792, 348,   1, 624, 812, 139, 477, 381, 406,\n",
       "        612, 702, 530, 631, 606, 713, 289, 640, 192, 627, 937, 372, 367, 780,\n",
       "        260, 232, 920, 504,  77, 926, 957, 834, 808, 144, 233, 807,  44, 672,\n",
       "        535, 527, 558, 762, 944, 508, 896,  89, 688, 176, 592, 461, 319, 493,\n",
       "        496, 821, 657, 120,  92, 523, 886, 472, 501, 585, 785, 345, 296, 314,\n",
       "        171,  29, 109,  80,  47, 598, 321,  42, 769, 917, 752, 507,  24, 218,\n",
       "        577, 300, 146, 243, 932, 465,  39, 343, 539, 140, 542, 429, 165, 279,\n",
       "        455, 586, 446, 913, 339, 121, 333, 127, 122, 400, 272, 177,  79, 918,\n",
       "        352,  54, 738, 919, 789, 651, 303, 846, 335, 330, 930, 741,  62,  98,\n",
       "        712, 811, 911, 873, 148, 495, 652, 388, 425, 476, 158, 670, 226, 781,\n",
       "        354, 861, 552, 377, 952, 726, 416, 174, 431, 906, 462, 727, 107, 802,\n",
       "        945, 515, 583, 855, 418, 424, 173, 923, 304, 383, 227, 161, 145, 111,\n",
       "        708, 664, 298, 329, 498, 897, 214, 733, 216, 166, 553,  60,  45, 546,\n",
       "        440, 268,  14, 457, 397, 360, 941, 859,   7, 692, 731, 956, 677, 579,\n",
       "        170, 848,  99, 543, 545, 432,  58, 194, 520, 795, 458, 342, 373, 722,\n",
       "        776, 137, 904, 720, 234, 488, 891, 225, 856, 101, 191, 118, 666, 760,\n",
       "         90, 622, 880, 311, 179, 389, 441,  22, 593, 528, 617, 193,  73, 689,\n",
       "        634,   8, 747, 470, 537, 638, 797, 732,  81, 750, 419, 697, 182, 184,\n",
       "        819, 659, 709, 822, 745,  64, 645, 286, 860, 925,  31, 497, 608, 490,\n",
       "        803, 946, 242, 150, 916,  59, 947, 863, 208, 248, 180, 596,  61,  26,\n",
       "        254, 927, 168, 531,  88, 376, 849, 473, 231, 185, 654, 374, 681, 405,\n",
       "        288, 908, 667,  85, 433, 902,  52, 779, 290, 340,  84, 468, 447, 275,\n",
       "        253, 255, 190, 647, 102, 337, 774,  71, 284, 832, 818, 541,  13, 828,\n",
       "        950, 169, 928, 900,  51, 901, 451, 459, 749, 924, 469, 814,  82, 456,\n",
       "        475, 653, 810, 229, 387, 756, 347, 181, 353, 503, 710, 412, 912, 332,\n",
       "         46, 696,  76, 805, 328, 117, 386, 499, 778, 590, 768, 740, 838, 607,\n",
       "        698, 706, 435, 247, 823, 839, 801, 124, 611, 899, 566, 274, 147, 241,\n",
       "        217, 784, 206,  36, 536, 159, 103, 212, 251, 939, 639, 269, 852, 682,\n",
       "        213,  15, 442, 909, 597, 898, 512, 178,   2, 791, 195, 390, 130, 375,\n",
       "        204, 203, 678, 813, 655, 884,   4, 719, 599,   9, 209, 591, 450, 309,\n",
       "        110, 491, 633, 385, 658, 582, 845,  67, 308,  40, 317, 649, 889, 604,\n",
       "         97, 556, 129, 798, 517, 804, 563, 198, 575, 114, 513, 356, 555, 423,\n",
       "        136, 734, 717, 245, 782, 422,  55, 875, 935, 949,  30, 293, 584, 445,\n",
       "        761, 443, 959, 623, 953, 282, 299, 392,  96, 561,  43, 700, 428, 603,\n",
       "        636, 643, 711, 644,  48, 922, 933, 767,  25,  83, 907, 152, 656, 285,\n",
       "        854, 132, 116, 876, 871, 868, 394, 350, 905, 271, 236, 437, 951,  41,\n",
       "        794, 601, 402, 942, 393, 211,  72, 338, 857, 521, 746, 650,  75, 914,\n",
       "         63, 757, 302, 619, 567, 128,  12,   6, 142, 453, 359, 851, 866,  18,\n",
       "         37, 448, 500, 151, 398, 506, 705, 460, 581, 554,   3, 736, 486, 626,\n",
       "        817, 505, 723, 164, 751, 571, 153, 853, 890, 115, 786, 273, 840, 133,\n",
       "        674, 526, 480, 675, 310, 326, 787, 316, 602, 297,  93, 143,  86, 252,\n",
       "        766, 427, 525, 534, 864, 587, 188, 259, 826, 743, 758, 399, 830, 346,\n",
       "        576, 467, 646, 790, 112, 368, 522, 618, 382,  74, 739, 701, 715, 487,\n",
       "        414, 281, 620,  53, 119, 569, 684,  20, 516, 894, 336, 207, 409, 770,\n",
       "        836, 396, 318, 426, 438, 270,  27, 759, 943, 570, 551, 958, 707, 841,\n",
       "        755, 625, 844, 934, 877, 256, 187, 474, 349, 835, 157, 716, 126, 307,\n",
       "        278, 135, 613, 479, 331, 858, 695, 549, 240, 632, 885, 371, 370, 315,\n",
       "        872, 265, 574, 358, 572, 704, 903, 519, 829, 929, 163, 847, 320, 693,\n",
       "        485, 879, 538, 199, 765, 687, 955, 108,   5, 737, 820, 673, 825, 568,\n",
       "        589, 600, 662, 223, 843, 642, 588, 466,  49, 283, 172,  78, 183, 344,\n",
       "        131, 661, 594, 239, 162, 230, 573, 763, 540, 680, 728, 887, 413, 201,\n",
       "         19, 235, 729, 305, 564, 775, 364, 463, 366, 827, 557, 648, 725, 380,\n",
       "        502, 478, 205, 403, 258, 777, 764, 149, 246, 276, 138, 544, 215, 355,\n",
       "        510, 771, 532, 257,  17,  35, 753, 742, 609, 699, 630, 668, 718,  16,\n",
       "        238, 267, 837, 361, 156, 671, 676, 200, 714, 154, 481, 100, 483,  65,\n",
       "        893, 408, 322, 948, 237, 514, 694, 744], device='cuda:0')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:29:26.041687Z",
     "start_time": "2024-12-27T23:29:26.034278Z"
    }
   },
   "cell_type": "code",
   "source": "gamma[509]",
   "id": "72b2343af3062996",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1065, device='cuda:0')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "0.10553279519081116\n",
    "0.10554493218660355\n",
    "0.10554537177085876"
   ],
   "id": "8c0a0339a9986f16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:40:15.072639Z",
     "start_time": "2024-12-27T23:40:15.066187Z"
    }
   },
   "cell_type": "code",
   "source": "threshold_.item()",
   "id": "9385c256caf225e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10554537177085876"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 351
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:40:21.500421Z",
     "start_time": "2024-12-27T23:40:21.493423Z"
    }
   },
   "cell_type": "code",
   "source": "test = torch.tensor([0.10554493218660355], device=\"cuda:0\")\n",
   "id": "defb134652a12c19",
   "outputs": [],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:40:26.055204Z",
     "start_time": "2024-12-27T23:40:26.047505Z"
    }
   },
   "cell_type": "code",
   "source": "test > threshold_",
   "id": "30af1aab3ca2a310",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False], device='cuda:0')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 354
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T23:24:10.009995Z",
     "start_time": "2024-12-27T23:24:10.005940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(gamma))\n",
    "print(len(keep_indices))"
   ],
   "id": "f4fba2750a8181c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n",
      "936\n"
     ]
    }
   ],
   "execution_count": 298
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T20:04:30.277494Z",
     "start_time": "2024-12-28T20:04:30.270069Z"
    }
   },
   "cell_type": "code",
   "source": "scales.values()",
   "id": "edf7484099b6d383",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.290224070250392, 0.10710066755533218, 0.05456456013649702, 0.0708910028219223, 0.0320092864985764, 0.020639183368161326, 0.037714954476952556, 0.03280501537919045, 0.0028062857322394873, 0.005678077998608354, 0.04911940982975065, 0.01889392982363701, 0.03535361602328718, 0.047462662765383724, 0.027049223961234093, 0.02664218142554164, 0.0418163540995121])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T20:06:04.465294Z",
     "start_time": "2024-12-28T20:06:04.458133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(sum(list(num_gamma.values())))\n",
    "print(sum(list(num_gamma_pruned.values())))\n"
   ],
   "id": "27061242465bf620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7136\n",
      "6423\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T20:05:44.726256Z",
     "start_time": "2024-12-28T20:05:44.718689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(list(num_gamma.values()))\n",
    "print(list(num_gamma_pruned.values()))\n"
   ],
   "id": "4983baae7424c7e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 96, 144, 144, 192, 192, 192, 384, 384, 384, 384, 576, 576, 576, 960, 960, 960]\n",
      "[3, 27, 124, 104, 187, 191, 191, 368, 384, 383, 314, 569, 530, 385, 960, 956, 747]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7136\n",
      "6423\n"
     ]
    }
   ],
   "execution_count": 364,
   "source": [
    "print(sum(list(num_gamma.values())))\n",
    "print(sum(list(num_gamma_pruned.values())))\n"
   ],
   "id": "43f76541c8cd6cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:19:39.079974Z",
     "start_time": "2024-12-28T01:19:39.068919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(list(num_gamma.values()))\n",
    "print(list(num_gamma_pruned.values()))\n"
   ],
   "id": "8f161d05d6147d14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 96, 144, 144, 192, 192, 192, 384, 384, 384, 384, 576, 576, 576, 960, 960, 960]\n",
      "[23, 94, 137, 144, 181, 178, 191, 344, 317, 313, 378, 514, 501, 519, 896, 875, 818]\n"
     ]
    }
   ],
   "execution_count": 365
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:18:57.634152Z",
     "start_time": "2024-12-28T01:18:57.627766Z"
    }
   },
   "cell_type": "code",
   "source": "100 - (6423/7136 * 100 )",
   "id": "ef19a54ba5465460",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.991591928251125"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 363
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:31:53.223439Z",
     "start_time": "2024-12-28T01:31:53.219194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruned = []\n",
    "for i in range(len(list(num_gamma.values()))):\n",
    "    gamma_pruned = list(num_gamma_pruned.values())[i]\n",
    "    gamma = list(num_gamma.values())[i]\n",
    "    perc = 100 - (gamma_pruned/ gamma * 100)\n",
    "    pruned.append(f\"{perc:.2f}\")\n"
   ],
   "id": "8ab894cbebb787d0",
   "outputs": [],
   "execution_count": 394
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T01:31:53.644831Z",
     "start_time": "2024-12-28T01:31:53.640386Z"
    }
   },
   "cell_type": "code",
   "source": "print(pruned)",
   "id": "baa5c109a0ed963b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28.12', '2.08', '4.86', '0.00', '5.73', '7.29', '0.52', '10.42', '17.45', '18.49', '1.56', '10.76', '13.02', '9.90', '6.67', '8.85', '14.79']\n"
     ]
    }
   ],
   "execution_count": 395
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T22:31:09.951999Z",
     "start_time": "2024-12-27T22:31:09.946978Z"
    }
   },
   "cell_type": "code",
   "source": "100 - (950/960 * 100 )",
   "id": "8672a34304f64a4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0416666666666572"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0381208 , 0.18724878, 0.19752091, 0.24511185, 0.13127756,\n",
       "        0.15904109, 0.08813592, 0.25522655, 0.08698447, 0.01191824,\n",
       "        0.4129336 , 0.11366259, 0.22449361, 0.30144963, 0.01136814,\n",
       "        0.01036511, 0.01283585, 0.00430115, 0.06677727, 0.41075766,\n",
       "        0.3577698 , 0.12777077, 0.52496415, 0.00393305, 0.44444874,\n",
       "        0.12979506, 0.32841888, 0.24531788, 0.35647354, 0.30660468,\n",
       "        0.4146144 , 0.14192693], dtype=float32),\n",
       " array([0.02575562, 0.5501319 , 0.71535563, 0.4909047 , 1.1533738 ,\n",
       "        0.49047863, 0.5998267 , 0.8479905 , 0.5139333 , 0.00609509,\n",
       "        0.210984  , 1.0502005 , 0.6857423 , 0.2375618 , 0.00594302,\n",
       "        0.00494451, 0.00846775, 0.00503939, 0.573383  , 0.19520941,\n",
       "        0.2612262 , 0.28118408, 0.2269232 , 0.0084268 , 0.08874259,\n",
       "        1.0205832 , 0.27261943, 0.26077464, 0.12849563, 0.17686303,\n",
       "        0.20431545, 0.40291643], dtype=float32),\n",
       " array([0.5125795 , 0.5345392 , 0.5720613 , 0.4572209 , 0.56945914,\n",
       "        0.61156434, 0.6057042 , 0.5404174 , 0.4620264 , 0.55111665,\n",
       "        0.41317612, 0.55571616, 0.69118804, 0.5953061 , 0.6496445 ,\n",
       "        0.5285069 ], dtype=float32),\n",
       " array([0.47806546, 0.35554197, 0.11790199, 0.20202969, 0.15454961,\n",
       "        0.19316895, 0.28500208, 0.11567583, 0.4599692 , 0.13875006,\n",
       "        0.21005805, 0.3856824 , 0.07402164, 0.09392407, 0.14349739,\n",
       "        0.45532632, 0.17624663, 0.12913375, 0.33024418, 0.22007127,\n",
       "        0.14882447, 0.13292126, 0.10360865, 0.09650918, 0.21415105,\n",
       "        0.46735945, 0.17099638, 0.27546513, 0.11636735, 0.08796483,\n",
       "        0.503173  , 0.08619752, 0.15303183, 0.31875518, 0.44203624,\n",
       "        0.20300038, 0.26941615, 0.34267357, 0.09591336, 0.3548629 ,\n",
       "        0.4764003 , 0.11921743, 0.14458834, 0.09824199, 0.15675187,\n",
       "        0.46869868, 0.3016701 , 0.15122645, 0.10947491, 0.07906949,\n",
       "        0.26885802, 0.41435364, 0.486736  , 0.25031677, 0.21359149,\n",
       "        0.11611281, 0.43637985, 0.16405   , 0.15492035, 0.32092825,\n",
       "        0.41289547, 0.43005484, 0.08108446, 0.13979195, 0.5202675 ,\n",
       "        0.12059452, 0.1202437 , 0.3220923 , 0.22561489, 0.17523277,\n",
       "        0.14608996, 0.5355778 , 0.02915841, 0.23629294, 0.1279813 ,\n",
       "        0.2229703 , 0.2934897 , 0.2699582 , 0.12226192, 0.25620696,\n",
       "        0.53627056, 0.26019663, 0.25236064, 0.41255227, 0.5833944 ,\n",
       "        0.5720149 , 0.09828477, 0.4991947 , 0.27525145, 0.1620689 ,\n",
       "        0.09129337, 0.5380496 , 0.43043947, 0.3548332 , 0.13901497,\n",
       "        0.14225598], dtype=float32),\n",
       " array([0.53909504, 0.3176859 , 0.36038023, 0.25819603, 0.33488208,\n",
       "        0.32658252, 0.17948331, 0.33569762, 0.2887013 , 0.2787054 ,\n",
       "        0.15193537, 0.454172  , 0.3938531 , 0.27208543, 0.24517272,\n",
       "        0.44224492, 0.2300349 , 0.21397185, 0.49194697, 0.4352135 ,\n",
       "        0.2534908 , 0.26546806, 0.41160288, 0.45237556, 0.31366473,\n",
       "        0.34859464, 0.32229406, 0.15291664, 0.21584636, 0.35832852,\n",
       "        0.5198926 , 0.2295277 , 0.33451447, 0.21131653, 0.21467493,\n",
       "        0.36296573, 0.1832858 , 0.32333937, 0.40996084, 0.29484284,\n",
       "        0.43226966, 0.23171175, 0.33874097, 0.35940167, 0.3377734 ,\n",
       "        0.29295325, 0.35113874, 0.25728396, 0.2543716 , 0.38180387,\n",
       "        0.12229731, 0.21797307, 0.4859046 , 0.21448056, 0.10483696,\n",
       "        0.47493687, 0.31598744, 0.23237544, 0.38408256, 0.37853402,\n",
       "        0.30223396, 0.280565  , 0.3981725 , 0.23976023, 0.5744788 ,\n",
       "        0.4190906 , 0.20173042, 0.19832551, 0.27566823, 0.32919034,\n",
       "        0.20233643, 0.841576  , 0.43684894, 0.1372091 , 0.3202483 ,\n",
       "        0.2253561 , 0.33416063, 0.13850036, 0.45644808, 0.33750302,\n",
       "        0.49808928, 0.15331542, 0.29676196, 0.32076982, 0.66019994,\n",
       "        0.3149142 , 0.2398505 , 0.34611553, 0.18767679, 0.35530877,\n",
       "        0.40894127, 0.42597947, 0.22031885, 0.17539074, 0.21497095,\n",
       "        0.36984822], dtype=float32),\n",
       " array([0.5480837 , 0.43338034, 0.46323302, 0.40370658, 0.36849758,\n",
       "        0.5151954 , 0.32145655, 0.63366914, 0.47736385, 0.6231848 ,\n",
       "        0.4404118 , 0.6813824 , 0.4741938 , 0.48327094, 0.59684604,\n",
       "        0.43556914, 0.4825529 , 0.5388483 , 0.6387112 , 0.48177752,\n",
       "        0.4304399 , 0.4283724 , 0.54632974, 0.40256754], dtype=float32),\n",
       " array([0.610566  , 0.10015336, 0.19464998, 0.19919147, 0.14799304,\n",
       "        0.15772295, 0.21511357, 0.1897789 , 0.13449873, 0.18210249,\n",
       "        0.21882205, 0.19170702, 0.13108164, 0.2484858 , 0.20213687,\n",
       "        0.24346589, 0.15515874, 0.12156956, 0.15757282, 0.17683797,\n",
       "        0.19297202, 0.15061028, 0.1673519 , 0.14969753, 0.16735174,\n",
       "        0.16200499, 0.22503915, 0.19634826, 0.18580337, 0.10950074,\n",
       "        0.1104275 , 0.19350728, 0.25410295, 0.20994063, 0.22409473,\n",
       "        0.13366568, 0.2133244 , 0.20580034, 0.2736065 , 0.27143198,\n",
       "        0.1672861 , 0.13540123, 0.13004611, 0.14374067, 0.04870837,\n",
       "        0.1300229 , 0.10404335, 0.14963661, 0.1988755 , 0.17584847,\n",
       "        0.24358015, 0.15815447, 0.17653315, 0.18777442, 0.13268693,\n",
       "        0.26891002, 0.21129945, 0.19699055, 0.18256032, 0.20301309,\n",
       "        0.19459194, 0.22327983, 0.2104173 , 0.15942332, 0.1026969 ,\n",
       "        0.24025342, 0.26720893, 0.1459821 , 0.16718926, 0.15939419,\n",
       "        0.1228634 , 0.2914677 , 0.163004  , 0.11423575, 0.17687307,\n",
       "        0.21159332, 0.1481206 , 0.20350164, 0.16538545, 0.08784536,\n",
       "        0.15416268, 0.13486701, 0.2368979 , 0.12135074, 0.16723055,\n",
       "        0.1841439 , 0.26727676, 0.05770916, 0.11858483, 0.16595979,\n",
       "        0.08477637, 0.1455803 , 0.23899567, 0.19785649, 0.23469622,\n",
       "        0.12257793, 0.17843975, 0.09335432, 0.24751392, 0.19508938,\n",
       "        0.19507796, 0.26453656, 0.20305708, 0.16951145, 0.25079247,\n",
       "        0.15186636, 0.12606771, 0.14380924, 0.20339876, 0.2713913 ,\n",
       "        0.04286508, 0.18537922, 0.16185053, 0.14988945, 0.09754318,\n",
       "        0.05753909, 0.14474432, 0.15678093, 0.12594445, 0.21362849,\n",
       "        0.08477815, 0.14662558, 0.1517736 , 0.16843225, 0.18718776,\n",
       "        0.13006563, 0.09637706, 0.16306263, 0.11458692, 0.4170834 ,\n",
       "        0.127497  , 0.15213989, 0.28885597, 0.15740976, 0.17446718,\n",
       "        0.14070359, 0.1759127 , 0.13686706, 0.10817872, 0.11380272,\n",
       "        0.11765443, 0.20385873, 0.10267424, 0.15265201], dtype=float32),\n",
       " array([0.1535854 , 0.22371528, 0.18072958, 0.26693854, 0.18023524,\n",
       "        0.27640027, 0.19329536, 0.18594727, 0.26855716, 0.31242093,\n",
       "        0.24031973, 0.26309195, 0.1839923 , 0.11202898, 0.15809022,\n",
       "        0.12211248, 0.17641854, 0.27011767, 0.30233306, 0.26796365,\n",
       "        0.3579105 , 0.27910307, 0.18894257, 0.29206884, 0.11041985,\n",
       "        0.23631728, 0.21938261, 0.28529876, 0.20798208, 0.34435758,\n",
       "        0.30109242, 0.2295648 , 0.20889968, 0.19595025, 0.20946915,\n",
       "        0.25379947, 0.27514318, 0.32158765, 0.1991075 , 0.24973683,\n",
       "        0.19842228, 0.28208587, 0.3262521 , 0.2827888 , 0.32555932,\n",
       "        0.25233525, 0.3277451 , 0.32447293, 0.19953045, 0.32733962,\n",
       "        0.17891133, 0.42693716, 0.26491106, 0.2742091 , 0.22631766,\n",
       "        0.16498604, 0.22130041, 0.20347698, 0.1889825 , 0.25689295,\n",
       "        0.2708199 , 0.19001292, 0.18215309, 0.2668933 , 0.2937637 ,\n",
       "        0.22589837, 0.2155579 , 0.3783517 , 0.22029388, 0.2816944 ,\n",
       "        0.25948653, 0.25388518, 0.29735383, 0.22004984, 0.26143062,\n",
       "        0.10624263, 0.2501972 , 0.1853745 , 0.22066572, 0.24192108,\n",
       "        0.2576019 , 0.29100537, 0.11619756, 0.19215228, 0.29058552,\n",
       "        0.15985198, 0.2177024 , 0.3722601 , 0.2970642 , 0.20747091,\n",
       "        0.30449682, 0.20248078, 0.22080892, 0.31065965, 0.19609825,\n",
       "        0.26974463, 0.28577587, 0.37030783, 0.26334378, 0.11724318,\n",
       "        0.24439809, 0.1893415 , 0.25489944, 0.2626827 , 0.17330559,\n",
       "        0.2558135 , 0.29763842, 0.24225356, 0.20549536, 0.26696596,\n",
       "        0.3903249 , 0.11095466, 0.24567792, 0.2456367 , 0.31199825,\n",
       "        0.2726124 , 0.14919609, 0.2490603 , 0.24996814, 0.263741  ,\n",
       "        0.31730393, 0.31559336, 0.25232774, 0.22194478, 0.19945757,\n",
       "        0.29373652, 0.30030882, 0.21900663, 0.27004832, 0.18937235,\n",
       "        0.21588059, 0.16790177, 0.24193935, 0.28510115, 0.3043472 ,\n",
       "        0.24140531, 0.25903565, 0.21950649, 0.3480818 , 0.24921949,\n",
       "        0.37567002, 0.22235234, 0.3160966 , 0.2920132 ], dtype=float32),\n",
       " array([0.67389107, 0.60958284, 0.5234447 , 0.56477803, 0.6697316 ,\n",
       "        0.6304003 , 0.6747297 , 0.42775   , 0.47067782, 0.44956616,\n",
       "        0.6937156 , 0.48063806, 0.5078851 , 0.5957885 , 0.5188862 ,\n",
       "        0.464835  , 0.4346866 , 0.5245216 , 0.38880593, 0.4779173 ,\n",
       "        0.49211177, 0.7221989 , 0.6039559 , 0.6276221 ], dtype=float32),\n",
       " array([0.38459712, 0.09865437, 0.25562027, 0.09567731, 0.24554935,\n",
       "        0.15456626, 0.27100483, 0.29518878, 0.20917407, 0.22549193,\n",
       "        0.2708061 , 0.2809168 , 0.20344213, 0.24082004, 0.30334175,\n",
       "        0.2718154 , 0.24874027, 0.30815896, 0.28044316, 0.27433005,\n",
       "        0.2966957 , 0.22240736, 0.2599401 , 0.2654996 , 0.28712606,\n",
       "        0.16993329, 0.09707087, 0.21264687, 0.29445466, 0.31676203,\n",
       "        0.22374868, 0.30300346, 0.11739983, 0.23236288, 0.23158793,\n",
       "        0.33619887, 0.21071135, 0.223793  , 0.24536729, 0.27130365,\n",
       "        0.30867746, 0.31857398, 0.39455009, 0.21517128, 0.28302246,\n",
       "        0.22126812, 0.12664917, 0.23465613, 0.12830226, 0.21937598,\n",
       "        0.24867138, 0.261891  , 0.29809245, 0.51602453, 0.26119548,\n",
       "        0.16744922, 0.283556  , 0.27958873, 0.2334313 , 0.0957511 ,\n",
       "        0.2421308 , 0.31324503, 0.04057973, 0.06132628, 0.24338518,\n",
       "        0.24949434, 0.2863833 , 0.23720363, 0.28112063, 0.30064812,\n",
       "        0.07185797, 0.27320656, 0.29859757, 0.28608513, 0.24726339,\n",
       "        0.30686954, 0.2280442 , 0.27191293, 0.15996633, 0.12387583,\n",
       "        0.3011378 , 0.09844571, 0.07172609, 0.3008523 , 0.29881066,\n",
       "        0.1884722 , 0.14691983, 0.30063555, 0.22592147, 0.43845862,\n",
       "        0.27255228, 0.24020824, 0.14858565, 0.29050457, 0.41101256,\n",
       "        0.07767451, 0.23854768, 0.14261985, 0.13432762, 0.2566731 ,\n",
       "        0.15070006, 0.09053548, 0.27982163, 0.27300042, 0.2678962 ,\n",
       "        0.25102514, 0.12878805, 0.05973645, 0.14002587, 0.22777346,\n",
       "        0.06380368, 0.29058498, 0.25977367, 0.3368983 , 0.16559334,\n",
       "        0.05486365, 0.12812482, 0.26750416, 0.1800255 , 0.1910023 ,\n",
       "        0.11191487, 0.37246865, 0.11426869, 0.23325895, 0.1703962 ,\n",
       "        0.29151407, 0.26370022, 0.18671988, 0.2155901 , 0.2918732 ,\n",
       "        0.30783677, 0.11959951, 0.22769047, 0.15845141, 0.1325012 ,\n",
       "        0.07740445, 0.2259159 , 0.28492957, 0.3039694 , 0.22945546,\n",
       "        0.14241686, 0.30708382, 0.24834386, 0.32517242], dtype=float32),\n",
       " array([0.3942743 , 0.33851907, 0.49235183, 0.3397433 , 0.28761885,\n",
       "        0.24328373, 0.2310129 , 0.23462635, 0.39924976, 0.43849638,\n",
       "        0.20045775, 0.27818486, 0.26403546, 0.22169569, 0.46125022,\n",
       "        0.27488813, 0.24346773, 0.26325384, 0.3554293 , 0.2401896 ,\n",
       "        0.3605004 , 0.25946128, 0.19305213, 0.26559946, 0.22980715,\n",
       "        0.31238195, 0.35808384, 0.18868867, 0.2457919 , 0.22282808,\n",
       "        0.22050719, 0.47966218, 0.2898873 , 0.15017597, 0.29839444,\n",
       "        0.21508402, 0.25445116, 0.25862616, 0.25023034, 0.25940624,\n",
       "        0.5202216 , 0.55517185, 0.22197153, 0.18636927, 0.23128778,\n",
       "        0.16903819, 0.29117578, 0.25934866, 0.20838265, 0.24178165,\n",
       "        0.20210192, 0.4894723 , 0.5233004 , 0.24722989, 0.26539844,\n",
       "        0.22576459, 0.27648258, 0.27743348, 0.25548214, 0.24563618,\n",
       "        0.3239255 , 0.29813734, 0.35767314, 0.413961  , 0.22006485,\n",
       "        0.19985987, 0.23626003, 0.26109138, 0.21564639, 0.26457444,\n",
       "        0.26553383, 0.30057853, 0.26568463, 0.24506697, 0.17491588,\n",
       "        0.2458793 , 0.45925906, 0.44064122, 0.24063155, 0.33319563,\n",
       "        0.32280684, 0.37458086, 0.35175887, 0.21017447, 0.30077064,\n",
       "        0.20984332, 0.2597905 , 0.20683217, 0.24222198, 0.22834383,\n",
       "        0.18392004, 0.24224345, 0.46944255, 0.30078518, 0.2409078 ,\n",
       "        0.365523  , 0.24857637, 0.16098547, 0.2979091 , 0.24539699,\n",
       "        0.23224238, 0.38744685, 0.25557527, 0.22222301, 0.23436457,\n",
       "        0.19962236, 0.32825765, 0.34302333, 0.26618037, 0.25847414,\n",
       "        0.32631943, 0.3195191 , 0.18517189, 0.36540437, 0.32745683,\n",
       "        0.3629909 , 0.271978  , 0.24513714, 0.32302397, 0.26421762,\n",
       "        0.32140422, 0.39765966, 0.26805454, 0.26068732, 0.27341655,\n",
       "        0.29868582, 0.28211373, 0.20796804, 0.28391606, 0.4555378 ,\n",
       "        0.2785056 , 0.28783104, 0.2240106 , 0.26094392, 0.30435848,\n",
       "        0.24775754, 0.2642094 , 0.25206614, 0.24414434, 0.24035332,\n",
       "        0.22756523, 0.25136086, 0.28636616, 0.26654992], dtype=float32),\n",
       " array([0.598205  , 0.62288743, 0.6554644 , 0.5900726 , 0.46507093,\n",
       "        0.5853899 , 0.7612752 , 0.59456205, 0.5167398 , 0.5135482 ,\n",
       "        0.4894606 , 0.7101929 , 0.592549  , 0.58101875, 0.7688883 ,\n",
       "        0.51436865, 0.5231503 , 0.5214044 , 0.5773973 , 0.47153923,\n",
       "        0.54721284, 0.6248191 , 0.55249035, 0.48271415, 0.6471303 ,\n",
       "        0.5818757 , 0.50575525, 0.5848743 , 0.6630927 , 0.59351283,\n",
       "        0.5255494 , 0.47126868], dtype=float32),\n",
       " array([0.14164503, 0.15000425, 0.07520358, 0.11896706, 0.17817102,\n",
       "        0.07934863, 0.10472434, 0.10907141, 0.1624984 , 0.20345306,\n",
       "        0.07229795, 0.16930638, 0.09562958, 0.12546673, 0.20697586,\n",
       "        0.18135652, 0.10805933, 0.12624204, 0.10711879, 0.11510143,\n",
       "        0.09242123, 0.18888266, 0.12065902, 0.13442063, 0.09837369,\n",
       "        0.07166435, 0.11713508, 0.13697322, 0.12043026, 0.18225427,\n",
       "        0.15013885, 0.11782613, 0.18246141, 0.19664575, 0.14093232,\n",
       "        0.07968035, 0.09122897, 0.11988362, 0.07450709, 0.13349588,\n",
       "        0.19079953, 0.16318183, 0.08094218, 0.10001537, 0.1309419 ,\n",
       "        0.12611905, 0.08266495, 0.10467914, 0.07329888, 0.15949191,\n",
       "        0.10607497, 0.13220544, 0.13523866, 0.16246417, 0.05527675,\n",
       "        0.08686384, 0.09338152, 0.17469585, 0.08124062, 0.13850072,\n",
       "        0.07362124, 0.20697229, 0.13536559, 0.08318748, 0.18624683,\n",
       "        0.0594392 , 0.1097542 , 0.06140812, 0.17288941, 0.16284962,\n",
       "        0.0823822 , 0.17650042, 0.12573078, 0.15517794, 0.1620639 ,\n",
       "        0.08821058, 0.06552179, 0.11366078, 0.13536824, 0.12022871,\n",
       "        0.15012449, 0.16963133, 0.12373275, 0.17023173, 0.18637359,\n",
       "        0.12281244, 0.09192487, 0.14403972, 0.16014822, 0.1205816 ,\n",
       "        0.14027266, 0.10379206, 0.14315285, 0.13607107, 0.23736235,\n",
       "        0.12972592, 0.15652867, 0.07448258, 0.1359064 , 0.10959907,\n",
       "        0.11631762, 0.06734694, 0.12042095, 0.13280879, 0.11230699,\n",
       "        0.12069979, 0.15441813, 0.13451426, 0.15379801, 0.07843327,\n",
       "        0.12342864, 0.10698202, 0.1634817 , 0.06583897, 0.11037346,\n",
       "        0.11774673, 0.15248181, 0.09483449, 0.0883844 , 0.12433883,\n",
       "        0.10343808, 0.10170539, 0.14933544, 0.1342965 , 0.17850327,\n",
       "        0.10271559, 0.12229659, 0.09772284, 0.1088936 , 0.1367436 ,\n",
       "        0.14145596, 0.12870787, 0.11698098, 0.06279782, 0.06307322,\n",
       "        0.16889735, 0.04217331, 0.08747068, 0.14917767, 0.15315355,\n",
       "        0.14089443, 0.13133119, 0.06705962, 0.18732873, 0.13126986,\n",
       "        0.22209096, 0.12519589, 0.07710005, 0.1046775 , 0.09771954,\n",
       "        0.09545228, 0.11704528, 0.1190458 , 0.12942153, 0.10365613,\n",
       "        0.3154237 , 0.12771165, 0.13645066, 0.14121707, 0.12379067,\n",
       "        0.13497691, 0.0641512 , 0.14569977, 0.2157731 , 0.10580157,\n",
       "        0.1082001 , 0.1365748 , 0.14745806, 0.12528431, 0.1511773 ,\n",
       "        0.16806498, 0.11619693, 0.11398894, 0.104812  , 0.14338107,\n",
       "        0.1170565 , 0.08995512, 0.1235648 , 0.14753184, 0.13545452,\n",
       "        0.13958474, 0.1085649 , 0.09590398, 0.10724063, 0.10267116,\n",
       "        0.12567109, 0.03967163, 0.0640014 , 0.07294347, 0.20673537,\n",
       "        0.11307012, 0.14937429], dtype=float32),\n",
       " array([0.22222443, 0.17169677, 0.24473162, 0.19514239, 0.11107253,\n",
       "        0.2827237 , 0.20065257, 0.2249558 , 0.10450371, 0.15356633,\n",
       "        0.30198324, 0.32891905, 0.2302035 , 0.2058503 , 0.1677651 ,\n",
       "        0.13422292, 0.26593912, 0.23764145, 0.16511114, 0.23957682,\n",
       "        0.314938  , 0.38644117, 0.17298979, 0.2523689 , 0.23269345,\n",
       "        0.2974248 , 0.15287328, 0.18009765, 0.22782278, 0.11472549,\n",
       "        0.15759842, 0.18997552, 0.10535266, 0.16222157, 0.15276606,\n",
       "        0.20942955, 0.260609  , 0.27873352, 0.2604601 , 0.21096805,\n",
       "        0.25048912, 0.20977643, 0.32409123, 0.19759555, 0.20984328,\n",
       "        0.22561765, 0.19783553, 0.2167362 , 0.3005377 , 0.21943559,\n",
       "        0.21416931, 0.21258679, 0.24518345, 0.15319091, 0.28307876,\n",
       "        0.2736708 , 0.16225442, 0.2953656 , 0.15092884, 0.13918073,\n",
       "        0.32069436, 0.20550491, 0.21446286, 0.24803418, 0.09649973,\n",
       "        0.2307708 , 0.20231201, 0.19752575, 0.1242693 , 0.162484  ,\n",
       "        0.26619136, 0.16552721, 0.13611332, 0.13443324, 0.18833022,\n",
       "        0.2563594 , 0.25278622, 0.23561591, 0.16138677, 0.15330288,\n",
       "        0.17823213, 0.20514819, 0.1769196 , 0.4053322 , 0.21721476,\n",
       "        0.20313917, 0.27454802, 0.16672003, 0.15946993, 0.25402856,\n",
       "        0.17662928, 0.20225179, 0.1804041 , 0.2846118 , 0.18501487,\n",
       "        0.13402064, 0.24240147, 0.27972046, 0.23258802, 0.26346186,\n",
       "        0.20789857, 0.20192285, 0.15263078, 0.23312008, 0.172219  ,\n",
       "        0.22212754, 0.21204798, 0.21019828, 0.24909016, 0.19403684,\n",
       "        0.20663494, 0.24101545, 0.19691086, 0.21192722, 0.1638758 ,\n",
       "        0.14907645, 0.11807378, 0.20958725, 0.24517967, 0.24085496,\n",
       "        0.17434631, 0.19221893, 0.177245  , 0.19224404, 0.1510375 ,\n",
       "        0.2306756 , 0.12634893, 0.18817101, 0.1927732 , 0.2467365 ,\n",
       "        0.24641186, 0.20575573, 0.22227228, 0.25489423, 0.20055872,\n",
       "        0.1605167 , 0.25049853, 0.20947704, 0.21262771, 0.21295312,\n",
       "        0.10866237, 0.16697383, 0.17811947, 0.30898076, 0.20064403,\n",
       "        0.23669931, 0.2002238 , 0.2514038 , 0.22859886, 0.21430542,\n",
       "        0.23324698, 0.19401057, 0.17852396, 0.20613252, 0.309014  ,\n",
       "        0.5739809 , 0.22866285, 0.3952458 , 0.27211973, 0.22912885,\n",
       "        0.2630208 , 0.28116065, 0.3246512 , 0.22235434, 0.23370355,\n",
       "        0.18820056, 0.1638352 , 0.28849736, 0.18132515, 0.23858717,\n",
       "        0.09818625, 0.23110287, 0.25592107, 0.24514441, 0.1196472 ,\n",
       "        0.21110319, 0.21041614, 0.16428217, 0.1698871 , 0.1554922 ,\n",
       "        0.1458631 , 0.19938694, 0.18763344, 0.22291575, 0.1895408 ,\n",
       "        0.3455301 , 0.29337934, 0.28356385, 0.27703315, 0.23031338,\n",
       "        0.19981854, 0.17063905], dtype=float32),\n",
       " array([0.36390135, 0.26018715, 0.33873555, 0.2875628 , 0.41174135,\n",
       "        0.20918712, 0.3593441 , 0.3649469 , 0.29268378, 0.35931206,\n",
       "        0.56390214, 0.2651292 , 0.23984641, 0.26467454, 0.23198658,\n",
       "        0.35874858, 0.51807237, 0.3084066 , 0.24671803, 0.50586575,\n",
       "        0.40095878, 0.22252016, 0.27319175, 0.5734526 , 0.3421371 ,\n",
       "        0.27840552, 0.28147647, 0.43623126, 0.2981037 , 0.42408773,\n",
       "        0.3185594 , 0.4119252 ], dtype=float32),\n",
       " array([0.10745117, 0.06059764, 0.0615646 , 0.05111494, 0.07425925,\n",
       "        0.13552903, 0.00590503, 0.13467573, 0.13578579, 0.10922324,\n",
       "        0.13425368, 0.15882443, 0.11831813, 0.10228994, 0.12075698,\n",
       "        0.12268416, 0.08115099, 0.10453897, 0.155996  , 0.16032217,\n",
       "        0.11018343, 0.15440258, 0.16275904, 0.08960909, 0.13128006,\n",
       "        0.15025793, 0.18011668, 0.06393497, 0.07490927, 0.11520676,\n",
       "        0.20662038, 0.07539508, 0.10964791, 0.13819942, 0.15386128,\n",
       "        0.12820014, 0.15261227, 0.13529451, 0.14832215, 0.12016596,\n",
       "        0.14953488, 0.12635903, 0.09542635, 0.08774186, 0.14356725,\n",
       "        0.17162465, 0.09077532, 0.17296734, 0.1276151 , 0.14794923,\n",
       "        0.14747451, 0.11439458, 0.20302168, 0.13271736, 0.10373827,\n",
       "        0.14893329, 0.06351307, 0.08315318, 0.15714198, 0.1337218 ,\n",
       "        0.08071418, 0.05883865, 0.03512296, 0.14549486, 0.21362565,\n",
       "        0.12151054, 0.11347448, 0.14101216, 0.15017502, 0.12518743,\n",
       "        0.16597916, 0.09632373, 0.13242769, 0.18601474, 0.14877248,\n",
       "        0.07425599, 0.05665629, 0.1368144 , 0.11671169, 0.12197407,\n",
       "        0.09434916, 0.13574779, 0.10840773, 0.15018626, 0.11892286,\n",
       "        0.15707268, 0.1402657 , 0.08827372, 0.12481524, 0.11388102,\n",
       "        0.13952939, 0.08734534, 0.14874008, 0.11814748, 0.14713468,\n",
       "        0.13583997, 0.16729335, 0.10677405, 0.11955231, 0.08581083,\n",
       "        0.13223976, 0.09194187, 0.08004711, 0.14677167, 0.09150742,\n",
       "        0.10516978, 0.15350442, 0.17095046, 0.09093869, 0.11445665,\n",
       "        0.16902633, 0.13422671, 0.14170939, 0.12555751, 0.15398091,\n",
       "        0.0835346 , 0.09469124, 0.06202538, 0.11599467, 0.14115489,\n",
       "        0.08754795, 0.15064606, 0.16976051, 0.08892981, 0.130622  ,\n",
       "        0.1259785 , 0.13741182, 0.15327385, 0.12424584, 0.09355541,\n",
       "        0.15140586, 0.0773997 , 0.1328884 , 0.11982995, 0.11703441,\n",
       "        0.08758335, 0.12316602, 0.12165678, 0.09081339, 0.17213373,\n",
       "        0.10670985, 0.1364498 , 0.1371788 , 0.15704638, 0.15278736,\n",
       "        0.10198459, 0.15210842, 0.16314606, 0.09152097, 0.1240706 ,\n",
       "        0.12458256, 0.14214374, 0.16016068, 0.12359111, 0.15448014,\n",
       "        0.18313888, 0.17485967, 0.11567347, 0.0860138 , 0.08285142,\n",
       "        0.12072255, 0.14225413, 0.12550138, 0.11929601, 0.15398577,\n",
       "        0.12426917, 0.09524628, 0.14455941, 0.11639356, 0.12239131,\n",
       "        0.11444153, 0.0908622 , 0.16876398, 0.16976635, 0.13871108,\n",
       "        0.13143234, 0.13092023, 0.16805235, 0.07072878, 0.1634654 ,\n",
       "        0.06788315, 0.19592693, 0.10848556, 0.12904912, 0.17545967,\n",
       "        0.11866561, 0.13224238, 0.11212353, 0.1309843 , 0.1428063 ,\n",
       "        0.120114  , 0.13381967], dtype=float32),\n",
       " array([0.18945387, 0.23289135, 0.26921126, 0.29424143, 0.20012829,\n",
       "        0.19269645, 0.01071881, 0.14551859, 0.20527361, 0.21980129,\n",
       "        0.11561044, 0.15229878, 0.13401549, 0.21883251, 0.19397551,\n",
       "        0.21962973, 0.24802849, 0.2773814 , 0.22655535, 0.15046768,\n",
       "        0.23328751, 0.2522337 , 0.20095798, 0.22598214, 0.25299415,\n",
       "        0.2524524 , 0.24779777, 0.28855556, 0.21125342, 0.24606793,\n",
       "        0.39556772, 0.24272093, 0.19762045, 0.11804207, 0.2230517 ,\n",
       "        0.14677311, 0.17899998, 0.22038816, 0.17745207, 0.22033946,\n",
       "        0.1419057 , 0.17242496, 0.24966533, 0.21345174, 0.13590865,\n",
       "        0.17261642, 0.26958472, 0.21786833, 0.27488452, 0.11098268,\n",
       "        0.14652245, 0.15114585, 0.11465655, 0.17610593, 0.25856864,\n",
       "        0.20107336, 0.27934787, 0.2648805 , 0.18785733, 0.15104586,\n",
       "        0.20058776, 0.3514222 , 0.23911911, 0.16994281, 0.35540068,\n",
       "        0.3101849 , 0.16647328, 0.25431892, 0.23722266, 0.19130215,\n",
       "        0.2241363 , 0.180853  , 0.1712921 , 0.22281988, 0.20541877,\n",
       "        0.2367367 , 0.18749976, 0.22313116, 0.18164028, 0.23787563,\n",
       "        0.24088396, 0.22393976, 0.20124733, 0.16293117, 0.1966261 ,\n",
       "        0.07152703, 0.15596122, 0.23303705, 0.21421076, 0.22385804,\n",
       "        0.2097271 , 0.25453612, 0.16856371, 0.14394283, 0.1448852 ,\n",
       "        0.15947127, 0.13841502, 0.19275272, 0.29155657, 0.24749358,\n",
       "        0.13024701, 0.16956566, 0.17004648, 0.1812624 , 0.30095991,\n",
       "        0.1757653 , 0.20528409, 0.11196781, 0.1969968 , 0.30319107,\n",
       "        0.13788792, 0.23633519, 0.187295  , 0.16935739, 0.2250657 ,\n",
       "        0.20828132, 0.24109007, 0.23534927, 0.14841583, 0.18451957,\n",
       "        0.24818048, 0.18004186, 0.15709585, 0.2631208 , 0.19730636,\n",
       "        0.25744346, 0.1451972 , 0.20750304, 0.19689128, 0.23374832,\n",
       "        0.10633505, 0.17146306, 0.25544086, 0.19385895, 0.13292632,\n",
       "        0.19513977, 0.3013148 , 0.22192858, 0.3464142 , 0.12307444,\n",
       "        0.23528442, 0.11674644, 0.21955478, 0.3540263 , 0.20713867,\n",
       "        0.17078605, 0.20944652, 0.3023449 , 0.2711368 , 0.23545246,\n",
       "        0.16955377, 0.2968876 , 0.20949915, 0.16211122, 0.16430031,\n",
       "        0.13417304, 0.36276808, 0.19224522, 0.23817316, 0.24270606,\n",
       "        0.13406745, 0.20112364, 0.16737626, 0.14278163, 0.22303528,\n",
       "        0.21887343, 0.2250671 , 0.18517284, 0.20313898, 0.20774023,\n",
       "        0.16970639, 0.2094717 , 0.27425554, 0.1921961 , 0.19872023,\n",
       "        0.19619107, 0.1591444 , 0.16700685, 0.22618155, 0.09470681,\n",
       "        0.2162111 , 0.14691283, 0.14807503, 0.14131004, 0.21025285,\n",
       "        0.12837215, 0.12735601, 0.19034205, 0.14748436, 0.21451443,\n",
       "        0.1687767 , 0.23674296], dtype=float32),\n",
       " array([0.34143984, 0.23263146, 0.30375603, 0.2895526 , 0.35867408,\n",
       "        0.24976109, 0.30947605, 0.31370547, 0.2919658 , 0.40697983,\n",
       "        0.4188668 , 0.20504794, 0.2556934 , 0.2296483 , 0.23236087,\n",
       "        0.340631  , 0.380667  , 0.36843285, 0.20050031, 0.48281014,\n",
       "        0.28824714, 0.25121972, 0.21472298, 0.3751821 , 0.23425047,\n",
       "        0.3942884 , 0.3915748 , 0.30221123, 0.28226608, 0.35592458,\n",
       "        0.36062363, 0.43813184], dtype=float32),\n",
       " array([0.261862  , 0.16159996, 0.09561922, 0.23593147, 0.11676674,\n",
       "        0.09959552, 0.20929934, 0.19405669, 0.21563317, 0.10922844,\n",
       "        0.15047486, 0.24518125, 0.09387126, 0.09858105, 0.09344185,\n",
       "        0.22526541, 0.13466683, 0.23449248, 0.12132833, 0.16010612,\n",
       "        0.21074845, 0.16120021, 0.05682711, 0.28481492, 0.19756542,\n",
       "        0.1137289 , 0.17900395, 0.19861773, 0.20668688, 0.23185435,\n",
       "        0.18548895, 0.2185048 , 0.20618857, 0.25412083, 0.20985107,\n",
       "        0.14267531, 0.24331443, 0.21285771, 0.21398564, 0.13556264,\n",
       "        0.12939018, 0.09724374, 0.22323418, 0.2230783 , 0.22707263,\n",
       "        0.1281402 , 0.2081612 , 0.2842243 , 0.18778604, 0.14830089,\n",
       "        0.2248423 , 0.21369122, 0.19661613, 0.163052  , 0.15280123,\n",
       "        0.13789642, 0.15880992, 0.25199148, 0.21800269, 0.1966132 ,\n",
       "        0.15563112, 0.06490895, 0.18092294, 0.26526594, 0.19317897,\n",
       "        0.20377187, 0.17588681, 0.19488694, 0.25171533, 0.15180548,\n",
       "        0.23495455, 0.1891752 , 0.2821005 , 0.175229  , 0.26077455,\n",
       "        0.11811756, 0.06010959, 0.15054032, 0.07457043, 0.22108915,\n",
       "        0.15664794, 0.25055277, 0.29924682, 0.08535111, 0.20246136,\n",
       "        0.06619446, 0.17452177, 0.24624285, 0.22103699, 0.29086736,\n",
       "        0.16555177, 0.22375342, 0.23947835, 0.2155456 , 0.15852648,\n",
       "        0.0973557 , 0.24962756, 0.22857203, 0.22559749, 0.25915235,\n",
       "        0.27875713, 0.08097931, 0.19221163, 0.16045283, 0.21014413,\n",
       "        0.22775377, 0.18634735, 0.08059215, 0.26452407, 0.20181504,\n",
       "        0.07826446, 0.09721109, 0.20279007, 0.10329725, 0.17461962,\n",
       "        0.20657833, 0.19577336, 0.20415056, 0.06205408, 0.2205237 ,\n",
       "        0.17057875, 0.28678846, 0.17908055, 0.14354976, 0.19286513,\n",
       "        0.20740338, 0.10273619, 0.15841515, 0.16877782, 0.22618525,\n",
       "        0.25402603, 0.15092386, 0.22727898, 0.16312994, 0.22910325,\n",
       "        0.16801424, 0.23086384, 0.1796841 , 0.24297231, 0.09808658,\n",
       "        0.12610763, 0.11560615, 0.1155232 , 0.20943251, 0.24136989,\n",
       "        0.1854733 , 0.27420175, 0.23232767, 0.23241869, 0.09768374,\n",
       "        0.29402086, 0.18075584, 0.19917734, 0.29235998, 0.19157466,\n",
       "        0.19508196, 0.11855137, 0.26393726, 0.0810717 , 0.20570986,\n",
       "        0.20048802, 0.23885435, 0.16147737, 0.25328958, 0.22015125,\n",
       "        0.2519788 , 0.26892194, 0.21812654, 0.17852472, 0.1393921 ,\n",
       "        0.13266867, 0.09088935, 0.11473308, 0.14517048, 0.15850843,\n",
       "        0.21621051, 0.24493021, 0.22558549, 0.25999874, 0.19952291,\n",
       "        0.175685  , 0.254743  , 0.16647717, 0.2069053 , 0.11689808,\n",
       "        0.13435501, 0.23529853, 0.10952985, 0.13247697, 0.23396437,\n",
       "        0.25241417, 0.2453912 ], dtype=float32),\n",
       " array([0.21187119, 0.16037653, 0.25604177, 0.22015662, 0.2257629 ,\n",
       "        0.26399884, 0.19300616, 0.30701387, 0.1647918 , 0.25098398,\n",
       "        0.17507079, 0.29799253, 0.19157799, 0.24438585, 0.22471358,\n",
       "        0.19953345, 0.23518609, 0.20477103, 0.41500834, 0.21851414,\n",
       "        0.19325072, 0.23900527, 0.22265881, 0.2789657 , 0.35683042,\n",
       "        0.24324289, 0.28663978, 0.16768622, 0.15945302, 0.22518474,\n",
       "        0.19672602, 0.2779145 , 0.19545588, 0.22984307, 0.1955456 ,\n",
       "        0.24047941, 0.16893974, 0.22024374, 0.16982621, 0.24244712,\n",
       "        0.22384065, 0.24500571, 0.18568507, 0.22579111, 0.23727608,\n",
       "        0.2619544 , 0.20609461, 0.264002  , 0.23248611, 0.23688848,\n",
       "        0.20145774, 0.27527696, 0.20220888, 0.1779079 , 0.19922268,\n",
       "        0.25817662, 0.24119271, 0.18921086, 0.20295341, 0.26609117,\n",
       "        0.2229415 , 0.3060925 , 0.1666439 , 0.212271  , 0.23073791,\n",
       "        0.20209183, 0.21540631, 0.19092089, 0.22919147, 0.11561202,\n",
       "        0.20927985, 0.18354222, 0.24878693, 0.20294435, 0.23207852,\n",
       "        0.2378761 , 0.25610468, 0.18317625, 0.24674001, 0.22862825,\n",
       "        0.23835237, 0.24290961, 0.22863452, 0.24742146, 0.2120357 ,\n",
       "        0.28722334, 0.243001  , 0.22078085, 0.16110776, 0.2318652 ,\n",
       "        0.15148874, 0.23713365, 0.20362817, 0.19927159, 0.1975487 ,\n",
       "        0.23390329, 0.17973311, 0.21850194, 0.20489037, 0.23718451,\n",
       "        0.22484475, 0.21837874, 0.18166222, 0.2058548 , 0.18631615,\n",
       "        0.22307763, 0.31468442, 0.2105359 , 0.23094983, 0.36275586,\n",
       "        0.23595388, 0.21521625, 0.18492208, 0.25698796, 0.1852091 ,\n",
       "        0.3016615 , 0.19556783, 0.19623516, 0.28037933, 0.18184389,\n",
       "        0.20954712, 0.19969663, 0.26430956, 0.17322418, 0.33945957,\n",
       "        0.17771302, 0.20084009, 0.2620286 , 0.28781793, 0.21386635,\n",
       "        0.24654172, 0.15961105, 0.20373169, 0.21718013, 0.21092926,\n",
       "        0.21980204, 0.22798453, 0.18636109, 0.2527766 , 0.20679098,\n",
       "        0.23283114, 0.21389693, 0.19046114, 0.17184943, 0.20887128,\n",
       "        0.25674862, 0.23017672, 0.21005566, 0.19724697, 0.2625807 ,\n",
       "        0.25473118, 0.18474753, 0.25149962, 0.18844806, 0.20763756,\n",
       "        0.2280219 , 0.21405704, 0.2511573 , 0.2084477 , 0.17919001,\n",
       "        0.20363207, 0.26400793, 0.15607096, 0.19507727, 0.2240652 ,\n",
       "        0.27043355, 0.22479321, 0.31557846, 0.18552278, 0.17577673,\n",
       "        0.21187426, 0.3568969 , 0.1541121 , 0.17627478, 0.21331172,\n",
       "        0.18968599, 0.16482718, 0.2130133 , 0.23149008, 0.21102066,\n",
       "        0.25721383, 0.17612937, 0.24018654, 0.17111683, 0.19909103,\n",
       "        0.2757383 , 0.23440857, 0.23382047, 0.24078406, 0.21501957,\n",
       "        0.20034237, 0.16928434], dtype=float32),\n",
       " array([0.43153164, 0.54990876, 0.45656103, 0.4432893 , 0.4656971 ,\n",
       "        0.5131125 , 0.48376736, 0.41795713, 0.3694504 , 0.4606375 ,\n",
       "        0.5353509 , 0.47839624, 0.55455416, 0.52043104, 0.520713  ,\n",
       "        0.5654123 , 0.5192096 , 0.58192086, 0.38113168, 0.57311445,\n",
       "        0.5596774 , 0.48484066, 0.45100585, 0.4185791 , 0.43174237,\n",
       "        0.4826939 , 0.40288627, 0.53049004, 0.36798346, 0.47438836,\n",
       "        0.40297425, 0.5021205 , 0.48013988, 0.5462195 , 0.4300206 ,\n",
       "        0.67369485, 0.46568352, 0.4586021 , 0.4823942 , 0.5495994 ,\n",
       "        0.5432304 , 0.44817337, 0.5250038 , 0.4376138 , 0.36650372,\n",
       "        0.5582689 , 0.4028388 , 0.44562882, 0.48372325, 0.6100713 ,\n",
       "        0.40064096, 0.35252675, 0.46135446, 0.45600894, 0.55761606,\n",
       "        0.4667851 , 0.39273992, 0.4538256 , 0.44192693, 0.3924599 ,\n",
       "        0.38636047, 0.7133815 , 0.5420525 , 0.46304628], dtype=float32),\n",
       " array([0.06992105, 0.08175121, 0.0927068 , 0.09350616, 0.1139941 ,\n",
       "        0.12300281, 0.13062331, 0.13635798, 0.09485502, 0.1170518 ,\n",
       "        0.10727823, 0.0579724 , 0.10472691, 0.09281737, 0.13271077,\n",
       "        0.11122955, 0.09359644, 0.12871945, 0.09605242, 0.08484633,\n",
       "        0.0906411 , 0.12356982, 0.13267662, 0.13482615, 0.06999449,\n",
       "        0.08913464, 0.13159737, 0.1479689 , 0.12010294, 0.08684573,\n",
       "        0.10257576, 0.11310676, 0.10498069, 0.11704352, 0.13512106,\n",
       "        0.11149108, 0.09424138, 0.09203853, 0.14111032, 0.1606444 ,\n",
       "        0.12991922, 0.07510227, 0.09440557, 0.10972359, 0.09273799,\n",
       "        0.08977439, 0.10241971, 0.09622989, 0.15298587, 0.13095142,\n",
       "        0.0962794 , 0.05006593, 0.07940362, 0.13020876, 0.09319265,\n",
       "        0.09615129, 0.14012052, 0.06640805, 0.09079342, 0.08892785,\n",
       "        0.07524186, 0.09246984, 0.1151163 , 0.10547762, 0.10385933,\n",
       "        0.11801125, 0.10550532, 0.0873947 , 0.11555617, 0.10684708,\n",
       "        0.10499307, 0.09010345, 0.13610344, 0.11037505, 0.09447396,\n",
       "        0.08613676, 0.09986043, 0.11136471, 0.13201319, 0.11186249,\n",
       "        0.09048649, 0.15655856, 0.1019941 , 0.09629816, 0.06936555,\n",
       "        0.1361915 , 0.1337676 , 0.15028286, 0.11887503, 0.09794449,\n",
       "        0.12876952, 0.09145182, 0.13925245, 0.10216255, 0.07392774,\n",
       "        0.10927262, 0.08850783, 0.11224818, 0.13147822, 0.1271542 ,\n",
       "        0.11470326, 0.11727698, 0.10224483, 0.08182349, 0.15180974,\n",
       "        0.11365966, 0.12197571, 0.0905211 , 0.13454399, 0.13042398,\n",
       "        0.08402921, 0.11042815, 0.1403562 , 0.07431533, 0.08264913,\n",
       "        0.11453693, 0.06168905, 0.10592508, 0.09651904, 0.11044914,\n",
       "        0.11695466, 0.12599884, 0.14084342, 0.12230174, 0.10262039,\n",
       "        0.13724275, 0.09482012, 0.16012824, 0.10373937, 0.09047667,\n",
       "        0.09686776, 0.04187523, 0.06917653, 0.10562669, 0.11267928,\n",
       "        0.09442817, 0.05382849, 0.11399189, 0.10751537, 0.15098017,\n",
       "        0.10921516, 0.10663566, 0.11562331, 0.06575653, 0.1005667 ,\n",
       "        0.10283653, 0.09353674, 0.10533803, 0.126524  , 0.06825923,\n",
       "        0.06318447, 0.11328195, 0.09684841, 0.13661188, 0.09623174,\n",
       "        0.12007227, 0.10530819, 0.09151872, 0.0862238 , 0.10917413,\n",
       "        0.10708537, 0.14399417, 0.07692593, 0.10760647, 0.09433011,\n",
       "        0.08471777, 0.12113911, 0.06995248, 0.07913499, 0.07799549,\n",
       "        0.1061696 , 0.10782355, 0.12070815, 0.1032254 , 0.06469379,\n",
       "        0.12387953, 0.1288835 , 0.11786664, 0.11593465, 0.10226668,\n",
       "        0.08675504, 0.09923477, 0.08736486, 0.07559434, 0.11524933,\n",
       "        0.10235906, 0.07362385, 0.08368186, 0.09483518, 0.09146191,\n",
       "        0.09010127, 0.09919784, 0.09495774, 0.05305803, 0.09958737,\n",
       "        0.09101333, 0.08524541, 0.08562883, 0.11148277, 0.08692686,\n",
       "        0.08799062, 0.10174304, 0.1178654 , 0.08901305, 0.0827448 ,\n",
       "        0.09418921, 0.06681593, 0.09088983, 0.10841841, 0.12664436,\n",
       "        0.09339731, 0.10659288, 0.12512654, 0.11816055, 0.12119592,\n",
       "        0.10797323, 0.17249024, 0.10914634, 0.14693074, 0.06229895,\n",
       "        0.13114944, 0.08732329, 0.06912979, 0.10975959, 0.10447761,\n",
       "        0.09850015, 0.080202  , 0.1414794 , 0.10207403, 0.07840877,\n",
       "        0.14515388, 0.08236177, 0.1138768 , 0.09273895, 0.08452979,\n",
       "        0.14251962, 0.10132486, 0.11168931, 0.09760245, 0.10730118,\n",
       "        0.10507373, 0.07951047, 0.08944511, 0.1306762 , 0.09185834,\n",
       "        0.16581431, 0.09360593, 0.10006147, 0.09220747, 0.08379398,\n",
       "        0.10078772, 0.10082043, 0.10549876, 0.12646776, 0.14566198,\n",
       "        0.1279312 , 0.11047862, 0.14426148, 0.07782884, 0.13790533,\n",
       "        0.11343588, 0.09507421, 0.09874247, 0.07827184, 0.12461575,\n",
       "        0.07679155, 0.07466284, 0.08877729, 0.10985515, 0.09282554,\n",
       "        0.12769797, 0.09427487, 0.10837118, 0.10504839, 0.09648112,\n",
       "        0.1151194 , 0.09173775, 0.08642662, 0.06640594, 0.09538248,\n",
       "        0.0999315 , 0.09419073, 0.100344  , 0.06798992, 0.13028081,\n",
       "        0.14474933, 0.08612047, 0.11406685, 0.11709194, 0.08215309,\n",
       "        0.18531463, 0.0857496 , 0.08435774, 0.07005146, 0.08095227,\n",
       "        0.12505157, 0.07375365, 0.07725558, 0.11085205, 0.09823965,\n",
       "        0.09304521, 0.08127791, 0.10263604, 0.13808598, 0.13594912,\n",
       "        0.06896987, 0.09859595, 0.07876499, 0.08526075, 0.12178212,\n",
       "        0.09288269, 0.15876801, 0.0674732 , 0.08835673, 0.13399984,\n",
       "        0.11315149, 0.15838598, 0.11260527, 0.11877622, 0.10276531,\n",
       "        0.11666284, 0.11147829, 0.09739869, 0.0902997 , 0.16146053,\n",
       "        0.10309522, 0.09762662, 0.1039544 , 0.0901905 , 0.09457133,\n",
       "        0.07663137, 0.08090817, 0.12171248, 0.11087412, 0.08623413,\n",
       "        0.11802724, 0.09678385, 0.12348874, 0.12294918, 0.08973846,\n",
       "        0.09984017, 0.0585082 , 0.13296376, 0.1449697 , 0.1210145 ,\n",
       "        0.11720667, 0.1239287 , 0.1755028 , 0.10981414, 0.10310593,\n",
       "        0.06224163, 0.09255431, 0.11592819, 0.17730476, 0.08222374,\n",
       "        0.09933531, 0.10191387, 0.10613429, 0.10606997, 0.15470272,\n",
       "        0.15424763, 0.09397261, 0.10034533, 0.10960823, 0.09744031,\n",
       "        0.12375128, 0.08566758, 0.13107888, 0.10628295, 0.05378797,\n",
       "        0.10308549, 0.15377651, 0.09048055, 0.10820175, 0.11221451,\n",
       "        0.08620232, 0.08719621, 0.14042003, 0.09670758, 0.09551904,\n",
       "        0.11260071, 0.10388206, 0.11621647, 0.13116679], dtype=float32),\n",
       " array([0.22511101, 0.20012207, 0.17868198, 0.2645429 , 0.22314289,\n",
       "        0.17141046, 0.229025  , 0.11256476, 0.21636611, 0.14424025,\n",
       "        0.17769642, 0.22319259, 0.19545685, 0.18489589, 0.16350123,\n",
       "        0.14632098, 0.19446175, 0.11704791, 0.19807272, 0.18247592,\n",
       "        0.18828973, 0.15750223, 0.21932553, 0.24380763, 0.15788278,\n",
       "        0.15981497, 0.14195393, 0.18814161, 0.18741561, 0.15524581,\n",
       "        0.22891636, 0.2158576 , 0.18105315, 0.13377121, 0.18629412,\n",
       "        0.113451  , 0.15144801, 0.18017985, 0.2047069 , 0.0957311 ,\n",
       "        0.16821708, 0.2077162 , 0.25863594, 0.25037715, 0.1633741 ,\n",
       "        0.17563352, 0.14758053, 0.22164954, 0.10892158, 0.173904  ,\n",
       "        0.25289473, 0.2099796 , 0.19070166, 0.12361405, 0.18732521,\n",
       "        0.1857538 , 0.11062574, 0.23453647, 0.18117504, 0.1937413 ,\n",
       "        0.33744064, 0.22892591, 0.15710603, 0.28001395, 0.1923621 ,\n",
       "        0.16670893, 0.2169763 , 0.16380866, 0.13418245, 0.20220202,\n",
       "        0.2310036 , 0.16437505, 0.08435912, 0.18121958, 0.18056473,\n",
       "        0.2724986 , 0.19840178, 0.23619643, 0.1546374 , 0.16367452,\n",
       "        0.15802114, 0.16061166, 0.13794754, 0.23028448, 0.18299524,\n",
       "        0.17504787, 0.11706446, 0.20949544, 0.07590427, 0.22255366,\n",
       "        0.17263271, 0.20261642, 0.12316303, 0.13353229, 0.10768773,\n",
       "        0.18342963, 0.18023469, 0.26026076, 0.1720911 , 0.16342331,\n",
       "        0.15356004, 0.18859035, 0.20354956, 0.22402133, 0.2539772 ,\n",
       "        0.24637175, 0.13032624, 0.1480296 , 0.14149736, 0.2284936 ,\n",
       "        0.13492794, 0.19734286, 0.17118116, 0.1898062 , 0.20474894,\n",
       "        0.17315865, 0.26582944, 0.2510043 , 0.22487894, 0.14327078,\n",
       "        0.12321308, 0.22508216, 0.16082652, 0.14612775, 0.16784197,\n",
       "        0.1445789 , 0.18080467, 0.1515863 , 0.19987555, 0.13242306,\n",
       "        0.20454046, 0.19499771, 0.26149708, 0.17280608, 0.19224004,\n",
       "        0.19212419, 0.16829324, 0.13595036, 0.20684728, 0.22724895,\n",
       "        0.23522872, 0.16765416, 0.19176856, 0.23241438, 0.16687812,\n",
       "        0.15941519, 0.2084234 , 0.14213347, 0.11763304, 0.32487056,\n",
       "        0.1751812 , 0.1430249 , 0.18414806, 0.16202202, 0.2180016 ,\n",
       "        0.19407023, 0.18481185, 0.14572558, 0.16407046, 0.18253894,\n",
       "        0.14525858, 0.1622258 , 0.24769086, 0.16735666, 0.18745697,\n",
       "        0.236109  , 0.22122552, 0.30765775, 0.19042037, 0.27313372,\n",
       "        0.14218569, 0.13070813, 0.20555125, 0.198935  , 0.19702332,\n",
       "        0.20978065, 0.25346574, 0.1154332 , 0.26780564, 0.15690942,\n",
       "        0.21293041, 0.16359693, 0.20332327, 0.2947166 , 0.21846293,\n",
       "        0.1090373 , 0.20026235, 0.27653483, 0.17559437, 0.22506404,\n",
       "        0.17682154, 0.19007069, 0.24620995, 0.22521955, 0.20837754,\n",
       "        0.22001113, 0.21626587, 0.14334019, 0.14916839, 0.18779065,\n",
       "        0.21025203, 0.21224064, 0.17392153, 0.20688842, 0.17304358,\n",
       "        0.16818884, 0.22818068, 0.15144548, 0.14822604, 0.22560835,\n",
       "        0.18538412, 0.14876834, 0.12468958, 0.14944127, 0.11214822,\n",
       "        0.17074464, 0.20484084, 0.09605689, 0.15211144, 0.23588128,\n",
       "        0.0974094 , 0.1844894 , 0.19778031, 0.16788596, 0.13640833,\n",
       "        0.20752446, 0.2121347 , 0.20481603, 0.16203523, 0.18860221,\n",
       "        0.11681739, 0.17045902, 0.1975973 , 0.20208678, 0.1985778 ,\n",
       "        0.19006696, 0.13117291, 0.20563877, 0.19926943, 0.13258941,\n",
       "        0.15362489, 0.1663239 , 0.18146119, 0.16560708, 0.20981328,\n",
       "        0.1686557 , 0.22064888, 0.15125865, 0.27474862, 0.18811303,\n",
       "        0.2015943 , 0.15632689, 0.24291216, 0.14615718, 0.15153252,\n",
       "        0.15298982, 0.12788759, 0.09682783, 0.18448003, 0.20228446,\n",
       "        0.20935881, 0.16664377, 0.20582828, 0.18283613, 0.12801845,\n",
       "        0.24387366, 0.22714178, 0.18330216, 0.12997407, 0.18557422,\n",
       "        0.19630297, 0.19270802, 0.20991105, 0.16041315, 0.21917391,\n",
       "        0.14227492, 0.1991507 , 0.21266574, 0.23571537, 0.17701656,\n",
       "        0.08795814, 0.15449245, 0.20667894, 0.24018055, 0.12970094,\n",
       "        0.14560501, 0.1745822 , 0.19511038, 0.14881155, 0.20322213,\n",
       "        0.23781674, 0.19105273, 0.1477078 , 0.14217746, 0.20525505,\n",
       "        0.20083934, 0.27467597, 0.2860656 , 0.22646227, 0.18360683,\n",
       "        0.1958785 , 0.18479185, 0.15035468, 0.24923912, 0.10262024,\n",
       "        0.24331658, 0.15877846, 0.2374229 , 0.18811248, 0.18322136,\n",
       "        0.14859183, 0.12886234, 0.14581741, 0.17783046, 0.15385775,\n",
       "        0.10399416, 0.45018205, 0.18713774, 0.22279665, 0.1982468 ,\n",
       "        0.16785336, 0.08648981, 0.19966911, 0.14479467, 0.12875818,\n",
       "        0.18469276, 0.13608362, 0.25307322, 0.18060519, 0.14968586,\n",
       "        0.19925945, 0.21778344, 0.17089988, 0.16942239, 0.1790698 ,\n",
       "        0.21728627, 0.18974116, 0.21810049, 0.19915259, 0.18746792,\n",
       "        0.20404838, 0.1315837 , 0.20323245, 0.2131243 , 0.13106114,\n",
       "        0.1787575 , 0.15408301, 0.19434349, 0.15684389, 0.10208114,\n",
       "        0.20982376, 0.19445097, 0.20417471, 0.12123639, 0.24008383,\n",
       "        0.17980008, 0.17968766, 0.14775586, 0.1467475 , 0.11752354,\n",
       "        0.21946765, 0.26022556, 0.23617512, 0.1879991 , 0.16464962,\n",
       "        0.18101282, 0.1446878 , 0.14547388, 0.17119698, 0.22937036,\n",
       "        0.24425533, 0.11567076, 0.21155915, 0.14802557, 0.16305031,\n",
       "        0.276254  , 0.14255431, 0.17799509, 0.20443487, 0.20562284,\n",
       "        0.1768352 , 0.17008795, 0.09487249, 0.12319388], dtype=float32),\n",
       " array([0.33624905, 0.24350259, 0.28747448, 0.3851344 , 0.26944336,\n",
       "        0.30778316, 0.26157865, 0.41000012, 0.40112153, 0.23274994,\n",
       "        0.22375745, 0.25876048, 0.21236628, 0.28341675, 0.28889415,\n",
       "        0.30604076, 0.23436633, 0.19209531, 0.33303067, 0.20229658,\n",
       "        0.20133789, 0.24583524, 0.28562668, 0.42496678, 0.2858685 ,\n",
       "        0.26447928, 0.41178703, 0.21577626, 0.38480467, 0.27184775,\n",
       "        0.31367794, 0.3004112 , 0.22877313, 0.1870055 , 0.5030292 ,\n",
       "        0.2717474 , 0.2787293 , 0.2541513 , 0.28850305, 0.21893957,\n",
       "        0.28293484, 0.26275298, 0.25595206, 0.24731135, 0.39411932,\n",
       "        0.2588629 , 0.38398832, 0.38777485, 0.25141153, 0.23088229,\n",
       "        0.35683024, 0.65209097, 0.29228768, 0.3132114 , 0.32835984,\n",
       "        0.2846713 , 0.40334895, 0.3396369 , 0.35267112, 0.54707146,\n",
       "        0.3717978 , 0.31776386, 0.20166531, 0.31015325], dtype=float32),\n",
       " array([0.11944663, 0.08074839, 0.09130917, 0.14190738, 0.15755348,\n",
       "        0.09862688, 0.06883112, 0.11033188, 0.13325617, 0.10009504,\n",
       "        0.12238075, 0.08351094, 0.11509067, 0.10316918, 0.0705956 ,\n",
       "        0.07578422, 0.108952  , 0.13088256, 0.10000262, 0.14569573,\n",
       "        0.07740607, 0.07837243, 0.12284569, 0.09744621, 0.1472202 ,\n",
       "        0.14982457, 0.07014216, 0.1081178 , 0.12206253, 0.1168742 ,\n",
       "        0.1075436 , 0.10735622, 0.06598799, 0.09864665, 0.1289837 ,\n",
       "        0.07296345, 0.15234427, 0.10156918, 0.11112215, 0.09625036,\n",
       "        0.10146272, 0.14174205, 0.10574888, 0.12814459, 0.08035084,\n",
       "        0.11166303, 0.17080346, 0.09914325, 0.08540282, 0.14548242,\n",
       "        0.10947601, 0.14427094, 0.13543646, 0.1099506 , 0.06458413,\n",
       "        0.07750791, 0.06918251, 0.12186892, 0.09130197, 0.1613293 ,\n",
       "        0.12642261, 0.10867603, 0.07422715, 0.10683897, 0.11001161,\n",
       "        0.09361402, 0.14797874, 0.10310844, 0.11807594, 0.06012864,\n",
       "        0.09376307, 0.07908385, 0.09003859, 0.08616652, 0.17625113,\n",
       "        0.12944311, 0.07346119, 0.08277337, 0.086435  , 0.09305798,\n",
       "        0.02992484, 0.13882652, 0.11673509, 0.1473009 , 0.10873039,\n",
       "        0.10469814, 0.11665105, 0.06804628, 0.10071528, 0.08417068,\n",
       "        0.12096506, 0.07958533, 0.00473263, 0.07722913, 0.19406842,\n",
       "        0.11403582, 0.12879632, 0.03716339, 0.09748889, 0.14167066,\n",
       "        0.1410923 , 0.10776152, 0.11042655, 0.10154638, 0.05225489,\n",
       "        0.12258995, 0.11528131, 0.09520947, 0.11539327, 0.08929235,\n",
       "        0.07742789, 0.06334871, 0.11258303, 0.11569691, 0.02858976,\n",
       "        0.15018918, 0.08617713, 0.14247511, 0.05813683, 0.11381523,\n",
       "        0.12446738, 0.08462075, 0.07806347, 0.13961184, 0.199813  ,\n",
       "        0.08085466, 0.10300729, 0.09691878, 0.12219477, 0.11350878,\n",
       "        0.07782929, 0.09281647, 0.12952451, 0.11143488, 0.08524988,\n",
       "        0.12628794, 0.13668136, 0.12076753, 0.10128298, 0.10200881,\n",
       "        0.13301094, 0.12765788, 0.1220743 , 0.15186147, 0.0850402 ,\n",
       "        0.13017653, 0.09269498, 0.10264307, 0.06373189, 0.11206486,\n",
       "        0.13832794, 0.10708762, 0.09171938, 0.10288333, 0.11635029,\n",
       "        0.10874914, 0.12223876, 0.09699574, 0.07950208, 0.10705538,\n",
       "        0.09399927, 0.11653658, 0.14379555, 0.12492977, 0.11010978,\n",
       "        0.10955694, 0.09186441, 0.13886406, 0.08080333, 0.06493731,\n",
       "        0.127615  , 0.1103362 , 0.14516829, 0.12072827, 0.11885313,\n",
       "        0.12203468, 0.12552965, 0.08748914, 0.06644587, 0.09803927,\n",
       "        0.14383918, 0.12941372, 0.08577149, 0.12602104, 0.12408172,\n",
       "        0.07672227, 0.08629777, 0.10245804, 0.09047411, 0.10939597,\n",
       "        0.0975756 , 0.12060426, 0.13069938, 0.09585182, 0.09860934,\n",
       "        0.1018717 , 0.09455618, 0.06217549, 0.11108698, 0.11059309,\n",
       "        0.13340686, 0.08763705, 0.08927174, 0.135716  , 0.10113388,\n",
       "        0.06846626, 0.1261948 , 0.12474176, 0.1339371 , 0.11980899,\n",
       "        0.1674308 , 0.08604797, 0.09224478, 0.09737267, 0.07063641,\n",
       "        0.09650485, 0.06174697, 0.0941049 , 0.02373824, 0.12575969,\n",
       "        0.08856367, 0.09884451, 0.08859342, 0.11042651, 0.11846215,\n",
       "        0.10102605, 0.0975003 , 0.12359753, 0.11349186, 0.11065928,\n",
       "        0.10305598, 0.20114301, 0.13176037, 0.11127645, 0.11627863,\n",
       "        0.09178028, 0.0969777 , 0.10503653, 0.08062578, 0.09204809,\n",
       "        0.11327093, 0.11086828, 0.12797363, 0.12851804, 0.13357183,\n",
       "        0.11011461, 0.07017475, 0.10007707, 0.11148001, 0.0882893 ,\n",
       "        0.08845051, 0.10801025, 0.12642924, 0.09269055, 0.06498678,\n",
       "        0.1000546 , 0.1831721 , 0.11617577, 0.10664943, 0.13430491,\n",
       "        0.11248595, 0.09339625, 0.11723307, 0.07474967, 0.14043288,\n",
       "        0.12565109, 0.00233803, 0.11731884, 0.10645232, 0.09687668,\n",
       "        0.10423835, 0.12671944, 0.08899878, 0.083338  , 0.10544252,\n",
       "        0.12611285, 0.09994338, 0.091515  , 0.11644147, 0.14000879,\n",
       "        0.11386591, 0.08497177, 0.10059084, 0.09438007, 0.10116394,\n",
       "        0.11162175, 0.08046199, 0.12223158, 0.06196954, 0.09599072,\n",
       "        0.08590078, 0.1172898 , 0.09612732, 0.10165972, 0.08710629,\n",
       "        0.09174313, 0.09369464, 0.04503248, 0.08575036, 0.11097008,\n",
       "        0.11395084, 0.08743341, 0.10590028, 0.09602148, 0.11646703,\n",
       "        0.08867204, 0.13387422, 0.13935685, 0.11516946, 0.13819413,\n",
       "        0.12478498, 0.07527091, 0.11883844, 0.11296166, 0.11021829,\n",
       "        0.06791741, 0.00425542, 0.11628938, 0.14369431, 0.09953881,\n",
       "        0.11535935, 0.12196063, 0.07818954, 0.10876887, 0.11457663,\n",
       "        0.10721382, 0.11618634, 0.07742792, 0.12592342, 0.11831674,\n",
       "        0.14030291, 0.08674726, 0.12414782, 0.07002869, 0.07089068,\n",
       "        0.10673656, 0.08205243, 0.11806411, 0.15253648, 0.11230666,\n",
       "        0.10633993, 0.10068409, 0.09356228, 0.09028896, 0.06300983,\n",
       "        0.1331868 , 0.06823412, 0.13221098, 0.08875419, 0.10288942,\n",
       "        0.12910762, 0.13882574, 0.09825101, 0.07088102, 0.07757752,\n",
       "        0.11383719, 0.09367359, 0.07379423, 0.1159021 , 0.13624623,\n",
       "        0.09543433, 0.13529092, 0.21574052, 0.09906121, 0.11371125,\n",
       "        0.09782466, 0.10572424, 0.12804893, 0.10209657, 0.07781564,\n",
       "        0.10403477, 0.11936891, 0.12616281, 0.13343675, 0.12265989,\n",
       "        0.10489687, 0.11049887, 0.09726556, 0.10487244, 0.12840125,\n",
       "        0.11517101, 0.12274511, 0.08360949, 0.17669095], dtype=float32),\n",
       " array([0.13886712, 0.2118692 , 0.17418264, 0.09997132, 0.18092123,\n",
       "        0.16847393, 0.19545092, 0.16320993, 0.15213849, 0.15367272,\n",
       "        0.14101166, 0.24785385, 0.17562006, 0.10083959, 0.21746399,\n",
       "        0.1642287 , 0.16676733, 0.18749198, 0.20937572, 0.18133307,\n",
       "        0.14485228, 0.18829359, 0.16790879, 0.1961188 , 0.10824189,\n",
       "        0.1516399 , 0.21816213, 0.17749427, 0.12263576, 0.14663781,\n",
       "        0.16628766, 0.1503223 , 0.23150301, 0.17367926, 0.14865118,\n",
       "        0.23435993, 0.17591456, 0.14314047, 0.13094166, 0.19785964,\n",
       "        0.23705184, 0.13629249, 0.14562792, 0.1207594 , 0.21623212,\n",
       "        0.19855882, 0.31757498, 0.19344021, 0.1800597 , 0.11500506,\n",
       "        0.19179897, 0.2696672 , 0.10917436, 0.16849579, 0.20074628,\n",
       "        0.15928736, 0.16004156, 0.15160646, 0.21920983, 0.1464365 ,\n",
       "        0.14725478, 0.16352467, 0.14239609, 0.15908773, 0.1833693 ,\n",
       "        0.16979608, 0.10115562, 0.18290307, 0.10453372, 0.16574743,\n",
       "        0.19651811, 0.19828834, 0.1722935 , 0.22341909, 0.2606411 ,\n",
       "        0.14299825, 0.18969797, 0.17393407, 0.14521675, 0.19003801,\n",
       "        0.23619783, 0.19614422, 0.14645004, 0.21835731, 0.23480412,\n",
       "        0.1353332 , 0.11585996, 0.19540405, 0.14649312, 0.26447782,\n",
       "        0.11711214, 0.1935016 , 0.05697666, 0.21008253, 0.1344981 ,\n",
       "        0.1658031 , 0.13513234, 0.25661853, 0.14763376, 0.15257828,\n",
       "        0.15672092, 0.2717478 , 0.19586939, 0.12332069, 0.18358786,\n",
       "        0.10905857, 0.17267567, 0.14845102, 0.11634433, 0.13146591,\n",
       "        0.17091122, 0.25921157, 0.16343226, 0.15733269, 0.2874291 ,\n",
       "        0.24711213, 0.1664566 , 0.16340166, 0.20598012, 0.28738675,\n",
       "        0.1867183 , 0.16108546, 0.20893554, 0.11741745, 0.24878123,\n",
       "        0.23439427, 0.1762404 , 0.16039562, 0.1610896 , 0.1560728 ,\n",
       "        0.15687   , 0.2013491 , 0.15455239, 0.15911886, 0.13092268,\n",
       "        0.23643091, 0.12100586, 0.17231415, 0.14612027, 0.20190836,\n",
       "        0.20511511, 0.21585181, 0.16572729, 0.13004059, 0.18627839,\n",
       "        0.13797645, 0.11785028, 0.11140454, 0.22747779, 0.14772296,\n",
       "        0.12484574, 0.20741439, 0.16323994, 0.24970813, 0.12181607,\n",
       "        0.23151237, 0.11593694, 0.3528174 , 0.2628971 , 0.12133317,\n",
       "        0.15436625, 0.11584394, 0.15800487, 0.1065969 , 0.17514046,\n",
       "        0.16500701, 0.25598177, 0.10229079, 0.12249218, 0.24409448,\n",
       "        0.1962375 , 0.20767342, 0.08639344, 0.15467183, 0.17576091,\n",
       "        0.143045  , 0.15874632, 0.11781678, 0.13288862, 0.19305955,\n",
       "        0.14157318, 0.16553913, 0.11051401, 0.160412  , 0.12037264,\n",
       "        0.1652954 , 0.1955247 , 0.16495621, 0.1728972 , 0.17288321,\n",
       "        0.17665923, 0.10846603, 0.15485682, 0.13613723, 0.1572242 ,\n",
       "        0.17099132, 0.15828024, 0.21153928, 0.12174655, 0.15843509,\n",
       "        0.18074068, 0.2188999 , 0.24348414, 0.13885862, 0.11559317,\n",
       "        0.21467867, 0.15358153, 0.18264079, 0.14808843, 0.18330769,\n",
       "        0.14668834, 0.12387402, 0.15079173, 0.14709921, 0.19734417,\n",
       "        0.14384294, 0.20788738, 0.1391656 , 0.25363293, 0.19778836,\n",
       "        0.19517958, 0.1740704 , 0.10089896, 0.14004159, 0.08522326,\n",
       "        0.12447947, 0.15234892, 0.17195311, 0.1175162 , 0.17691343,\n",
       "        0.12280309, 0.27101842, 0.15534288, 0.16520573, 0.29629704,\n",
       "        0.21217124, 0.16923761, 0.1932078 , 0.1578402 , 0.18457209,\n",
       "        0.17200501, 0.1561437 , 0.21735252, 0.28123885, 0.16797732,\n",
       "        0.12345136, 0.17020175, 0.13782987, 0.16112758, 0.20351847,\n",
       "        0.17177592, 0.127858  , 0.11276554, 0.21180452, 0.1912907 ,\n",
       "        0.15707874, 0.14322598, 0.16077916, 0.18611215, 0.12374607,\n",
       "        0.18418062, 0.10257454, 0.14722288, 0.16939405, 0.13100053,\n",
       "        0.1804204 , 0.01216018, 0.19645758, 0.17630449, 0.19328839,\n",
       "        0.13023406, 0.13432315, 0.17785974, 0.29691243, 0.22117138,\n",
       "        0.128658  , 0.15517552, 0.1872813 , 0.1369116 , 0.1802382 ,\n",
       "        0.16932192, 0.17656288, 0.1603686 , 0.18658052, 0.13223733,\n",
       "        0.18533678, 0.27158734, 0.18222257, 0.26243803, 0.20106113,\n",
       "        0.23816644, 0.10259128, 0.12734696, 0.15721323, 0.22461209,\n",
       "        0.14498618, 0.18932977, 0.20917882, 0.14088997, 0.17263032,\n",
       "        0.21465053, 0.17745611, 0.13194445, 0.28885278, 0.11207411,\n",
       "        0.20146047, 0.16946952, 0.14524332, 0.15008944, 0.18052918,\n",
       "        0.19730026, 0.2122761 , 0.20985237, 0.10648723, 0.21939655,\n",
       "        0.24745882, 0.02971236, 0.1257155 , 0.1351336 , 0.11945719,\n",
       "        0.17971066, 0.17662722, 0.16001798, 0.16725664, 0.11654092,\n",
       "        0.13734688, 0.10222002, 0.22396086, 0.16885531, 0.2398091 ,\n",
       "        0.10941152, 0.15334623, 0.10906606, 0.19102228, 0.21872757,\n",
       "        0.1678812 , 0.29704362, 0.108404  , 0.17702278, 0.17773817,\n",
       "        0.09645669, 0.11688024, 0.23230052, 0.18550056, 0.28546816,\n",
       "        0.19353096, 0.18517731, 0.12274989, 0.19028465, 0.13699107,\n",
       "        0.15668134, 0.13567795, 0.15997088, 0.22929876, 0.21899906,\n",
       "        0.20976768, 0.19273038, 0.20649087, 0.2365673 , 0.13144498,\n",
       "        0.15464824, 0.17673472, 0.13849837, 0.18352719, 0.12725349,\n",
       "        0.23329596, 0.1794576 , 0.17283793, 0.15184025, 0.19596699,\n",
       "        0.16956025, 0.14164603, 0.22401091, 0.18221797, 0.25304365,\n",
       "        0.15484177, 0.16775747, 0.17371331, 0.14014359, 0.13897024,\n",
       "        0.15334024, 0.14084862, 0.13066304, 0.12077443], dtype=float32),\n",
       " array([0.27883175, 0.19353083, 0.23340327, 0.2404531 , 0.21276413,\n",
       "        0.21505691, 0.26295006, 0.26936567, 0.34015214, 0.25101775,\n",
       "        0.20326775, 0.24941728, 0.23583838, 0.231806  , 0.20352171,\n",
       "        0.17976725, 0.24057604, 0.17787461, 0.36826438, 0.16217875,\n",
       "        0.19902693, 0.23120806, 0.27034628, 0.27858147, 0.27726182,\n",
       "        0.23342925, 0.27938804, 0.21658602, 0.33811915, 0.22291009,\n",
       "        0.33441955, 0.300136  , 0.2027327 , 0.19767909, 0.34309673,\n",
       "        0.16806708, 0.21533684, 0.20992872, 0.27083853, 0.19377372,\n",
       "        0.23646428, 0.32570902, 0.17710069, 0.3021937 , 0.31438956,\n",
       "        0.20051149, 0.26648957, 0.27401417, 0.23167631, 0.20353778,\n",
       "        0.3181514 , 0.3262443 , 0.2802894 , 0.245008  , 0.29779616,\n",
       "        0.21792352, 0.31156683, 0.27108428, 0.22970943, 0.31223446,\n",
       "        0.30760112, 0.22754075, 0.15984523, 0.22936597], dtype=float32),\n",
       " array([0.00354157, 0.15179344, 0.09461849, 0.10599834, 0.10522746,\n",
       "        0.09432472, 0.12056044, 0.13584504, 0.11613891, 0.09680261,\n",
       "        0.07786866, 0.1262283 , 0.10598517, 0.10658296, 0.14597107,\n",
       "        0.12500541, 0.12698634, 0.0957799 , 0.1083963 , 0.10041394,\n",
       "        0.1088438 , 0.11161821, 0.10418428, 0.04271697, 0.1511393 ,\n",
       "        0.1096926 , 0.10939803, 0.09506714, 0.09210725, 0.12841375,\n",
       "        0.13797827, 0.10372788, 0.12304805, 0.05307267, 0.1305471 ,\n",
       "        0.13941772, 0.11607021, 0.13669528, 0.11593766, 0.12285881,\n",
       "        0.12898822, 0.11614614, 0.12397581, 0.11994432, 0.13252121,\n",
       "        0.08686726, 0.11129997, 0.12079376, 0.09753583, 0.12660222,\n",
       "        0.13559146, 0.07013731, 0.05362075, 0.1895801 , 0.14869359,\n",
       "        0.09873675, 0.0968003 , 0.09885316, 0.12223835, 0.10566723,\n",
       "        0.11559705, 0.13269636, 0.11614729, 0.09230998, 0.11247437,\n",
       "        0.11726207, 0.08093599, 0.10290947, 0.11548126, 0.1087411 ,\n",
       "        0.10455647, 0.10100214, 0.16092545, 0.1216842 , 0.11759847,\n",
       "        0.10628533, 0.0971867 , 0.11778607, 0.13357379, 0.08868486,\n",
       "        0.14145716, 0.10543539, 0.11038409, 0.15459096, 0.07320413,\n",
       "        0.09463227, 0.12297647, 0.07886838, 0.10977061, 0.11856727,\n",
       "        0.11172705, 0.05294114, 0.09668171, 0.10991377, 0.11189657,\n",
       "        0.11614985, 0.05034812, 0.07554093, 0.1042221 , 0.12786333,\n",
       "        0.08980465, 0.11178273, 0.11636782, 0.08006552, 0.0907459 ,\n",
       "        0.10089254, 0.11700416, 0.0815149 , 0.1513534 , 0.1412712 ,\n",
       "        0.10650755, 0.13520144, 0.12573919, 0.10367876, 0.10336826,\n",
       "        0.11916894, 0.12339797, 0.08941606, 0.11825724, 0.2015604 ,\n",
       "        0.08556223, 0.10720345, 0.12187576, 0.07170689, 0.11063231,\n",
       "        0.08356753, 0.14291747, 0.09700821, 0.11427606, 0.07987286,\n",
       "        0.12499375, 0.11620735, 0.10081214, 0.10670525, 0.10163455,\n",
       "        0.13727117, 0.10262606, 0.08975335, 0.09792256, 0.10293642,\n",
       "        0.08290936, 0.12317429, 0.10287683, 0.11393736, 0.07521797,\n",
       "        0.13430627, 0.0969519 , 0.09955052, 0.103691  , 0.00099241,\n",
       "        0.15855286, 0.0901375 , 0.09216051, 0.09839123, 0.11637954,\n",
       "        0.11353169, 0.01063304, 0.1248638 , 0.09776789, 0.10907734,\n",
       "        0.12426115, 0.1186059 , 0.08317661, 0.09024818, 0.1067333 ,\n",
       "        0.09211171, 0.1053628 , 0.13124838, 0.10629858, 0.12922092,\n",
       "        0.09928844, 0.12822273, 0.09408581, 0.09385153, 0.11599485,\n",
       "        0.11289497, 0.11055405, 0.05979485, 0.06999359, 0.10352442,\n",
       "        0.09592686, 0.10985436, 0.00870568, 0.10634778, 0.10258725,\n",
       "        0.12963948, 0.10703529, 0.16507065, 0.09154781, 0.12528756,\n",
       "        0.11558016, 0.14499071, 0.14069216, 0.09169718, 0.11090875,\n",
       "        0.11744688, 0.09549093, 0.09368147, 0.12601502, 0.13855994,\n",
       "        0.04837756, 0.12964924, 0.1392493 , 0.07302536, 0.12830928,\n",
       "        0.11525875, 0.08359036, 0.08247095, 0.09771836, 0.10903295,\n",
       "        0.12205189, 0.1193073 , 0.11022563, 0.11120713, 0.1382461 ,\n",
       "        0.12099883, 0.11397107, 0.09529208, 0.1198827 , 0.12947392,\n",
       "        0.1314816 , 0.08802471, 0.11115276, 0.09041661, 0.13195871,\n",
       "        0.13301638, 0.10430692, 0.1689547 , 0.105797  , 0.11220179,\n",
       "        0.10232432, 0.06440881, 0.10579844, 0.11401609, 0.08674391,\n",
       "        0.08430241, 0.12043869, 0.08618342, 0.1432927 , 0.08560658,\n",
       "        0.12226789, 0.08776177, 0.10694101, 0.11640259, 0.10313278,\n",
       "        0.11577404, 0.0828045 , 0.11102253, 0.13673156, 0.14710033,\n",
       "        0.14609954, 0.11953431, 0.12793615, 0.10624704, 0.08900628,\n",
       "        0.11306049, 0.06905277, 0.14042763, 0.11021817, 0.12790327,\n",
       "        0.09250168, 0.06739673, 0.13940695, 0.11786895, 0.11035228,\n",
       "        0.14733826, 0.11993888, 0.04439748, 0.08524884, 0.12335619,\n",
       "        0.10644957, 0.00692548, 0.08917424, 0.11538746, 0.12283433,\n",
       "        0.11320292, 0.10416625, 0.11993647, 0.1213967 , 0.10104368,\n",
       "        0.12303858, 0.12124002, 0.11332835, 0.07571547, 0.10435302,\n",
       "        0.10351288, 0.07292675, 0.07927319, 0.11863647, 0.1021805 ,\n",
       "        0.11449143, 0.10298864, 0.09976125, 0.08309383, 0.11349738,\n",
       "        0.14877813, 0.1136057 , 0.12281752, 0.11609886, 0.13514955,\n",
       "        0.11570833, 0.08622573, 0.11264066, 0.12612022, 0.07570446,\n",
       "        0.10454834, 0.13105235, 0.10575817, 0.12646793, 0.12824665,\n",
       "        0.10861065, 0.11831613, 0.08457127, 0.11509771, 0.12646744,\n",
       "        0.1290208 , 0.12761682, 0.12934518, 0.11645727, 0.05941165,\n",
       "        0.10923104, 0.11007964, 0.13513717, 0.08876502, 0.09347118,\n",
       "        0.11693698, 0.13116188, 0.11477933, 0.11252918, 0.09639633,\n",
       "        0.09641928, 0.10025136, 0.1527836 , 0.12136158, 0.07313782,\n",
       "        0.08507912, 0.12011243, 0.08940189, 0.12223292, 0.09441712,\n",
       "        0.10879197, 0.09429722, 0.10601326, 0.13371095, 0.05793078,\n",
       "        0.12483196, 0.10834178, 0.11471914, 0.07323503, 0.12618478,\n",
       "        0.12519671, 0.12119641, 0.08675015, 0.0780154 , 0.14359306,\n",
       "        0.13159929, 0.06366348, 0.10693403, 0.08449886, 0.10776242,\n",
       "        0.08647957, 0.06357001, 0.11814182, 0.09785198, 0.12770677,\n",
       "        0.11113247, 0.1713708 , 0.04089267, 0.12826982, 0.10935371,\n",
       "        0.13243437, 0.22517309, 0.13557608, 0.1251202 , 0.12182497,\n",
       "        0.15499218, 0.07435373, 0.11003345, 0.10298579, 0.09446207,\n",
       "        0.11193663, 0.06347675, 0.11407179, 0.10771561], dtype=float32),\n",
       " array([0.02838034, 0.16080593, 0.14823511, 0.22593245, 0.18710423,\n",
       "        0.19186386, 0.16538908, 0.12673666, 0.16860041, 0.13399659,\n",
       "        0.20641756, 0.21187994, 0.17227396, 0.13759287, 0.19818157,\n",
       "        0.12801322, 0.12054053, 0.16097407, 0.18886141, 0.18513428,\n",
       "        0.17556019, 0.15817156, 0.10921562, 0.19279398, 0.10604001,\n",
       "        0.12975828, 0.13010512, 0.14991575, 0.20599592, 0.24550648,\n",
       "        0.12383195, 0.19714208, 0.14906429, 0.17875917, 0.20515579,\n",
       "        0.17186102, 0.11145432, 0.15447007, 0.1733589 , 0.1832928 ,\n",
       "        0.13398975, 0.11233476, 0.15706255, 0.15663804, 0.22838113,\n",
       "        0.13338666, 0.13668254, 0.0978886 , 0.12668402, 0.16869806,\n",
       "        0.14054395, 0.17517705, 0.20993468, 0.13818897, 0.13731533,\n",
       "        0.23571244, 0.17401403, 0.20008704, 0.09268095, 0.17514047,\n",
       "        0.16194229, 0.18599796, 0.13092992, 0.13095579, 0.16155966,\n",
       "        0.17586112, 0.18206114, 0.15227732, 0.17195134, 0.12248319,\n",
       "        0.13734989, 0.15404396, 0.17540732, 0.09049197, 0.1394367 ,\n",
       "        0.1399208 , 0.11987387, 0.19063589, 0.11427765, 0.17012693,\n",
       "        0.1430342 , 0.14384583, 0.13642171, 0.14186126, 0.16789457,\n",
       "        0.18093468, 0.13156213, 0.17702222, 0.19209924, 0.08482883,\n",
       "        0.19585547, 0.20298786, 0.1275881 , 0.13973199, 0.12681723,\n",
       "        0.13810746, 0.18258677, 0.13908084, 0.14634342, 0.16961786,\n",
       "        0.10520998, 0.21467629, 0.13716395, 0.20755127, 0.17633048,\n",
       "        0.17563128, 0.17061248, 0.20191808, 0.16290845, 0.22336805,\n",
       "        0.14912789, 0.10903182, 0.13461602, 0.21779007, 0.17495313,\n",
       "        0.18063775, 0.16219889, 0.16838425, 0.14155331, 0.22998826,\n",
       "        0.12889093, 0.16292483, 0.20838514, 0.24539503, 0.11396743,\n",
       "        0.22599453, 0.14968191, 0.12034345, 0.16721492, 0.14179885,\n",
       "        0.17246372, 0.15437844, 0.12008535, 0.14823477, 0.12446963,\n",
       "        0.1835725 , 0.09805085, 0.24281602, 0.17446177, 0.22842161,\n",
       "        0.18840986, 0.13251024, 0.18386438, 0.27583545, 0.16106993,\n",
       "        0.11483769, 0.19615315, 0.19230644, 0.26085472, 0.01567813,\n",
       "        0.19768596, 0.1527364 , 0.20052193, 0.24810933, 0.21279985,\n",
       "        0.25392038, 0.04343387, 0.14588964, 0.17690746, 0.16262658,\n",
       "        0.13901141, 0.1860676 , 0.22050874, 0.12887013, 0.14144345,\n",
       "        0.128159  , 0.18096347, 0.14223534, 0.14956824, 0.1204538 ,\n",
       "        0.23101774, 0.13356309, 0.2647866 , 0.14736904, 0.16725941,\n",
       "        0.17520647, 0.17935479, 0.2275779 , 0.16219223, 0.1912736 ,\n",
       "        0.12083076, 0.16120772, 0.04807039, 0.16924264, 0.19514896,\n",
       "        0.12636474, 0.12300098, 0.14095856, 0.24730556, 0.11956426,\n",
       "        0.15132034, 0.18242028, 0.16040486, 0.14482552, 0.1497729 ,\n",
       "        0.21373866, 0.16542566, 0.13454488, 0.15193553, 0.15182386,\n",
       "        0.19369587, 0.19468679, 0.15510014, 0.15291566, 0.20310298,\n",
       "        0.14282614, 0.265083  , 0.20061496, 0.13538523, 0.1308974 ,\n",
       "        0.13397413, 0.15896724, 0.11837742, 0.14632209, 0.11816043,\n",
       "        0.10594489, 0.11890193, 0.15568519, 0.19570857, 0.13619764,\n",
       "        0.17939919, 0.22041753, 0.19118376, 0.17343208, 0.16700967,\n",
       "        0.21540846, 0.19700566, 0.1183761 , 0.16794388, 0.12528424,\n",
       "        0.13705167, 0.1781446 , 0.11863968, 0.11719614, 0.16122566,\n",
       "        0.23406267, 0.13349499, 0.2129574 , 0.17890055, 0.17830835,\n",
       "        0.185725  , 0.18887083, 0.14585751, 0.20623228, 0.19928509,\n",
       "        0.14800334, 0.19744359, 0.21796055, 0.14071321, 0.12236051,\n",
       "        0.14731862, 0.1480159 , 0.15236412, 0.09570296, 0.33869144,\n",
       "        0.1280826 , 0.20998704, 0.15879142, 0.14013414, 0.26952058,\n",
       "        0.21603148, 0.20989473, 0.14278172, 0.21810919, 0.12634483,\n",
       "        0.13335218, 0.14084639, 0.31756148, 0.23369098, 0.16277297,\n",
       "        0.12398239, 0.05351152, 0.2061348 , 0.13203958, 0.14874813,\n",
       "        0.15661933, 0.11016918, 0.16982974, 0.12102972, 0.1439691 ,\n",
       "        0.1468725 , 0.09362104, 0.13579297, 0.17476109, 0.12389761,\n",
       "        0.13584483, 0.16724023, 0.19452505, 0.19544354, 0.13741583,\n",
       "        0.138977  , 0.206022  , 0.17987917, 0.24580896, 0.166494  ,\n",
       "        0.16551793, 0.17370683, 0.14500587, 0.1577021 , 0.15517552,\n",
       "        0.14777185, 0.15711777, 0.18873681, 0.22026749, 0.1942313 ,\n",
       "        0.16816598, 0.13278536, 0.1356506 , 0.1809024 , 0.25679356,\n",
       "        0.18718241, 0.14504835, 0.16358775, 0.1223003 , 0.21886602,\n",
       "        0.10445425, 0.09753255, 0.16574536, 0.16370708, 0.22555928,\n",
       "        0.15235628, 0.17149298, 0.14611456, 0.13655192, 0.14994365,\n",
       "        0.09735678, 0.13257577, 0.11154908, 0.16924621, 0.19316266,\n",
       "        0.10709108, 0.22949913, 0.1872222 , 0.14644837, 0.14961036,\n",
       "        0.16090454, 0.17464043, 0.26446897, 0.12418714, 0.15420666,\n",
       "        0.14256337, 0.2648652 , 0.154677  , 0.16801898, 0.2865664 ,\n",
       "        0.12533696, 0.15670677, 0.17629585, 0.26295725, 0.23933902,\n",
       "        0.16920507, 0.11496852, 0.21305327, 0.21331207, 0.15397534,\n",
       "        0.12863198, 0.18786451, 0.13386507, 0.19175376, 0.13401754,\n",
       "        0.16374733, 0.2634573 , 0.20573115, 0.16198431, 0.15316848,\n",
       "        0.18623531, 0.12785366, 0.22628939, 0.21121898, 0.21832767,\n",
       "        0.18020357, 0.44346833, 0.15495041, 0.1204512 , 0.16390528,\n",
       "        0.20489407, 0.24772581, 0.11693307, 0.1474569 , 0.20978107,\n",
       "        0.17754914, 0.23835999, 0.13454443, 0.15825507], dtype=float32),\n",
       " array([0.25570384, 0.22170313, 0.41055506, 0.2666929 , 0.29277223,\n",
       "        0.23005642, 0.29488793, 0.25004268, 0.28472018, 0.2877564 ,\n",
       "        0.22672163, 0.2435612 , 0.20091146, 0.19027944, 0.21580143,\n",
       "        0.22522858, 0.19453222, 0.19742826, 0.33597007, 0.18387525,\n",
       "        0.20581545, 0.21768491, 0.2769149 , 0.29664716, 0.23420724,\n",
       "        0.25537115, 0.31952104, 0.21965128, 0.29674223, 0.2296646 ,\n",
       "        0.30551106, 0.3118313 , 0.3126301 , 0.21440437, 0.26783046,\n",
       "        0.3008888 , 0.27928132, 0.21480985, 0.24156888, 0.27127835,\n",
       "        0.29015613, 0.3047931 , 0.26287284, 0.25104204, 0.3056555 ,\n",
       "        0.24626762, 0.2984594 , 0.35701498, 0.24818929, 0.23284681,\n",
       "        0.32222426, 0.29458877, 0.25459203, 0.29207173, 0.23771879,\n",
       "        0.23055685, 0.34291515, 0.29046333, 0.28917605, 0.32493925,\n",
       "        0.41772926, 0.31242624, 0.18024139, 0.24467364], dtype=float32),\n",
       " array([0.13121764, 0.11726921, 0.15946774, 0.11025539, 0.14043039,\n",
       "        0.14806068, 0.14988391, 0.12256825, 0.13072878, 0.07834946,\n",
       "        0.04904595, 0.15815079, 0.13403304, 0.15835541, 0.16920096,\n",
       "        0.13837087, 0.14481941, 0.1158978 , 0.14624275, 0.15164311,\n",
       "        0.14505447, 0.14655648, 0.14911553, 0.16192827, 0.12331531,\n",
       "        0.11220171, 0.11292588, 0.14743319, 0.1616034 , 0.14744346,\n",
       "        0.11293201, 0.08549434, 0.13601245, 0.10431726, 0.1379936 ,\n",
       "        0.08191639, 0.14296088, 0.10912699, 0.15716851, 0.17720242,\n",
       "        0.15017182, 0.1374369 , 0.06244466, 0.1287864 , 0.15611537,\n",
       "        0.13848878, 0.13281922, 0.11640806, 0.1252938 , 0.15961061,\n",
       "        0.07214489, 0.12516767, 0.12607083, 0.11681439, 0.10957442,\n",
       "        0.09238724, 0.12476093, 0.15244916, 0.08941694, 0.07399163,\n",
       "        0.12286558, 0.13944769, 0.07584444, 0.13427554, 0.14132114,\n",
       "        0.11838095, 0.10262419, 0.10829456, 0.15823354, 0.07272616,\n",
       "        0.13878252, 0.19288592, 0.04998787, 0.12275123, 0.14763062,\n",
       "        0.12072668, 0.12971665, 0.15448117, 0.15151496, 0.14233962,\n",
       "        0.14859234, 0.18567479, 0.1581976 , 0.18997021, 0.14270633,\n",
       "        0.15987195, 0.18905301, 0.18193017, 0.15875714, 0.13621172,\n",
       "        0.13395567, 0.15936519, 0.14942154, 0.14272687, 0.18705055,\n",
       "        0.12383834, 0.15031746, 0.14489582, 0.14253931, 0.16700566,\n",
       "        0.13076743, 0.12932631, 0.12404405, 0.15466946, 0.12988974,\n",
       "        0.12039344, 0.1447389 , 0.1603495 , 0.13872242, 0.191224  ,\n",
       "        0.11659663, 0.13659228, 0.0658129 , 0.12853052, 0.1099921 ,\n",
       "        0.13512993, 0.17942104, 0.12984034, 0.08703771, 0.12981834,\n",
       "        0.14552253, 0.09980761, 0.12826407, 0.13102354, 0.11628016,\n",
       "        0.10756383, 0.1305855 , 0.1709539 , 0.14226103, 0.13347472,\n",
       "        0.12325989, 0.14889936, 0.14412229, 0.12327811, 0.12744208,\n",
       "        0.15248936, 0.12856351, 0.09762981, 0.14509964, 0.16775058,\n",
       "        0.14499822, 0.15551957, 0.15720727, 0.16443564, 0.15911254,\n",
       "        0.15690243, 0.12019128, 0.14399093, 0.11564375, 0.17805451,\n",
       "        0.12426628, 0.15535092, 0.08420754, 0.1399545 , 0.1164462 ,\n",
       "        0.149521  , 0.12884401, 0.10848694, 0.13507335, 0.06811089,\n",
       "        0.09513653, 0.09138522, 0.12861039, 0.09023874, 0.15565895,\n",
       "        0.07248319, 0.14629203, 0.12216605, 0.15918708, 0.15017115,\n",
       "        0.09532316, 0.10344471, 0.11467699, 0.1055613 , 0.17493385,\n",
       "        0.12080719, 0.12962148, 0.11131258, 0.17866914, 0.16692789,\n",
       "        0.15150528, 0.0893366 , 0.16041967, 0.12869312, 0.13818859,\n",
       "        0.15744859, 0.12349398, 0.13813484, 0.16732903, 0.12814513,\n",
       "        0.12162787, 0.13380985, 0.1388866 , 0.14840804, 0.14115715,\n",
       "        0.14175662, 0.16205364, 0.1284192 , 0.17164944, 0.07262439,\n",
       "        0.13308734, 0.1511765 , 0.14892843, 0.15725942, 0.21338792,\n",
       "        0.12728585, 0.15430649, 0.11727446, 0.08229187, 0.16731563,\n",
       "        0.15806296, 0.13704544, 0.13261217, 0.15914202, 0.13170904,\n",
       "        0.13398717, 0.17578802, 0.12662727, 0.09787544, 0.15380321,\n",
       "        0.13796645, 0.13977529, 0.12872769, 0.14457467, 0.08494298,\n",
       "        0.16633907, 0.17941566, 0.13367537, 0.15900256, 0.14021137,\n",
       "        0.15800253, 0.15467155, 0.06894358, 0.15445764, 0.12005999,\n",
       "        0.08368689, 0.14992227, 0.13391873, 0.07916296, 0.16951756,\n",
       "        0.13056292, 0.10985829, 0.14486775, 0.14767614, 0.17355014,\n",
       "        0.11027179, 0.12724811, 0.14259233, 0.14055835, 0.16162194,\n",
       "        0.10270602, 0.13419718, 0.1522265 , 0.12640616, 0.10124235,\n",
       "        0.15221117, 0.15468635, 0.14998671, 0.13121526, 0.06989848,\n",
       "        0.13753486, 0.11203136, 0.11192647, 0.1864654 , 0.13198642,\n",
       "        0.1445646 , 0.15620352, 0.14159317, 0.07610299, 0.16553925,\n",
       "        0.11539218, 0.15874259, 0.14918403, 0.09210069, 0.13617434,\n",
       "        0.14027263, 0.13059188, 0.14535709, 0.17893015, 0.15522489,\n",
       "        0.16220132, 0.14922991, 0.11688227, 0.12190132, 0.13825287,\n",
       "        0.13565707, 0.14209464, 0.11924424, 0.139792  , 0.08745988,\n",
       "        0.1545657 , 0.15310784, 0.17497215, 0.13972619, 0.14393385,\n",
       "        0.1305447 , 0.1276128 , 0.11727127, 0.13240558, 0.16048995,\n",
       "        0.13555881, 0.12284043, 0.13416524, 0.13633129, 0.15309875,\n",
       "        0.15065469, 0.17479737, 0.11792328, 0.15305479, 0.13316165,\n",
       "        0.19111386, 0.12685172, 0.10462718, 0.06229299, 0.14053977,\n",
       "        0.15107666, 0.14925578, 0.07607726, 0.06319237, 0.16358636,\n",
       "        0.16846442, 0.13725147, 0.15669973, 0.15417585, 0.12808116,\n",
       "        0.12548754, 0.14108175, 0.07225626, 0.12585852, 0.18320166,\n",
       "        0.16096984, 0.14206593, 0.15545674, 0.14208367, 0.12954663,\n",
       "        0.11397307, 0.08705941, 0.18385158, 0.09419686, 0.16062231,\n",
       "        0.14822376, 0.14152996, 0.06289803, 0.08293834, 0.14007296,\n",
       "        0.15223719, 0.13559099, 0.12960637, 0.13670723, 0.12971318,\n",
       "        0.11200059, 0.14575188, 0.11702394, 0.07113734, 0.1035088 ,\n",
       "        0.14751902, 0.14867888, 0.14946379, 0.14302778, 0.1530587 ,\n",
       "        0.13728838, 0.16247462, 0.1156744 , 0.13870917, 0.10865434,\n",
       "        0.14148754, 0.1453799 , 0.12470078, 0.11367544, 0.14123519,\n",
       "        0.13751972, 0.1673735 , 0.12014664, 0.14028224, 0.14649701,\n",
       "        0.1144914 , 0.08020552, 0.12630701, 0.12588438, 0.10550211,\n",
       "        0.15449736, 0.18548599, 0.0971158 , 0.16176637], dtype=float32),\n",
       " array([0.18137307, 0.23676595, 0.15972786, 0.18041694, 0.25785035,\n",
       "        0.19574116, 0.24837652, 0.22030981, 0.14309642, 0.24697179,\n",
       "        0.22364083, 0.17586319, 0.15908682, 0.21890327, 0.18285549,\n",
       "        0.19264819, 0.18567969, 0.19833891, 0.17259277, 0.14233895,\n",
       "        0.20078401, 0.17144836, 0.23136877, 0.20543787, 0.24262533,\n",
       "        0.44303852, 0.19928864, 0.1850201 , 0.27713916, 0.17089802,\n",
       "        0.22614978, 0.25473434, 0.17997524, 0.22414352, 0.16475949,\n",
       "        0.22632848, 0.19644769, 0.20615204, 0.21585153, 0.21151248,\n",
       "        0.22030824, 0.15860185, 0.32834157, 0.12519528, 0.24600893,\n",
       "        0.18456104, 0.18108244, 0.2136176 , 0.22366731, 0.135968  ,\n",
       "        0.22916679, 0.19918288, 0.15551525, 0.24596584, 0.21808062,\n",
       "        0.19658424, 0.23723713, 0.2779876 , 0.24722476, 0.22941531,\n",
       "        0.24576251, 0.13917288, 0.1640611 , 0.16153352, 0.23583212,\n",
       "        0.2504506 , 0.22351831, 0.22425258, 0.20916721, 0.23070477,\n",
       "        0.14334576, 0.20150127, 0.26138157, 0.23240183, 0.16685767,\n",
       "        0.1946935 , 0.18340805, 0.2091874 , 0.14626254, 0.18104713,\n",
       "        0.1961888 , 0.28632113, 0.22016425, 0.19901066, 0.17294948,\n",
       "        0.17072445, 0.24266896, 0.28708187, 0.22157617, 0.24423234,\n",
       "        0.23777442, 0.18709998, 0.18741283, 0.22546421, 0.23506372,\n",
       "        0.20170899, 0.15171352, 0.1656647 , 0.19276136, 0.18856089,\n",
       "        0.24363889, 0.2627937 , 0.19158432, 0.2671907 , 0.1509773 ,\n",
       "        0.2741834 , 0.17940953, 0.27383125, 0.19319536, 0.24596536,\n",
       "        0.16558678, 0.17445126, 0.45867807, 0.13719256, 0.4382323 ,\n",
       "        0.3182518 , 0.24309   , 0.13380206, 0.27973658, 0.13412908,\n",
       "        0.21493109, 0.19066091, 0.30211714, 0.22361508, 0.2224716 ,\n",
       "        0.20191132, 0.15985134, 0.16476499, 0.17339797, 0.16485931,\n",
       "        0.16193633, 0.17890944, 0.20669608, 0.15443514, 0.18405265,\n",
       "        0.23032042, 0.13293688, 0.16371024, 0.17920728, 0.21730071,\n",
       "        0.17815678, 0.19879028, 0.18906036, 0.29926223, 0.1849472 ,\n",
       "        0.14322983, 0.23436742, 0.16029291, 0.20319767, 0.23616824,\n",
       "        0.19346446, 0.15789296, 0.1763249 , 0.21475999, 0.18999414,\n",
       "        0.23017061, 0.24999215, 0.28490996, 0.13415174, 0.20245098,\n",
       "        0.18872608, 0.21308081, 0.1632718 , 0.23272008, 0.32757336,\n",
       "        0.32665607, 0.19450192, 0.23434363, 0.291127  , 0.24559869,\n",
       "        0.20914522, 0.25251308, 0.22506061, 0.21240743, 0.19329922,\n",
       "        0.15572388, 0.17929387, 0.22858654, 0.20269226, 0.20168528,\n",
       "        0.1903522 , 0.16878898, 0.23625548, 0.42936713, 0.27440518,\n",
       "        0.24792822, 0.15784751, 0.18520829, 0.17206837, 0.17195742,\n",
       "        0.20091219, 0.28128207, 0.14725961, 0.19694167, 0.16685748,\n",
       "        0.19108704, 0.1486993 , 0.17810959, 0.24313478, 0.19218826,\n",
       "        0.17715342, 0.21101171, 0.11947916, 0.25602248, 0.18542838,\n",
       "        0.16558255, 0.18581313, 0.2255555 , 0.27438864, 0.18112224,\n",
       "        0.189167  , 0.25394925, 0.14542143, 0.19575402, 0.19202289,\n",
       "        0.2078676 , 0.18038313, 0.14584112, 0.22442943, 0.20497681,\n",
       "        0.23444292, 0.20491911, 0.27189952, 0.16145438, 0.3249136 ,\n",
       "        0.16291822, 0.17939135, 0.14821659, 0.17127016, 0.22570805,\n",
       "        0.1917993 , 0.21594547, 0.20466483, 0.15238245, 0.18390356,\n",
       "        0.22935379, 0.18719597, 0.43469152, 0.29332733, 0.20636256,\n",
       "        0.18963951, 0.23306543, 0.17177698, 0.13027099, 0.1509873 ,\n",
       "        0.23406866, 0.16763397, 0.24185851, 0.1467085 , 0.19851935,\n",
       "        0.3474869 , 0.30436212, 0.23119998, 0.20451076, 0.16257301,\n",
       "        0.23128033, 0.1539979 , 0.12809572, 0.23875344, 0.32521287,\n",
       "        0.19810994, 0.176932  , 0.24727842, 0.19827276, 0.1260265 ,\n",
       "        0.13626525, 0.15063418, 0.15494019, 0.2708999 , 0.15948175,\n",
       "        0.15286103, 0.16675103, 0.18274331, 0.2840433 , 0.17890757,\n",
       "        0.17553341, 0.23074606, 0.35432366, 0.34243727, 0.1972675 ,\n",
       "        0.20579454, 0.25929514, 0.2113804 , 0.41888204, 0.23323105,\n",
       "        0.23617865, 0.18029793, 0.1340599 , 0.19561225, 0.21458748,\n",
       "        0.19723634, 0.1431801 , 0.19240156, 0.11785853, 0.15830861,\n",
       "        0.22458908, 0.30847934, 0.26385686, 0.14744209, 0.22785702,\n",
       "        0.22689168, 0.2362631 , 0.2002438 , 0.2609949 , 0.20182611,\n",
       "        0.1361764 , 0.16616254, 0.23630479, 0.2605807 , 0.18339807,\n",
       "        0.19219773, 0.3740571 , 0.30971298, 0.30013922, 0.24967997,\n",
       "        0.22146042, 0.14765379, 0.27540806, 0.26518685, 0.19650356,\n",
       "        0.26564354, 0.21123958, 0.2073872 , 0.27144948, 0.18170926,\n",
       "        0.19807671, 0.24654987, 0.26864257, 0.26994953, 0.24197456,\n",
       "        0.16789062, 0.17909081, 0.2382782 , 0.16634561, 0.20055315,\n",
       "        0.30616057, 0.3402772 , 0.22541995, 0.28824043, 0.19682726,\n",
       "        0.27715018, 0.19470574, 0.36553553, 0.2739101 , 0.15793462,\n",
       "        0.20366175, 0.21679378, 0.24854173, 0.22592461, 0.24746546,\n",
       "        0.2954901 , 0.27896097, 0.17058498, 0.27702048, 0.26664996,\n",
       "        0.2593072 , 0.18497215, 0.18220532, 0.19610314, 0.28105488,\n",
       "        0.1871146 , 0.22337027, 0.26531214, 0.1763213 , 0.19327976,\n",
       "        0.1961936 , 0.30456704, 0.1812997 , 0.23557115, 0.26983747,\n",
       "        0.16856581, 0.1830538 , 0.20619528, 0.19261241, 0.19336271,\n",
       "        0.14659259, 0.2801504 , 0.19244757, 0.17417884, 0.21223356,\n",
       "        0.20599493, 0.20766558, 0.2280529 , 0.16394332], dtype=float32),\n",
       " array([0.3616948 , 0.35953036, 0.3886143 , 0.5456486 , 0.36054274,\n",
       "        0.34867907, 0.4042117 , 0.36296552, 0.4408222 , 0.3190615 ,\n",
       "        0.40132403, 0.36804536, 0.3936079 , 0.42668423, 0.34804338,\n",
       "        0.33924305, 0.36700904, 0.39919233, 0.32992408, 0.3801531 ,\n",
       "        0.39850438, 0.2750577 , 0.45686597, 0.32725912, 0.35233667,\n",
       "        0.31494787, 0.42330974, 0.3461538 , 0.475711  , 0.39812142,\n",
       "        0.34148034, 0.41071102, 0.35839373, 0.40945622, 0.3746327 ,\n",
       "        0.3675133 , 0.35623822, 0.32862496, 0.3640025 , 0.32931948,\n",
       "        0.43881088, 0.3742566 , 0.28775007, 0.3893429 , 0.40348694,\n",
       "        0.37824407, 0.37186104, 0.36892623, 0.37905163, 0.3732443 ,\n",
       "        0.4247082 , 0.36822987, 0.47367087, 0.40066007, 0.30784315,\n",
       "        0.3642847 , 0.4028987 , 0.4449807 , 0.36164853, 0.32597083,\n",
       "        0.383115  , 0.36934105, 0.4437508 , 0.35073417, 0.47913706,\n",
       "        0.45553946, 0.34196723, 0.3303683 , 0.3829291 , 0.34327933,\n",
       "        0.33363858, 0.3292553 , 0.39838222, 0.37563154, 0.37864837,\n",
       "        0.3988502 , 0.34428567, 0.36202845, 0.43840578, 0.41024867,\n",
       "        0.3488779 , 0.3448823 , 0.3085458 , 0.3725853 , 0.37160435,\n",
       "        0.34070858, 0.319365  , 0.38867784, 0.3506616 , 0.40176433,\n",
       "        0.3361432 , 0.39626276, 0.34317535, 0.3646078 , 0.31191942,\n",
       "        0.3253632 ], dtype=float32),\n",
       " array([0.10739286, 0.17571077, 0.10454547, 0.12215623, 0.10092776,\n",
       "        0.11954256, 0.1580463 , 0.1390697 , 0.11392447, 0.11465222,\n",
       "        0.10273444, 0.07369331, 0.11597236, 0.11955302, 0.14841275,\n",
       "        0.10748836, 0.11490478, 0.13938518, 0.11103676, 0.11962119,\n",
       "        0.09715471, 0.12358478, 0.11373752, 0.10499892, 0.10449261,\n",
       "        0.11453097, 0.13954759, 0.09696826, 0.17607065, 0.08429832,\n",
       "        0.0843715 , 0.1506816 , 0.13463552, 0.08922319, 0.06329702,\n",
       "        0.1079725 , 0.08681112, 0.10386609, 0.14239727, 0.1443098 ,\n",
       "        0.10010764, 0.12107991, 0.1723271 , 0.08921903, 0.14490475,\n",
       "        0.10472615, 0.10788623, 0.17317693, 0.13524203, 0.07252898,\n",
       "        0.12299898, 0.12620927, 0.1458871 , 0.1348105 , 0.1275015 ,\n",
       "        0.15147148, 0.11099613, 0.12094396, 0.09986769, 0.11057837,\n",
       "        0.09908056, 0.13657235, 0.12351175, 0.10089109, 0.10107465,\n",
       "        0.13227566, 0.13200746, 0.08799423, 0.11613219, 0.11976188,\n",
       "        0.11963626, 0.14648102, 0.09757417, 0.10598206, 0.1263014 ,\n",
       "        0.12870458, 0.11812425, 0.12228605, 0.0787208 , 0.10919727,\n",
       "        0.06898673, 0.11075264, 0.15391791, 0.14793071, 0.17922531,\n",
       "        0.13332778, 0.11406118, 0.11893541, 0.12067355, 0.00260029,\n",
       "        0.00269116, 0.15612002, 0.17330813, 0.10770429, 0.11769743,\n",
       "        0.13797008, 0.12554795, 0.08285893, 0.11780072, 0.08445713,\n",
       "        0.14154935, 0.14092633, 0.11144823, 0.18341215, 0.09276842,\n",
       "        0.11537455, 0.1176433 , 0.11936974, 0.0949754 , 0.13664895,\n",
       "        0.09312234, 0.14096394, 0.14541642, 0.08796661, 0.1245381 ,\n",
       "        0.10231897, 0.11202821, 0.00358126, 0.10172166, 0.13631174,\n",
       "        0.12529486, 0.08938061, 0.11967717, 0.11003953, 0.07864132,\n",
       "        0.09055105, 0.09096766, 0.08929054, 0.12197501, 0.12330768,\n",
       "        0.0976287 , 0.1900132 , 0.09372647, 0.13719508, 0.09208997,\n",
       "        0.15400372, 0.11711285, 0.11281697, 0.06940685, 0.11183361,\n",
       "        0.12411304, 0.14050713, 0.11913063, 0.10347272, 0.16256057,\n",
       "        0.11316895, 0.11879259, 0.15619867, 0.1255749 , 0.1085304 ,\n",
       "        0.12107191, 0.08969335, 0.1209686 , 0.14460532, 0.1308288 ,\n",
       "        0.11505729, 0.1049034 , 0.10940135, 0.16780782, 0.12447774,\n",
       "        0.13600081, 0.10641128, 0.14500546, 0.00294621, 0.12818491,\n",
       "        0.11877015, 0.13322926, 0.13113172, 0.08961981, 0.12825939,\n",
       "        0.1504843 , 0.08675377, 0.12161988, 0.07574605, 0.10955709,\n",
       "        0.14526679, 0.09963235, 0.10651616, 0.15998964, 0.09441109,\n",
       "        0.09481481, 0.09693471, 0.12620702, 0.12537985, 0.14941584,\n",
       "        0.1200043 , 0.1401349 , 0.07949924, 0.13329603, 0.14048825,\n",
       "        0.1052387 , 0.13843514, 0.16081797, 0.13935085, 0.09377197,\n",
       "        0.12800442, 0.12662818, 0.14773144, 0.12986672, 0.12684052,\n",
       "        0.09122263, 0.11216392, 0.1171648 , 0.15798984, 0.15599512,\n",
       "        0.09371648, 0.10539478, 0.14451143, 0.08333447, 0.1359104 ,\n",
       "        0.12245087, 0.10840198, 0.14109153, 0.10890628, 0.14872886,\n",
       "        0.13888687, 0.13938317, 0.10474414, 0.17386125, 0.10849507,\n",
       "        0.1067485 , 0.15620203, 0.11071569, 0.08252484, 0.16073003,\n",
       "        0.09311681, 0.13414752, 0.1318096 , 0.10343111, 0.10832147,\n",
       "        0.09567942, 0.1332698 , 0.00280078, 0.0729103 , 0.16054107,\n",
       "        0.00230582, 0.13784297, 0.1110371 , 0.08907561, 0.10800271,\n",
       "        0.10453435, 0.13586353, 0.14688009, 0.1362288 , 0.11323396,\n",
       "        0.10657077, 0.1457891 , 0.14993715, 0.15305004, 0.1221444 ,\n",
       "        0.18522893, 0.14245483, 0.07362466, 0.1858932 , 0.10365088,\n",
       "        0.13300112, 0.11544384, 0.08751657, 0.11202583, 0.13194722,\n",
       "        0.09902516, 0.10588564, 0.14148429, 0.07627947, 0.09010971,\n",
       "        0.12981594, 0.13919966, 0.1106927 , 0.10554493, 0.15463577,\n",
       "        0.15128228, 0.10646304, 0.19099113, 0.10812518, 0.12800765,\n",
       "        0.14276662, 0.14281355, 0.1204664 , 0.09427603, 0.10994846,\n",
       "        0.10452791, 0.12060063, 0.08179988, 0.13459787, 0.14019145,\n",
       "        0.07844832, 0.10620826, 0.06734821, 0.1518748 , 0.10740622,\n",
       "        0.12174285, 0.13931197, 0.13209686, 0.11381435, 0.13346124,\n",
       "        0.09655508, 0.07405569, 0.12625945, 0.05672687, 0.10256882,\n",
       "        0.16107789, 0.08542172, 0.12246326, 0.10320879, 0.12564723,\n",
       "        0.10893444, 0.11202931, 0.13654934, 0.07934599, 0.12367786,\n",
       "        0.13925156, 0.13261755, 0.05726219, 0.12645522, 0.13110451,\n",
       "        0.15187803, 0.1405002 , 0.11527708, 0.11683977, 0.12223535,\n",
       "        0.10499068, 0.13027722, 0.12694886, 0.13088445, 0.07838501,\n",
       "        0.12324825, 0.1284756 , 0.1106152 , 0.08575734, 0.13618432,\n",
       "        0.11234069, 0.12750685, 0.12509774, 0.12195399, 0.11097462,\n",
       "        0.15604754, 0.12215468, 0.0760512 , 0.12812822, 0.15430021,\n",
       "        0.09279725, 0.12060517, 0.11701301, 0.13421065, 0.11024648,\n",
       "        0.07135554, 0.15226397, 0.14245525, 0.11847121, 0.11951039,\n",
       "        0.13080107, 0.09197469, 0.1183707 , 0.12850286, 0.14371695,\n",
       "        0.18489012, 0.11066611, 0.09477335, 0.13226081, 0.10810034,\n",
       "        0.15008457, 0.12648547, 0.14732483, 0.08081876, 0.15298687,\n",
       "        0.14198823, 0.12028693, 0.14005366, 0.16755609, 0.00273954,\n",
       "        0.10027181, 0.08455037, 0.14644963, 0.10180411, 0.13234982,\n",
       "        0.1260276 , 0.12434359, 0.06179937, 0.14358298, 0.10886534,\n",
       "        0.1309231 , 0.10868216, 0.15962371, 0.13216834, 0.11284371,\n",
       "        0.11949734, 0.05499691, 0.12159566, 0.1348835 , 0.13875505,\n",
       "        0.16019005, 0.12172072, 0.12904137, 0.13831942, 0.127534  ,\n",
       "        0.11914209, 0.12239549, 0.13529083, 0.10618152, 0.14568463,\n",
       "        0.10247599, 0.12359756, 0.14215322, 0.07431921, 0.11243931,\n",
       "        0.10712621, 0.09019789, 0.13366382, 0.0998881 , 0.10953279,\n",
       "        0.10579484, 0.09054235, 0.1346826 , 0.08644962, 0.10868849,\n",
       "        0.16037233, 0.05550354, 0.1283975 , 0.11154944, 0.13361083,\n",
       "        0.1078661 , 0.10943251, 0.15168522, 0.11810789, 0.11946658,\n",
       "        0.12661405, 0.149581  , 0.12000576, 0.08791633, 0.11254378,\n",
       "        0.08077203, 0.16307244, 0.12245622, 0.1340083 , 0.12697543,\n",
       "        0.13147038, 0.1547503 , 0.09278318, 0.24678695, 0.11481999,\n",
       "        0.14285316, 0.18372281, 0.14090776, 0.13889101, 0.14469413,\n",
       "        0.1523931 , 0.13475129, 0.15217638, 0.0940204 , 0.15800099,\n",
       "        0.07961901, 0.13944592, 0.09319207, 0.09713214, 0.11507224,\n",
       "        0.1367022 , 0.14166139, 0.16432516, 0.13979582, 0.10325268,\n",
       "        0.13259578, 0.04700627, 0.07439327, 0.15271886, 0.05722015,\n",
       "        0.15310577, 0.07900111, 0.11331671, 0.14117669, 0.08438412,\n",
       "        0.12295241, 0.11374719, 0.13805696, 0.12836863, 0.140539  ,\n",
       "        0.1500486 , 0.09959427, 0.15205944, 0.15959811, 0.11218505,\n",
       "        0.09890865, 0.09271412, 0.11401691, 0.14753354, 0.16367985,\n",
       "        0.16022325, 0.15470879, 0.12658136, 0.17575674, 0.09363031,\n",
       "        0.08011879, 0.10085209, 0.14582165, 0.10961854, 0.15949503,\n",
       "        0.09763084, 0.14553079, 0.14774884, 0.08664971, 0.14768769,\n",
       "        0.14662251, 0.14385615, 0.14073151, 0.10051085, 0.0593657 ,\n",
       "        0.10438249, 0.08548281, 0.13825932, 0.12840037, 0.08418065,\n",
       "        0.14605094, 0.15721622, 0.10333844, 0.10813402, 0.10918201,\n",
       "        0.11350765, 0.12689824, 0.11476179, 0.11511667, 0.13963278,\n",
       "        0.16474372, 0.11194026, 0.14163582, 0.13108541, 0.12243232,\n",
       "        0.17519955, 0.06970028, 0.15254842, 0.14055224, 0.08827979,\n",
       "        0.11860424, 0.10550608, 0.09529913, 0.1467977 , 0.19550914,\n",
       "        0.07193186, 0.12947862, 0.10435044, 0.08458362, 0.09865298,\n",
       "        0.14908928, 0.07661188, 0.12982887, 0.16363728, 0.1567833 ,\n",
       "        0.11324124, 0.13018893, 0.07102516, 0.14773153, 0.12741518,\n",
       "        0.07946613, 0.11422437, 0.13682796, 0.1071403 , 0.07410653,\n",
       "        0.10074874, 0.1279255 , 0.1712124 , 0.12501529, 0.12413548,\n",
       "        0.11891144, 0.13965763, 0.13085109, 0.13436544, 0.16071585,\n",
       "        0.09104241, 0.12704352, 0.11480768, 0.11993991, 0.10855176,\n",
       "        0.14238094, 0.16390489, 0.16191503, 0.1412259 , 0.13431247,\n",
       "        0.12297449], dtype=float32),\n",
       " array([0.24027023, 0.22315042, 0.19769186, 0.17743252, 0.15874778,\n",
       "        0.15869695, 0.11243793, 0.1523524 , 0.21587504, 0.184437  ,\n",
       "        0.14956725, 0.2159057 , 0.29708654, 0.23178662, 0.18693715,\n",
       "        0.1928    , 0.19407779, 0.09903392, 0.16291946, 0.10575393,\n",
       "        0.23386335, 0.20967524, 0.22343686, 0.10530937, 0.16669658,\n",
       "        0.20429604, 0.15330996, 0.18441439, 0.15577094, 0.17045036,\n",
       "        0.21029432, 0.11279018, 0.16341986, 0.319667  , 0.23112625,\n",
       "        0.15893061, 0.19492695, 0.1577213 , 0.17687598, 0.16068041,\n",
       "        0.1732794 , 0.13331436, 0.16501574, 0.14749669, 0.17384404,\n",
       "        0.1279023 , 0.1759438 , 0.24483815, 0.3131351 , 0.20692049,\n",
       "        0.17794555, 0.16950512, 0.13251595, 0.20271838, 0.22341059,\n",
       "        0.1887769 , 0.18443859, 0.15802315, 0.22043955, 0.13963637,\n",
       "        0.20541799, 0.18615949, 0.17508109, 0.21785928, 0.15437171,\n",
       "        0.17920415, 0.22593312, 0.23750186, 0.1983494 , 0.19220342,\n",
       "        0.12625405, 0.22939909, 0.22770458, 0.18374458, 0.11874869,\n",
       "        0.13380112, 0.17743114, 0.15759504, 0.2550198 , 0.13031198,\n",
       "        0.20758642, 0.19485818, 0.1202649 , 0.15948364, 0.19870678,\n",
       "        0.16632463, 0.2046528 , 0.13713974, 0.17882511, 0.01782592,\n",
       "        0.02836283, 0.12717384, 0.21219642, 0.28405327, 0.18546465,\n",
       "        0.20650719, 0.19818735, 0.2048215 , 0.15618919, 0.21053547,\n",
       "        0.20507888, 0.14629771, 0.18557839, 0.12428686, 0.20808356,\n",
       "        0.16297553, 0.18933633, 0.16300805, 0.24562094, 0.20565319,\n",
       "        0.25037423, 0.21908815, 0.1248769 , 0.1805201 , 0.2530196 ,\n",
       "        0.13956659, 0.17288376, 0.02186484, 0.21489437, 0.26759756,\n",
       "        0.1842297 , 0.31772858, 0.15313703, 0.1964568 , 0.18708311,\n",
       "        0.20757085, 0.19134596, 0.21575414, 0.17381299, 0.1958561 ,\n",
       "        0.18749699, 0.32208446, 0.22295423, 0.24989401, 0.12131948,\n",
       "        0.16599232, 0.14316218, 0.15647258, 0.18175961, 0.20321508,\n",
       "        0.1834989 , 0.1270423 , 0.15422863, 0.21689485, 0.21035013,\n",
       "        0.1740848 , 0.18868223, 0.24984385, 0.17910586, 0.1274362 ,\n",
       "        0.21049756, 0.26240444, 0.17414877, 0.17606217, 0.14189333,\n",
       "        0.22983707, 0.19966325, 0.1831926 , 0.17558108, 0.1916331 ,\n",
       "        0.18404618, 0.2413729 , 0.18038426, 0.01599129, 0.19917902,\n",
       "        0.12891869, 0.19379994, 0.19225328, 0.15968016, 0.1263004 ,\n",
       "        0.12765206, 0.21698427, 0.15310442, 0.22584502, 0.22098114,\n",
       "        0.15918456, 0.15271537, 0.23200326, 0.11936757, 0.18001297,\n",
       "        0.17925833, 0.18073909, 0.15123548, 0.3218644 , 0.0927706 ,\n",
       "        0.11188079, 0.24606262, 0.18808407, 0.1839946 , 0.22188312,\n",
       "        0.23153292, 0.22502366, 0.18166411, 0.18118747, 0.1798804 ,\n",
       "        0.16150416, 0.18411191, 0.1564252 , 0.14743349, 0.11922301,\n",
       "        0.20213266, 0.19946256, 0.16548237, 0.19505462, 0.13674317,\n",
       "        0.1434822 , 0.22370234, 0.19089463, 0.18986712, 0.1573098 ,\n",
       "        0.18797238, 0.17010176, 0.10690509, 0.1733086 , 0.21398951,\n",
       "        0.18703106, 0.16652952, 0.1664699 , 0.21514903, 0.17014289,\n",
       "        0.1635596 , 0.14241482, 0.2736133 , 0.28820133, 0.2143023 ,\n",
       "        0.21288614, 0.22805975, 0.1332452 , 0.16078204, 0.16468363,\n",
       "        0.14502679, 0.21692123, 0.01719176, 0.21693428, 0.11535209,\n",
       "        0.01261344, 0.16983959, 0.17444202, 0.14485341, 0.19883265,\n",
       "        0.16132057, 0.23083873, 0.13278246, 0.1678522 , 0.18576868,\n",
       "        0.1301959 , 0.16285384, 0.17158489, 0.1930165 , 0.17981942,\n",
       "        0.20893127, 0.19866797, 0.21562375, 0.18207958, 0.1902747 ,\n",
       "        0.1529519 , 0.10233209, 0.18734601, 0.14948407, 0.1332845 ,\n",
       "        0.21669088, 0.17333831, 0.21364559, 0.18671308, 0.24399579,\n",
       "        0.13737752, 0.16914268, 0.19396144, 0.23469312, 0.17647657,\n",
       "        0.13424091, 0.15392022, 0.25240332, 0.13674891, 0.109992  ,\n",
       "        0.23072794, 0.17295924, 0.16287932, 0.20545064, 0.16730954,\n",
       "        0.22267632, 0.18122278, 0.1981728 , 0.20278633, 0.16964468,\n",
       "        0.22524367, 0.19778627, 0.23629114, 0.21718818, 0.1903168 ,\n",
       "        0.18412949, 0.14335874, 0.13505477, 0.14792265, 0.10944628,\n",
       "        0.15874237, 0.21261473, 0.19815493, 0.22085428, 0.13869877,\n",
       "        0.2020666 , 0.27661645, 0.23187056, 0.15718937, 0.22493085,\n",
       "        0.17051883, 0.22000362, 0.23174106, 0.16424215, 0.17200921,\n",
       "        0.20686135, 0.13415536, 0.21395817, 0.22782367, 0.21693785,\n",
       "        0.1291922 , 0.19167638, 0.14783153, 0.30356368, 0.1328787 ,\n",
       "        0.19845238, 0.14444157, 0.18048237, 0.25731567, 0.16560751,\n",
       "        0.18384536, 0.14114495, 0.15424873, 0.24168165, 0.15000162,\n",
       "        0.16584966, 0.18843311, 0.19912164, 0.14840192, 0.17196806,\n",
       "        0.1598765 , 0.15137392, 0.23307088, 0.1388022 , 0.23645045,\n",
       "        0.21009688, 0.18567257, 0.12334323, 0.16576937, 0.1149499 ,\n",
       "        0.21385051, 0.18991943, 0.17801376, 0.20440125, 0.20348947,\n",
       "        0.23473652, 0.17781632, 0.17974755, 0.12654582, 0.18930171,\n",
       "        0.22144611, 0.19118795, 0.19373739, 0.19355704, 0.20268223,\n",
       "        0.13220437, 0.19880338, 0.13100235, 0.23258802, 0.11405635,\n",
       "        0.22233845, 0.21052514, 0.22156754, 0.16656502, 0.02337081,\n",
       "        0.21704236, 0.19415905, 0.15395764, 0.19363366, 0.2690489 ,\n",
       "        0.14268912, 0.18190566, 0.28788546, 0.2275163 , 0.24362476,\n",
       "        0.19654998, 0.12569053, 0.22953933, 0.18992907, 0.14939769,\n",
       "        0.18475735, 0.26941106, 0.1725786 , 0.13224494, 0.14419737,\n",
       "        0.22991261, 0.14992537, 0.12112669, 0.21549168, 0.17708553,\n",
       "        0.30674863, 0.16315086, 0.18099661, 0.13676775, 0.19627558,\n",
       "        0.17976338, 0.17767024, 0.1163372 , 0.19239803, 0.14648944,\n",
       "        0.16293997, 0.21128844, 0.18369812, 0.20290974, 0.25066882,\n",
       "        0.1960896 , 0.20605353, 0.21272965, 0.2585914 , 0.21381487,\n",
       "        0.26219222, 0.23776242, 0.16924955, 0.19767012, 0.12911902,\n",
       "        0.23844102, 0.18136866, 0.24996702, 0.14512227, 0.22175828,\n",
       "        0.18431799, 0.13198774, 0.14175878, 0.26243752, 0.17653102,\n",
       "        0.16402969, 0.1615942 , 0.18655668, 0.1683423 , 0.19891888,\n",
       "        0.19303952, 0.11663368, 0.13953426, 0.35356143, 0.190779  ,\n",
       "        0.14303017, 0.20552492, 0.16998596, 0.16629282, 0.17737298,\n",
       "        0.10947146, 0.12451447, 0.2245763 , 0.18365678, 0.13165344,\n",
       "        0.2120293 , 0.20807691, 0.1980823 , 0.34953746, 0.23848563,\n",
       "        0.2513381 , 0.18313462, 0.16182393, 0.12559527, 0.14734168,\n",
       "        0.17337917, 0.24124207, 0.21391405, 0.18911837, 0.31279677,\n",
       "        0.13180955, 0.19487984, 0.15415505, 0.15398909, 0.22072944,\n",
       "        0.23947863, 0.1906388 , 0.13813938, 0.10735182, 0.15523201,\n",
       "        0.13896646, 0.27540895, 0.10571508, 0.2523386 , 0.19407679,\n",
       "        0.16518603, 0.28890675, 0.17838475, 0.14386083, 0.11957353,\n",
       "        0.20863076, 0.12265801, 0.1830429 , 0.1814205 , 0.18657735,\n",
       "        0.23526445, 0.17219052, 0.16551234, 0.19863574, 0.14612189,\n",
       "        0.21441934, 0.11229459, 0.17747176, 0.14085758, 0.19768377,\n",
       "        0.28110525, 0.15160121, 0.15700857, 0.18018705, 0.2706073 ,\n",
       "        0.1740782 , 0.24488582, 0.14990978, 0.13398488, 0.24319679,\n",
       "        0.19327277, 0.15460092, 0.16331093, 0.2035691 , 0.21194983,\n",
       "        0.25248614, 0.14849542, 0.1771467 , 0.18004829, 0.15192384,\n",
       "        0.24839519, 0.21230999, 0.23330364, 0.15106355, 0.18696648,\n",
       "        0.17402084, 0.21105586, 0.21508795, 0.2043082 , 0.20162532,\n",
       "        0.13383292, 0.20854285, 0.15508448, 0.11999706, 0.25549287,\n",
       "        0.18483923, 0.19038734, 0.23704572, 0.17230865, 0.1612132 ,\n",
       "        0.15543906, 0.24014688, 0.13508424, 0.16719069, 0.27805898,\n",
       "        0.11759356, 0.11618537, 0.28428492, 0.24960972, 0.20925127,\n",
       "        0.2345487 , 0.1598645 , 0.32444152, 0.17513265, 0.23023702,\n",
       "        0.20710614, 0.1596198 , 0.12593198, 0.20200846, 0.17072694,\n",
       "        0.12361579, 0.1795414 , 0.23558596, 0.17562969, 0.20653124,\n",
       "        0.11253572, 0.17992659, 0.17718884, 0.2065128 , 0.23991515,\n",
       "        0.10535979, 0.1517789 , 0.14764018, 0.17790419, 0.16083606,\n",
       "        0.10865209], dtype=float32),\n",
       " array([0.2877551 , 0.27927986, 0.2226135 , 0.1279309 , 0.26814988,\n",
       "        0.2549838 , 0.22699039, 0.25845852, 0.19068283, 0.31071502,\n",
       "        0.20474923, 0.28584987, 0.23299992, 0.17547272, 0.24244742,\n",
       "        0.2845723 , 0.30411547, 0.23099658, 0.26926807, 0.24612273,\n",
       "        0.2276788 , 0.2965213 , 0.21787114, 0.33680537, 0.2748502 ,\n",
       "        0.3009331 , 0.20038551, 0.23396339, 0.16477029, 0.21739465,\n",
       "        0.27257252, 0.19901213, 0.26532388, 0.21614046, 0.2515997 ,\n",
       "        0.23445395, 0.22539558, 0.26982763, 0.26219597, 0.2306504 ,\n",
       "        0.17044686, 0.26479656, 0.3385484 , 0.22368331, 0.21319333,\n",
       "        0.20481728, 0.21787474, 0.26205522, 0.23772658, 0.286085  ,\n",
       "        0.19783095, 0.22068183, 0.18222737, 0.2543659 , 0.29460436,\n",
       "        0.287598  , 0.21257906, 0.17794439, 0.26261485, 0.28324547,\n",
       "        0.24592187, 0.28506944, 0.19973023, 0.22902751, 0.17338592,\n",
       "        0.20486808, 0.2702103 , 0.28946888, 0.2208608 , 0.2586498 ,\n",
       "        0.28920308, 0.33337602, 0.26365608, 0.2526367 , 0.25795734,\n",
       "        0.22079167, 0.26873046, 0.26601732, 0.20712222, 0.18035597,\n",
       "        0.2858892 , 0.27580965, 0.32635212, 0.30165023, 0.3131256 ,\n",
       "        0.24947906, 0.2831601 , 0.27312937, 0.2487692 , 0.23558202,\n",
       "        0.27079123, 0.21710423, 0.24913074, 0.26365837, 0.2845353 ,\n",
       "        0.3315692 ], dtype=float32),\n",
       " array([0.09022041, 0.12691249, 0.14847942, 0.13094321, 0.14485174,\n",
       "        0.10426312, 0.07091635, 0.14663248, 0.07464299, 0.11329636,\n",
       "        0.13653892, 0.14220946, 0.13526447, 0.07766536, 0.13067426,\n",
       "        0.1130028 , 0.15330693, 0.10069185, 0.14751744, 0.15269096,\n",
       "        0.14600506, 0.10372669, 0.10044812, 0.12607345, 0.12062582,\n",
       "        0.00174803, 0.12521471, 0.14596638, 0.11934657, 0.07974251,\n",
       "        0.12405567, 0.12378854, 0.11016372, 0.10903677, 0.14898086,\n",
       "        0.09866939, 0.10404753, 0.10713269, 0.0922007 , 0.1285001 ,\n",
       "        0.09229298, 0.09502839, 0.09948713, 0.13591148, 0.12322117,\n",
       "        0.1411356 , 0.13824502, 0.0914502 , 0.12953632, 0.12014734,\n",
       "        0.11304291, 0.14343785, 0.12748222, 0.12501915, 0.11834852,\n",
       "        0.11669499, 0.08758184, 0.16629833, 0.15938671, 0.09235517,\n",
       "        0.13714138, 0.11970244, 0.09611309, 0.10814639, 0.13548508,\n",
       "        0.16009834, 0.11457311, 0.11726259, 0.11104004, 0.10515296,\n",
       "        0.13227713, 0.11730502, 0.0929433 , 0.11025371, 0.13657473,\n",
       "        0.06951498, 0.1463061 , 0.08638554, 0.11169837, 0.12462304,\n",
       "        0.12396711, 0.07968047, 0.11574958, 0.09009343, 0.10917271,\n",
       "        0.14294304, 0.12299722, 0.1426022 , 0.14596596, 0.12159759,\n",
       "        0.1187093 , 0.12085706, 0.1015511 , 0.12512542, 0.12491821,\n",
       "        0.08404488, 0.08875648, 0.13272862, 0.14838183, 0.1557182 ,\n",
       "        0.11700501, 0.13050295, 0.13236237, 0.16489989, 0.09736945,\n",
       "        0.15578756, 0.08990342, 0.12491558, 0.10666027, 0.10198353,\n",
       "        0.11268   , 0.16452055, 0.10560467, 0.20912552, 0.12017142,\n",
       "        0.12375204, 0.10611943, 0.16412088, 0.13613573, 0.14065397,\n",
       "        0.09540278, 0.11619007, 0.00384616, 0.13241842, 0.20604256,\n",
       "        0.11362562, 0.09921513, 0.14472277, 0.10025385, 0.12152804,\n",
       "        0.08834974, 0.11695608, 0.00357379, 0.09354465, 0.11010601,\n",
       "        0.11038785, 0.12360215, 0.09717754, 0.10704306, 0.11704842,\n",
       "        0.1277543 , 0.09127979, 0.12609994, 0.1387123 , 0.10566904,\n",
       "        0.13599136, 0.11821792, 0.04839762, 0.09167308, 0.10781318,\n",
       "        0.13054669, 0.06219657, 0.10081833, 0.13007781, 0.1237348 ,\n",
       "        0.14286284, 0.13853115, 0.10414307, 0.08834817, 0.12662797,\n",
       "        0.16481924, 0.10939645, 0.12133842, 0.1281755 , 0.04832264,\n",
       "        0.15065323, 0.12383118, 0.11273327, 0.08546039, 0.116851  ,\n",
       "        0.07665031, 0.14358182, 0.1656182 , 0.11084862, 0.1480585 ,\n",
       "        0.10694315, 0.15308233, 0.14299548, 0.14949436, 0.14191891,\n",
       "        0.14206193, 0.10636035, 0.12932684, 0.1507129 , 0.15671954,\n",
       "        0.09205065, 0.11643203, 0.09953079, 0.0879776 , 0.14286691,\n",
       "        0.08874758, 0.11712094, 0.11241038, 0.12164198, 0.14127664,\n",
       "        0.11159723, 0.15638907, 0.12570687, 0.11799791, 0.06227918,\n",
       "        0.12403947, 0.10820392, 0.15095091, 0.3807538 , 0.09654456,\n",
       "        0.11383181, 0.12601194, 0.14390853, 0.10057293, 0.10291853,\n",
       "        0.11329415, 0.15267083, 0.14256538, 0.1378499 , 0.14619768,\n",
       "        0.14562713, 0.13646513, 0.115589  , 0.10114114, 0.1462141 ,\n",
       "        0.15283057, 0.1113403 , 0.14858004, 0.1117233 , 0.15369956,\n",
       "        0.07211377, 0.12986664, 0.15155588, 0.1247206 , 0.12414696,\n",
       "        0.13715528, 0.07578269, 0.13749556, 0.15605234, 0.1126078 ,\n",
       "        0.13012095, 0.11174398, 0.08558591, 0.12138107, 0.12637211,\n",
       "        0.12653124, 0.12132994, 0.14174816, 0.12164507, 0.18691905,\n",
       "        0.11449636, 0.11031491, 0.10315853, 0.16025083, 0.13998462,\n",
       "        0.13310145, 0.09136017, 0.10314833, 0.12545425, 0.11839587,\n",
       "        0.12529054, 0.10120878, 0.12408219, 0.12726907, 0.08769018,\n",
       "        0.11856533, 0.09656579, 0.10392439, 0.13576214, 0.08153389,\n",
       "        0.13494429, 0.14806706, 0.09877482, 0.13623732, 0.12099738,\n",
       "        0.14430758, 0.11760611, 0.09783572, 0.13301665, 0.14678398,\n",
       "        0.12056302, 0.11108027, 0.10988279, 0.14304864, 0.09089447,\n",
       "        0.00181656, 0.0038898 , 0.17642444, 0.12121904, 0.12693924,\n",
       "        0.10189366, 0.186808  , 0.08716299, 0.16019535, 0.11658004,\n",
       "        0.16912259, 0.1078217 , 0.15466918, 0.12162786, 0.10745893,\n",
       "        0.12005116, 0.11569294, 0.1004359 , 0.13266349, 0.11256565,\n",
       "        0.1222624 , 0.12639807, 0.12342624, 0.13673247, 0.12667644,\n",
       "        0.11558525, 0.14256682, 0.13029063, 0.08296469, 0.12605378,\n",
       "        0.11523584, 0.14116326, 0.11698692, 0.10394949, 0.14233795,\n",
       "        0.10382222, 0.16296864, 0.10109762, 0.1030158 , 0.14328104,\n",
       "        0.0980382 , 0.1228315 , 0.12072951, 0.14365175, 0.1203584 ,\n",
       "        0.11610341, 0.12334004, 0.15603144, 0.13586496, 0.11635318,\n",
       "        0.14397009, 0.1776858 , 0.11096343, 0.11959915, 0.13729468,\n",
       "        0.14835791, 0.06516065, 0.16525467, 0.11522135, 0.14301123,\n",
       "        0.11845336, 0.1152378 , 0.12189578, 0.13036287, 0.14189504,\n",
       "        0.13621476, 0.10984464, 0.11761136, 0.1284756 , 0.14668746,\n",
       "        0.1403501 , 0.10973783, 0.11146194, 0.04997632, 0.12549698,\n",
       "        0.11704147, 0.10144948, 0.13121577, 0.16158156, 0.10667864,\n",
       "        0.08817472, 0.12242032, 0.11605126, 0.1450456 , 0.23371783,\n",
       "        0.11424621, 0.11839956, 0.12970908, 0.12755631, 0.10879828,\n",
       "        0.10291263, 0.11727218, 0.11846796, 0.1055328 , 0.14323106,\n",
       "        0.11104286, 0.12057441, 0.10769823, 0.14228784, 0.12911515,\n",
       "        0.10227472, 0.1468253 , 0.14015779, 0.11604985, 0.15586667,\n",
       "        0.12753992, 0.14034049, 0.11372527, 0.09315734, 0.14175627,\n",
       "        0.08374083, 0.12336887, 0.12224516, 0.15964134, 0.10585906,\n",
       "        0.16094396, 0.07897369, 0.12640736, 0.11311913, 0.10212879,\n",
       "        0.1326614 , 0.11755311, 0.06904976, 0.07549284, 0.08821294,\n",
       "        0.10730831, 0.13058323, 0.09866405, 0.10396098, 0.08876333,\n",
       "        0.11646388, 0.12705362, 0.07673457, 0.09501789, 0.15787698,\n",
       "        0.12755868, 0.07182354, 0.11983168, 0.1221521 , 0.1459006 ,\n",
       "        0.12223198, 0.11661871, 0.09840185, 0.14525442, 0.10990255,\n",
       "        0.12784632, 0.10732049, 0.10111122, 0.1048741 , 0.10295779,\n",
       "        0.16108902, 0.10631098, 0.10240456, 0.14983116, 0.10924064,\n",
       "        0.14345406, 0.15125771, 0.10422666, 0.13928048, 0.12483068,\n",
       "        0.13220069, 0.04789299, 0.11912435, 0.097014  , 0.14550787,\n",
       "        0.10562676, 0.11314948, 0.10586805, 0.11299209, 0.15387245,\n",
       "        0.16853341, 0.11736485, 0.12056399, 0.14976116, 0.10293151,\n",
       "        0.13168477, 0.13766253, 0.1418897 , 0.14406292, 0.13006274,\n",
       "        0.10195096, 0.14640547, 0.10910678, 0.12722357, 0.1206881 ,\n",
       "        0.06166265, 0.13562842, 0.13208452, 0.13108775, 0.10743088,\n",
       "        0.07760239, 0.16389611, 0.12527695, 0.13881241, 0.09031531,\n",
       "        0.1230521 , 0.12798424, 0.16894051, 0.10388298, 0.12689315,\n",
       "        0.11035977, 0.16844682, 0.12176853, 0.12639815, 0.17939344,\n",
       "        0.15745986, 0.1251269 , 0.11293349, 0.11155529, 0.15896817,\n",
       "        0.10043958, 0.16109255, 0.13304247, 0.12107778, 0.17624469,\n",
       "        0.10156991, 0.10759382, 0.10774072, 0.11442624, 0.10569617,\n",
       "        0.09111314, 0.15255007, 0.11886636, 0.30027413, 0.12146102,\n",
       "        0.08780784, 0.13496462, 0.0791429 , 0.17666978, 0.10787889,\n",
       "        0.07436466, 0.14564727, 0.12756166, 0.12007324, 0.14259812,\n",
       "        0.10776807, 0.14844698, 0.13545613, 0.12830293, 0.1374859 ,\n",
       "        0.17047787, 0.10903575, 0.10225798, 0.15023588, 0.09733851,\n",
       "        0.12161233, 0.14135331, 0.15559487, 0.09020203, 0.08783453,\n",
       "        0.11277135, 0.13052958, 0.13766126, 0.15022539, 0.00170159,\n",
       "        0.10291274, 0.13292666, 0.09593973, 0.08753343, 0.16127028,\n",
       "        0.12364415, 0.12963402, 0.16566956, 0.10766052, 0.00278888,\n",
       "        0.10406743, 0.15599985, 0.12213066, 0.09800033, 0.12227625,\n",
       "        0.12568288, 0.11313405, 0.13291356, 0.11224519, 0.14497481,\n",
       "        0.13596614, 0.1479319 , 0.15977748, 0.12469848, 0.14990377,\n",
       "        0.10864138, 0.12721533, 0.0897631 , 0.11993559, 0.13023172,\n",
       "        0.10778068, 0.12864031, 0.0928596 , 0.09610225, 0.14451122,\n",
       "        0.1082252 , 0.11770835, 0.14984915, 0.09459586, 0.14116067,\n",
       "        0.15094182], dtype=float32),\n",
       " array([0.26965162, 0.15605672, 0.1694403 , 0.13841137, 0.20558803,\n",
       "        0.18394081, 0.24288484, 0.20178469, 0.29087204, 0.16475852,\n",
       "        0.16800536, 0.24299912, 0.13097541, 0.20119111, 0.19567716,\n",
       "        0.16890845, 0.22194694, 0.24671988, 0.14763975, 0.15481898,\n",
       "        0.19354306, 0.2456598 , 0.23049948, 0.13303444, 0.14656596,\n",
       "        0.01253989, 0.13425101, 0.14778766, 0.17516083, 0.21549328,\n",
       "        0.2543901 , 0.20943743, 0.13475926, 0.21304485, 0.13340215,\n",
       "        0.195942  , 0.16242707, 0.16558814, 0.14972885, 0.1184068 ,\n",
       "        0.24562627, 0.19300407, 0.20383619, 0.1821466 , 0.19205974,\n",
       "        0.206374  , 0.18493445, 0.1918476 , 0.12418902, 0.13173215,\n",
       "        0.2816881 , 0.25600368, 0.13839774, 0.16288342, 0.12995669,\n",
       "        0.19899166, 0.2144318 , 0.20343801, 0.12309501, 0.20382103,\n",
       "        0.14392245, 0.21158782, 0.17565836, 0.1734152 , 0.15855913,\n",
       "        0.18701872, 0.15281212, 0.20332426, 0.2393437 , 0.1541775 ,\n",
       "        0.19736363, 0.13319197, 0.16618998, 0.16735291, 0.1598586 ,\n",
       "        0.22406892, 0.32722285, 0.22545986, 0.13054389, 0.23845944,\n",
       "        0.17959669, 0.2290103 , 0.24263811, 0.26558793, 0.13298514,\n",
       "        0.19870892, 0.1837048 , 0.16453457, 0.1719519 , 0.11528723,\n",
       "        0.1965847 , 0.15844296, 0.14361687, 0.13565004, 0.14873767,\n",
       "        0.20484927, 0.25516543, 0.09899904, 0.17263488, 0.1044229 ,\n",
       "        0.23502024, 0.17800541, 0.10140179, 0.19757068, 0.21788967,\n",
       "        0.09120042, 0.22492073, 0.15914205, 0.17027114, 0.20374219,\n",
       "        0.1630747 , 0.13595009, 0.16243166, 0.15555917, 0.23030674,\n",
       "        0.18102038, 0.1773736 , 0.23348644, 0.17390162, 0.12507245,\n",
       "        0.17590646, 0.14932586, 0.02481532, 0.11235902, 0.29761955,\n",
       "        0.12637126, 0.20156462, 0.16831744, 0.25325614, 0.16761748,\n",
       "        0.14606261, 0.17696409, 0.02866047, 0.22435297, 0.16545019,\n",
       "        0.19452998, 0.13599555, 0.15724485, 0.25784886, 0.21566063,\n",
       "        0.19112538, 0.17016621, 0.1650053 , 0.19331196, 0.18336296,\n",
       "        0.19230661, 0.15462115, 0.16100527, 0.21340144, 0.1293767 ,\n",
       "        0.20075226, 0.2460774 , 0.2051592 , 0.16856866, 0.13601032,\n",
       "        0.15258542, 0.21058579, 0.13283676, 0.30634513, 0.30259863,\n",
       "        0.174911  , 0.21361318, 0.16588463, 0.1935794 , 0.16966352,\n",
       "        0.13023   , 0.12817168, 0.18797189, 0.2345805 , 0.1723788 ,\n",
       "        0.23310949, 0.16317049, 0.11013779, 0.173204  , 0.18546371,\n",
       "        0.22800314, 0.18874453, 0.13554445, 0.15378293, 0.15408473,\n",
       "        0.14571941, 0.16492236, 0.13989979, 0.14171611, 0.14427537,\n",
       "        0.25688416, 0.16545095, 0.20909865, 0.29428086, 0.12040624,\n",
       "        0.2043778 , 0.16302632, 0.17303056, 0.14852194, 0.15755573,\n",
       "        0.18994364, 0.19318697, 0.13535064, 0.18876106, 0.2831548 ,\n",
       "        0.16611929, 0.14625219, 0.17362313, 0.30741784, 0.16797948,\n",
       "        0.21709435, 0.16741596, 0.11527696, 0.16388395, 0.17165652,\n",
       "        0.19117816, 0.11554736, 0.1738613 , 0.1191171 , 0.2252082 ,\n",
       "        0.2367723 , 0.19065928, 0.1464564 , 0.21195467, 0.2305814 ,\n",
       "        0.18335943, 0.15179563, 0.13807108, 0.1876682 , 0.11474463,\n",
       "        0.21561342, 0.14115316, 0.2617176 , 0.19273019, 0.18077295,\n",
       "        0.1924817 , 0.22766462, 0.15023953, 0.14515038, 0.17000186,\n",
       "        0.18747602, 0.19346106, 0.2386147 , 0.10120088, 0.1739423 ,\n",
       "        0.19326054, 0.19629212, 0.23584004, 0.18790808, 0.19572297,\n",
       "        0.16244103, 0.20377916, 0.22396082, 0.13574742, 0.14460494,\n",
       "        0.16239476, 0.15836306, 0.13384956, 0.10576791, 0.1575239 ,\n",
       "        0.1485946 , 0.19597398, 0.17760335, 0.1828513 , 0.2271224 ,\n",
       "        0.21913597, 0.1184729 , 0.18976828, 0.20247677, 0.23513588,\n",
       "        0.21423218, 0.10501182, 0.18231954, 0.1699995 , 0.13140763,\n",
       "        0.1762335 , 0.20979188, 0.21091178, 0.13026251, 0.19212103,\n",
       "        0.15359677, 0.17326955, 0.17686099, 0.13801807, 0.20157649,\n",
       "        0.01285505, 0.02407128, 0.17908098, 0.20340525, 0.28378797,\n",
       "        0.21121404, 0.16804537, 0.19945474, 0.34968185, 0.24612096,\n",
       "        0.15149112, 0.2257938 , 0.17659752, 0.14274585, 0.2189854 ,\n",
       "        0.19887623, 0.13557722, 0.20868324, 0.16914588, 0.21699207,\n",
       "        0.22421718, 0.1476049 , 0.16889443, 0.13910902, 0.18863876,\n",
       "        0.12330307, 0.11320806, 0.24847001, 0.2328948 , 0.11579148,\n",
       "        0.17005566, 0.19620992, 0.16897777, 0.15199265, 0.15867646,\n",
       "        0.19140124, 0.15996774, 0.1948234 , 0.19757473, 0.20967509,\n",
       "        0.19646752, 0.19350903, 0.18816318, 0.19895715, 0.23869863,\n",
       "        0.16838601, 0.15505579, 0.11943781, 0.21065165, 0.28665495,\n",
       "        0.10156725, 0.16841657, 0.15266362, 0.20959885, 0.15922013,\n",
       "        0.12392274, 0.20787059, 0.18732423, 0.22305498, 0.16280426,\n",
       "        0.19263944, 0.20643263, 0.12397335, 0.14228156, 0.11019405,\n",
       "        0.14258067, 0.17397614, 0.14729664, 0.21316476, 0.18909031,\n",
       "        0.1739009 , 0.14717367, 0.18385755, 0.26215404, 0.17695592,\n",
       "        0.2118739 , 0.1455948 , 0.18309134, 0.11240587, 0.12911692,\n",
       "        0.15842015, 0.17644005, 0.2034954 , 0.19647343, 0.19050108,\n",
       "        0.26045695, 0.20838498, 0.11773131, 0.1876847 , 0.12235904,\n",
       "        0.23156132, 0.16649656, 0.2864968 , 0.22504927, 0.18196549,\n",
       "        0.19563395, 0.25637433, 0.14047821, 0.20511149, 0.17855863,\n",
       "        0.18121587, 0.1820871 , 0.20161238, 0.14273433, 0.16752817,\n",
       "        0.17696568, 0.21581848, 0.16403098, 0.10178128, 0.13592257,\n",
       "        0.20134887, 0.1168872 , 0.18759365, 0.11634432, 0.17337684,\n",
       "        0.13279955, 0.19582729, 0.16523461, 0.22972335, 0.2458623 ,\n",
       "        0.15285172, 0.13306105, 0.2541011 , 0.17889708, 0.23282702,\n",
       "        0.16170579, 0.16397765, 0.24679744, 0.1280454 , 0.21923456,\n",
       "        0.12642555, 0.1643026 , 0.20789392, 0.15181653, 0.12891224,\n",
       "        0.16505106, 0.23861465, 0.16829108, 0.24603242, 0.15655078,\n",
       "        0.13831371, 0.15569144, 0.13680108, 0.19379342, 0.19503893,\n",
       "        0.10427767, 0.22864056, 0.2192456 , 0.18876258, 0.24146405,\n",
       "        0.16674548, 0.1580054 , 0.15021653, 0.15080708, 0.1472078 ,\n",
       "        0.13248174, 0.24836712, 0.16890611, 0.13864219, 0.18646365,\n",
       "        0.1521859 , 0.17239434, 0.18937474, 0.21948384, 0.15234652,\n",
       "        0.18582496, 0.2193456 , 0.17698647, 0.17890106, 0.17730294,\n",
       "        0.24596958, 0.21856183, 0.13994955, 0.12895279, 0.11874162,\n",
       "        0.11851571, 0.12332254, 0.22329685, 0.14479119, 0.203858  ,\n",
       "        0.23397008, 0.15755151, 0.09126988, 0.15929013, 0.16741608,\n",
       "        0.24773808, 0.2110988 , 0.15402316, 0.12717581, 0.11303446,\n",
       "        0.22160718, 0.15343961, 0.15350284, 0.18354456, 0.2296391 ,\n",
       "        0.28291726, 0.12925985, 0.24596973, 0.18662143, 0.28021917,\n",
       "        0.14515401, 0.14888722, 0.17367774, 0.26986444, 0.10928216,\n",
       "        0.15493624, 0.12253764, 0.16406491, 0.15056889, 0.19763319,\n",
       "        0.14281358, 0.17155403, 0.14824003, 0.17723677, 0.15639946,\n",
       "        0.21464202, 0.20715968, 0.16380447, 0.2090624 , 0.18800528,\n",
       "        0.260541  , 0.14647698, 0.16090411, 0.6568186 , 0.16067438,\n",
       "        0.16968948, 0.10927246, 0.18694597, 0.22021903, 0.16791676,\n",
       "        0.1770792 , 0.1444145 , 0.08513424, 0.14940925, 0.22371298,\n",
       "        0.17882074, 0.19245675, 0.11699054, 0.18717414, 0.18570666,\n",
       "        0.10891694, 0.17602795, 0.15866575, 0.18333295, 0.16601533,\n",
       "        0.18566959, 0.14686538, 0.23901314, 0.1651542 , 0.15146315,\n",
       "        0.15175855, 0.22077726, 0.16755368, 0.12861022, 0.01805313,\n",
       "        0.18306795, 0.1791952 , 0.18836215, 0.12317289, 0.1786157 ,\n",
       "        0.15149932, 0.15628614, 0.12041333, 0.14799   , 0.01679645,\n",
       "        0.18042701, 0.14341973, 0.14091523, 0.19758686, 0.19021557,\n",
       "        0.1222397 , 0.17348364, 0.19454633, 0.15858282, 0.22575063,\n",
       "        0.18360698, 0.15251549, 0.11267012, 0.22676674, 0.2128406 ,\n",
       "        0.15137778, 0.12221746, 0.18141288, 0.15131742, 0.1184435 ,\n",
       "        0.17865661, 0.11712519, 0.24194364, 0.13423125, 0.13957797,\n",
       "        0.1634165 , 0.14020862, 0.1674764 , 0.19329853, 0.16739877,\n",
       "        0.13798727], dtype=float32),\n",
       " array([0.28725544, 0.40405115, 0.22592239, 0.14686109, 0.31783292,\n",
       "        0.34927407, 0.24492855, 0.30493185, 0.20585702, 0.34678355,\n",
       "        0.21669145, 0.30312493, 0.24015824, 0.21730924, 0.3381306 ,\n",
       "        0.3348883 , 0.3261312 , 0.27390808, 0.3851562 , 0.42158216,\n",
       "        0.30894852, 0.3768817 , 0.32510254, 0.3340573 , 0.3718986 ,\n",
       "        0.32129097, 0.24441157, 0.25314745, 0.23629244, 0.25106296,\n",
       "        0.31023043, 0.2715695 , 0.38130903, 0.2600059 , 0.2460638 ,\n",
       "        0.30803344, 0.28368703, 0.34719783, 0.28375843, 0.23911014,\n",
       "        0.19484593, 0.32742792, 0.33830732, 0.25517657, 0.2537401 ,\n",
       "        0.23558152, 0.23793529, 0.2779013 , 0.33381158, 0.35107407,\n",
       "        0.1969086 , 0.28667057, 0.19263509, 0.27259946, 0.42008162,\n",
       "        0.37215585, 0.2734413 , 0.16750595, 0.2999616 , 0.28594705,\n",
       "        0.27648067, 0.33380318, 0.24948141, 0.30037102, 0.1810529 ,\n",
       "        0.18664587, 0.34724507, 0.31010497, 0.23401564, 0.34162378,\n",
       "        0.40262124, 0.5084368 , 0.28163838, 0.26608667, 0.27996764,\n",
       "        0.34712657, 0.42425576, 0.33612052, 0.1949728 , 0.1827079 ,\n",
       "        0.35289603, 0.4307632 , 0.42369768, 0.30532485, 0.30597195,\n",
       "        0.32096103, 0.4915358 , 0.31825972, 0.24558094, 0.22150224,\n",
       "        0.33985502, 0.25159878, 0.31473273, 0.32537314, 0.39453423,\n",
       "        0.3223832 ], dtype=float32),\n",
       " array([0.10716747, 0.18765791, 0.25305614, 0.2172492 , 0.18383904,\n",
       "        0.12658024, 0.16543092, 0.23144265, 0.1965466 , 0.2199184 ,\n",
       "        0.21723372, 0.16064881, 0.20775771, 0.14213184, 0.1938198 ,\n",
       "        0.17882566, 0.08070829, 0.15952712, 0.18733737, 0.22262916,\n",
       "        0.18651006, 0.22957562, 0.20569107, 0.19570884, 0.1370752 ,\n",
       "        0.05907298, 0.1566458 , 0.1557552 , 0.18009216, 0.22315158,\n",
       "        0.17132133, 0.15392834, 0.19298016, 0.20619963, 0.22547959,\n",
       "        0.17633316, 0.16856238, 0.18654965, 0.22459653, 0.17767859,\n",
       "        0.06615835, 0.18157227, 0.19374597, 0.20375283, 0.17131834,\n",
       "        0.18852827, 0.18553202, 0.13208368, 0.1889911 , 0.19898242,\n",
       "        0.1818739 , 0.22610863, 0.18630745, 0.1556526 , 0.17821467,\n",
       "        0.19104311, 0.16290267, 0.18380882, 0.18737541, 0.19076893,\n",
       "        0.19737102, 0.19147757, 0.1673771 , 0.14867203, 0.09853473,\n",
       "        0.18441954, 0.1884883 , 0.17881244, 0.15655486, 0.16416149,\n",
       "        0.1557783 , 0.2202027 , 0.19666179, 0.14622957, 0.21111964,\n",
       "        0.2166327 , 0.17625386, 0.2095032 , 0.09285985, 0.11739776,\n",
       "        0.17815039, 0.09078052, 0.20260298, 0.20224364, 0.17529586,\n",
       "        0.15451927, 0.16471641, 0.22259445, 0.17162636, 0.17409681,\n",
       "        0.16567796, 0.21144858, 0.2440048 , 0.20187421, 0.18047872,\n",
       "        0.20117301, 0.09948847, 0.10625718, 0.18771262, 0.15874083,\n",
       "        0.24880679, 0.23194344, 0.21182728, 0.24575277, 0.20176093,\n",
       "        0.21511985, 0.21396326, 0.21132179, 0.22844979, 0.19264229,\n",
       "        0.1754573 , 0.25293338, 0.23094182, 0.1141843 , 0.16442254,\n",
       "        0.18307886, 0.1871557 , 0.18602283, 0.22603558, 0.26088732,\n",
       "        0.20680693, 0.08301888, 0.22836259, 0.1341032 , 0.18013923,\n",
       "        0.15205567, 0.20847476, 0.14878061, 0.18576959, 0.11406466,\n",
       "        0.14260894, 0.19943437, 0.21209675, 0.18716453, 0.09782015,\n",
       "        0.06907171, 0.19723915, 0.18064547, 0.2073219 , 0.16229467,\n",
       "        0.21313533, 0.21824624, 0.09218524, 0.20565931, 0.14590247,\n",
       "        0.19468118, 0.17805566, 0.06458402, 0.19923449, 0.15918691,\n",
       "        0.18947652, 0.15664288, 0.22746123, 0.20371385, 0.2402333 ,\n",
       "        0.16621867, 0.19595815, 0.22989723, 0.16489345, 0.18985394,\n",
       "        0.18295938, 0.18610503, 0.1844751 , 0.12657717, 0.20179406,\n",
       "        0.20321241, 0.1517859 , 0.14320827, 0.19999206, 0.23223922,\n",
       "        0.17629357, 0.2684633 , 0.15176812, 0.16913837, 0.1564245 ,\n",
       "        0.15930893, 0.18850596, 0.16178478, 0.1786713 , 0.19512922,\n",
       "        0.07241375, 0.12401954, 0.19152692, 0.19654149, 0.15606399,\n",
       "        0.19639263, 0.20062949, 0.16605943, 0.25964475, 0.1682908 ,\n",
       "        0.16321279, 0.12251423, 0.1399867 , 0.1296746 , 0.20516486,\n",
       "        0.13577063, 0.17480873, 0.11584365, 0.12340563, 0.16882907,\n",
       "        0.19118237, 0.24390206, 0.18080993, 0.2377769 , 0.23159924,\n",
       "        0.13973355, 0.20373422, 0.15833122, 0.19827576, 0.16070503,\n",
       "        0.21228024, 0.20234422, 0.14637822, 0.09313504, 0.21093287,\n",
       "        0.13009621, 0.16429587, 0.2006663 , 0.18594992, 0.22404663,\n",
       "        0.12431202, 0.15121578, 0.20232116, 0.2050696 , 0.21358329,\n",
       "        0.10856737, 0.19716026, 0.19809788, 0.22549613, 0.14875342,\n",
       "        0.22860599, 0.16967103, 0.19106695, 0.26237136, 0.16441505,\n",
       "        0.13988219, 0.2165571 , 0.1768302 , 0.09194987, 0.16055468,\n",
       "        0.15835164, 0.18664618, 0.18490048, 0.1723761 , 0.17952812,\n",
       "        0.18557689, 0.12722547, 0.18046325, 0.15568727, 0.15986808,\n",
       "        0.0985174 , 0.1555115 , 0.2649025 , 0.17735611, 0.19391942,\n",
       "        0.08332953, 0.19559288, 0.11882608, 0.2008377 , 0.08234239,\n",
       "        0.17717583, 0.13082843, 0.17432605, 0.21894248, 0.08084151,\n",
       "        0.20407166, 0.23548654, 0.05910212, 0.18041423, 0.19120373,\n",
       "        0.17309427, 0.19359095, 0.25673562, 0.20008317, 0.20821632,\n",
       "        0.18425526, 0.1920108 , 0.18622056, 0.17740147, 0.19611146,\n",
       "        0.14679167, 0.21464157, 0.21676879, 0.20143557, 0.14906593,\n",
       "        0.21577436, 0.19999865, 0.14517446, 0.17551662, 0.2496761 ,\n",
       "        0.2004914 , 0.1748979 , 0.19568473, 0.19822629, 0.15813847,\n",
       "        0.11956306, 0.18610488, 0.09161112, 0.11423183, 0.10491172,\n",
       "        0.20912401, 0.10882922, 0.16024014, 0.14349613, 0.00311173,\n",
       "        0.21395002, 0.2604504 , 0.2083832 , 0.15208972, 0.20422363,\n",
       "        0.17778973, 0.17084117, 0.19739507, 0.14579134, 0.14675893,\n",
       "        0.16202396, 0.31978452, 0.18561444, 0.18181282, 0.13411534,\n",
       "        0.19909728, 0.18099622, 0.19007082, 0.18063992, 0.17500784,\n",
       "        0.2050944 , 0.19907704, 0.2227185 , 0.23973408, 0.2177788 ,\n",
       "        0.15936191, 0.17265809, 0.20035785, 0.09716755, 0.10687023,\n",
       "        0.1809444 , 0.17749263, 0.14205709, 0.1603425 , 0.15637709,\n",
       "        0.14956802, 0.18798782, 0.17179078, 0.19613472, 0.17959253,\n",
       "        0.16708113, 0.20279318, 0.16767642, 0.20867808, 0.15519632,\n",
       "        0.15788186, 0.14832805, 0.11562747, 0.18869942, 0.20659825,\n",
       "        0.16371027, 0.17678316, 0.14327687, 0.15134323, 0.14775391,\n",
       "        0.14194414, 0.15083507, 0.1303203 , 0.18984859, 0.1462162 ,\n",
       "        0.18753164, 0.1863401 , 0.16034836, 0.20981406, 0.17647165,\n",
       "        0.06001652, 0.22176093, 0.20464112, 0.2560495 , 0.1881333 ,\n",
       "        0.13215266, 0.17274092, 0.21429014, 0.22550376, 0.17679897,\n",
       "        0.21139336, 0.2094596 , 0.15009154, 0.17782716, 0.16354987,\n",
       "        0.20373318, 0.18904944, 0.25091714, 0.1849935 , 0.19925898,\n",
       "        0.18003544, 0.19582146, 0.1459142 , 0.18389052, 0.17498665,\n",
       "        0.18090004, 0.18958904, 0.17841847, 0.24954525, 0.18216643,\n",
       "        0.17898834, 0.1587686 , 0.05224337, 0.17409928, 0.20188592,\n",
       "        0.14105491, 0.15047902, 0.19050539, 0.20422404, 0.19839261,\n",
       "        0.15555668, 0.12034145, 0.14362296, 0.2091822 , 0.1593786 ,\n",
       "        0.15505607, 0.09732071, 0.1903732 , 0.15756369, 0.15031064,\n",
       "        0.20751308, 0.1770021 , 0.18050589, 0.03522794, 0.18406902,\n",
       "        0.18629837, 0.20352674, 0.24587725, 0.17792293, 0.13031705,\n",
       "        0.22119376, 0.14271764, 0.0793516 , 0.2056741 , 0.13046367,\n",
       "        0.17453307, 0.18955405, 0.20400555, 0.10246133, 0.09033328,\n",
       "        0.13995023, 0.14296684, 0.15220723, 0.20127799, 0.17559633,\n",
       "        0.08512033, 0.20335841, 0.18752967, 0.17964558, 0.17390317,\n",
       "        0.19061148, 0.15968366, 0.16708465, 0.18512958, 0.17204696,\n",
       "        0.04415045, 0.07562521, 0.16390909, 0.17042047, 0.177991  ,\n",
       "        0.13917424, 0.13259783, 0.157099  , 0.23472632, 0.06477569,\n",
       "        0.16011831, 0.1722609 , 0.14437683, 0.15834033, 0.1758515 ,\n",
       "        0.21817529, 0.21019326, 0.19617124, 0.14652874, 0.18342921,\n",
       "        0.13443588, 0.21577038, 0.17184775, 0.18570465, 0.16677839,\n",
       "        0.15443665, 0.16986148, 0.15795757, 0.17809317, 0.11352839,\n",
       "        0.19172885, 0.19825597, 0.19351508, 0.17495169, 0.2253828 ,\n",
       "        0.18393272, 0.21096283, 0.1868916 , 0.21375728, 0.15480845,\n",
       "        0.20153385, 0.1683353 , 0.19123513, 0.18877526, 0.21134049,\n",
       "        0.14546098, 0.17171732, 0.18930556, 0.18785365, 0.20210499,\n",
       "        0.18901001, 0.20287135, 0.19159514, 0.06747913, 0.2013572 ,\n",
       "        0.16938408, 0.20141616, 0.10459973, 0.18691531, 0.1591692 ,\n",
       "        0.18974523, 0.21546866, 0.1720771 , 0.23595162, 0.20069876,\n",
       "        0.21294828, 0.18254413, 0.1919349 , 0.22852594, 0.23237205,\n",
       "        0.18976247, 0.17122021, 0.21232975, 0.18706428, 0.1572406 ,\n",
       "        0.20491023, 0.16963139, 0.20958805, 0.1998177 , 0.23023309,\n",
       "        0.18011342, 0.18551415, 0.18591793, 0.24941261, 0.14254443,\n",
       "        0.19089417, 0.16380842, 0.19541864, 0.00353799, 0.18581456,\n",
       "        0.1058071 , 0.21278593, 0.10400137, 0.23522027, 0.1832309 ,\n",
       "        0.1824283 , 0.16177663, 0.18052743, 0.16756593, 0.16829026,\n",
       "        0.1801149 , 0.16039559, 0.1870248 , 0.1714523 , 0.17919229,\n",
       "        0.21217632, 0.2144791 , 0.19418082, 0.11816324, 0.16753776,\n",
       "        0.20460474, 0.10002912, 0.15844658, 0.19499081, 0.15463607,\n",
       "        0.19217114, 0.16498625, 0.17064752, 0.14004733, 0.21530345,\n",
       "        0.12422983], dtype=float32),\n",
       " array([0.1575958 , 0.15814355, 0.3555074 , 0.20433795, 0.15077831,\n",
       "        0.14487618, 0.15089016, 0.29639253, 0.19272931, 0.21778175,\n",
       "        0.28305203, 0.13277912, 0.2790038 , 0.10934784, 0.16604196,\n",
       "        0.22860993, 0.1773039 , 0.14257611, 0.2367093 , 0.2599256 ,\n",
       "        0.17883939, 0.2447717 , 0.18208945, 0.18456696, 0.11492115,\n",
       "        0.16725522, 0.1700532 , 0.11622433, 0.17640866, 0.24685614,\n",
       "        0.14580907, 0.15808119, 0.16556503, 0.2973169 , 0.17509969,\n",
       "        0.15270312, 0.14308284, 0.1603623 , 0.16310236, 0.12833244,\n",
       "        0.16527863, 0.21918042, 0.1637536 , 0.18356283, 0.15357998,\n",
       "        0.17409578, 0.23562472, 0.12915465, 0.2144563 , 0.27220395,\n",
       "        0.18786629, 0.23334748, 0.16437995, 0.13786604, 0.16656303,\n",
       "        0.23888323, 0.16899216, 0.15813705, 0.19136985, 0.22292128,\n",
       "        0.19264309, 0.18497387, 0.17234832, 0.12917586, 0.16496898,\n",
       "        0.12244587, 0.18356317, 0.15351309, 0.18090884, 0.1250421 ,\n",
       "        0.13604693, 0.24887505, 0.2180536 , 0.1249403 , 0.19075269,\n",
       "        0.18698254, 0.15581565, 0.15142168, 0.3422871 , 0.12269104,\n",
       "        0.18270145, 0.17773865, 0.17265132, 0.2855265 , 0.22801617,\n",
       "        0.1712788 , 0.10182553, 0.21636346, 0.19129111, 0.1603508 ,\n",
       "        0.12671317, 0.23583189, 0.21205902, 0.22097374, 0.16589388,\n",
       "        0.20740211, 0.15980507, 0.24224567, 0.17382309, 0.2998251 ,\n",
       "        0.2805824 , 0.2580481 , 0.22245827, 0.33041134, 0.13812324,\n",
       "        0.24441758, 0.20254748, 0.22586738, 0.20474638, 0.29193032,\n",
       "        0.23748808, 0.33427873, 0.21291763, 0.1404623 , 0.12562665,\n",
       "        0.25246054, 0.19839756, 0.1711912 , 0.23332335, 0.24995369,\n",
       "        0.2001634 , 0.2330702 , 0.38630164, 0.14176969, 0.17607345,\n",
       "        0.15133035, 0.19597557, 0.12941737, 0.18946058, 0.10078769,\n",
       "        0.17564157, 0.16873239, 0.26230553, 0.21524231, 0.16026385,\n",
       "        0.20262481, 0.24868336, 0.1789902 , 0.2805127 , 0.1380264 ,\n",
       "        0.1806742 , 0.23452382, 0.18785647, 0.22337587, 0.12601282,\n",
       "        0.18158089, 0.2130874 , 0.15794078, 0.18667398, 0.19523484,\n",
       "        0.19184273, 0.14407657, 0.27164623, 0.22995232, 0.23149858,\n",
       "        0.22843401, 0.19092844, 0.15438889, 0.15981029, 0.22522633,\n",
       "        0.2043423 , 0.18867457, 0.20551522, 0.16471078, 0.22724023,\n",
       "        0.16977762, 0.13949852, 0.2007694 , 0.15891059, 0.21447203,\n",
       "        0.14622581, 0.33433634, 0.14938864, 0.19001277, 0.13407005,\n",
       "        0.17041059, 0.19111334, 0.12529534, 0.20665844, 0.22688769,\n",
       "        0.3145954 , 0.12448325, 0.19751403, 0.1617786 , 0.1575866 ,\n",
       "        0.20602563, 0.19120276, 0.2032229 , 0.18008845, 0.13530532,\n",
       "        0.18992028, 0.18947917, 0.18679605, 0.16737524, 0.16252767,\n",
       "        0.15918958, 0.19732717, 0.18388332, 0.19339386, 0.14524895,\n",
       "        0.18564886, 0.23975414, 0.17510056, 0.19283968, 0.28016156,\n",
       "        0.12848328, 0.18135917, 0.14440185, 0.18277305, 0.14778733,\n",
       "        0.25547886, 0.29929912, 0.16646431, 0.12900488, 0.23448358,\n",
       "        0.12048138, 0.18421808, 0.24812983, 0.1464608 , 0.34777224,\n",
       "        0.19961251, 0.11718132, 0.23964067, 0.28310624, 0.19847769,\n",
       "        0.13686101, 0.22000745, 0.2145855 , 0.21266839, 0.15359552,\n",
       "        0.23203383, 0.20424603, 0.15534635, 0.3126235 , 0.12417668,\n",
       "        0.13473484, 0.30595532, 0.16282968, 0.16425785, 0.12849227,\n",
       "        0.19001353, 0.21776386, 0.19142225, 0.21204849, 0.20371664,\n",
       "        0.24101977, 0.14092775, 0.17447726, 0.13793953, 0.14143276,\n",
       "        0.14513257, 0.15803011, 0.23827991, 0.18324764, 0.1931252 ,\n",
       "        0.14159733, 0.27296814, 0.15540145, 0.15204951, 0.20569775,\n",
       "        0.19921115, 0.17007658, 0.12790358, 0.20257634, 0.19392422,\n",
       "        0.26585093, 0.24835232, 0.1611581 , 0.18222395, 0.27068284,\n",
       "        0.15366168, 0.22056526, 0.25864676, 0.18231705, 0.18454888,\n",
       "        0.20395881, 0.16442999, 0.17578468, 0.16453728, 0.14431179,\n",
       "        0.15337175, 0.2822694 , 0.1705364 , 0.21043731, 0.12423554,\n",
       "        0.16331722, 0.20014295, 0.16094945, 0.17439778, 0.21002208,\n",
       "        0.1488499 , 0.15664023, 0.204276  , 0.18330872, 0.16408996,\n",
       "        0.14898324, 0.15798283, 0.20302378, 0.17124231, 0.1629818 ,\n",
       "        0.21470638, 0.15713672, 0.20266479, 0.10591657, 0.03829065,\n",
       "        0.26701596, 0.24267414, 0.20996994, 0.1150395 , 0.30117628,\n",
       "        0.20232841, 0.11272539, 0.19362512, 0.14682403, 0.12476142,\n",
       "        0.14572221, 0.4013266 , 0.19720724, 0.26717222, 0.13541542,\n",
       "        0.23690903, 0.17605312, 0.17785127, 0.15557244, 0.19324099,\n",
       "        0.2315349 , 0.20372829, 0.22747365, 0.20608084, 0.21055993,\n",
       "        0.17530438, 0.22975573, 0.21168596, 0.35423526, 0.12202606,\n",
       "        0.20583437, 0.19256884, 0.13635522, 0.17676337, 0.15634514,\n",
       "        0.12749867, 0.22974373, 0.14462863, 0.17993787, 0.17546941,\n",
       "        0.18243688, 0.1472582 , 0.1305116 , 0.19430594, 0.11031542,\n",
       "        0.17232585, 0.15704317, 0.19282687, 0.2050823 , 0.17651767,\n",
       "        0.13966246, 0.19461344, 0.1309075 , 0.11163948, 0.15385936,\n",
       "        0.13735558, 0.1454308 , 0.12574501, 0.18574418, 0.15208645,\n",
       "        0.19774008, 0.19373035, 0.13422331, 0.20446229, 0.19595389,\n",
       "        0.16245332, 0.31783128, 0.19831252, 0.22006798, 0.17235467,\n",
       "        0.14929497, 0.132396  , 0.2145264 , 0.2463306 , 0.18638909,\n",
       "        0.22835267, 0.27478293, 0.13499758, 0.22919622, 0.23288897,\n",
       "        0.2340294 , 0.2260247 , 0.24769108, 0.15823402, 0.17543177,\n",
       "        0.19701254, 0.17888966, 0.13101591, 0.15577874, 0.20047171,\n",
       "        0.16892268, 0.19783622, 0.1299253 , 0.24523585, 0.18363151,\n",
       "        0.20723666, 0.12252701, 0.34105375, 0.23495509, 0.270592  ,\n",
       "        0.16747148, 0.13071805, 0.2031278 , 0.19820893, 0.19657636,\n",
       "        0.13598022, 0.19323884, 0.12435424, 0.25340304, 0.18706392,\n",
       "        0.18849157, 0.12908173, 0.184778  , 0.19580425, 0.11762041,\n",
       "        0.15426455, 0.12926307, 0.18298192, 0.38817763, 0.1983619 ,\n",
       "        0.2571705 , 0.2510794 , 0.28312215, 0.18212035, 0.12313569,\n",
       "        0.22531523, 0.10267542, 0.17209452, 0.19107515, 0.14883547,\n",
       "        0.13025409, 0.18662679, 0.23901987, 0.16227703, 0.16934456,\n",
       "        0.1570685 , 0.27281666, 0.13543211, 0.26018602, 0.17743124,\n",
       "        0.1542482 , 0.20437731, 0.13139848, 0.18450078, 0.20230691,\n",
       "        0.18324485, 0.13558643, 0.1390421 , 0.17874321, 0.14868297,\n",
       "        0.13972081, 0.17772979, 0.13371468, 0.1621226 , 0.13624363,\n",
       "        0.12443534, 0.16205202, 0.13310926, 0.2291055 , 0.15133981,\n",
       "        0.17550817, 0.18234174, 0.11737084, 0.15583983, 0.158734  ,\n",
       "        0.2644868 , 0.21665318, 0.22561347, 0.1429346 , 0.2551799 ,\n",
       "        0.12199521, 0.2623511 , 0.13091667, 0.163126  , 0.16939934,\n",
       "        0.13376959, 0.16738929, 0.119225  , 0.11745915, 0.14318405,\n",
       "        0.1787158 , 0.16495116, 0.20879218, 0.13785203, 0.2751609 ,\n",
       "        0.21679802, 0.22573419, 0.17683686, 0.1340976 , 0.13092028,\n",
       "        0.15676199, 0.1447946 , 0.18985903, 0.17803857, 0.16056283,\n",
       "        0.13474336, 0.17818996, 0.22929752, 0.17829467, 0.16213171,\n",
       "        0.18505609, 0.20569804, 0.15194528, 0.3771708 , 0.2414032 ,\n",
       "        0.13543719, 0.20531103, 0.16078274, 0.18882242, 0.1452392 ,\n",
       "        0.15187722, 0.28456512, 0.14697394, 0.24787903, 0.15488951,\n",
       "        0.2544396 , 0.20760103, 0.12180798, 0.2506674 , 0.21940589,\n",
       "        0.16018729, 0.16978247, 0.17853782, 0.15415265, 0.12689453,\n",
       "        0.25404006, 0.14941609, 0.18598151, 0.15679468, 0.2449579 ,\n",
       "        0.26435754, 0.27204576, 0.17118427, 0.58315307, 0.15556248,\n",
       "        0.2049397 , 0.1865205 , 0.15084447, 0.04495771, 0.17426397,\n",
       "        0.16214302, 0.17177252, 0.16989766, 0.17437439, 0.18901691,\n",
       "        0.15798609, 0.17204818, 0.2035196 , 0.14522891, 0.18784161,\n",
       "        0.17525074, 0.12479237, 0.24842505, 0.1170271 , 0.16510169,\n",
       "        0.19914442, 0.26264143, 0.18241268, 0.1611655 , 0.15481889,\n",
       "        0.20669949, 0.17864636, 0.18216133, 0.1857824 , 0.15725552,\n",
       "        0.18488741, 0.18739986, 0.17663305, 0.23507385, 0.20394294,\n",
       "        0.14621092], dtype=float32),\n",
       " array([0.1857194 , 0.18093581, 0.22948518, 0.20157859, 0.22245294,\n",
       "        0.18790226, 0.22253503, 0.16863325, 0.19153725, 0.23901805,\n",
       "        0.18140633, 0.22509122, 0.20549837, 0.24239033, 0.18731652,\n",
       "        0.25175157, 0.18966898, 0.25294805, 0.2475365 , 0.21398295,\n",
       "        0.21145825, 0.18068321, 0.21024582, 0.19044724, 0.20253201,\n",
       "        0.17830786, 0.28177017, 0.20679413, 0.5443598 , 0.18963572,\n",
       "        0.23962004, 0.1658235 , 0.19976595, 0.22196789, 0.16930182,\n",
       "        0.24170966, 0.26398647, 0.22552226, 0.19824336, 0.21505995,\n",
       "        0.2110733 , 0.22366352, 0.25389907, 0.3026723 , 0.26021248,\n",
       "        0.20299092, 0.23846382, 0.36799887, 0.21703911, 0.24743414,\n",
       "        0.17870687, 0.19805768, 0.19478557, 0.20025514, 0.20080735,\n",
       "        0.2531518 , 0.19518065, 0.28573874, 0.2030534 , 0.17915802,\n",
       "        0.22953549, 0.24802734, 0.26127452, 0.21548696, 0.19588467,\n",
       "        0.20044743, 0.22441554, 0.19120462, 0.24325688, 0.20391567,\n",
       "        0.20844537, 0.20338607, 0.25277212, 0.2850474 , 0.1919304 ,\n",
       "        0.22980261, 0.1892945 , 0.25648618, 0.24960077, 0.22499672,\n",
       "        0.27919012, 0.19562362, 0.18241921, 0.19912562, 0.17295282,\n",
       "        0.26039866, 0.22390205, 0.20441481, 0.26145673, 0.1934148 ,\n",
       "        0.24273427, 0.20055473, 0.3065896 , 0.19517195, 0.32608414,\n",
       "        0.23224778, 0.22331898, 0.19934168, 0.34281358, 0.24199511,\n",
       "        0.2183446 , 0.2348977 , 0.26955867, 0.20408472, 0.23925908,\n",
       "        0.2167473 , 0.23667204, 0.19354533, 0.2490632 , 0.27984038,\n",
       "        0.31841198, 0.26898253, 0.22202791, 0.2604403 , 0.28586105,\n",
       "        0.21350732, 0.2317952 , 0.24433003, 0.174949  , 0.2107621 ,\n",
       "        0.19151963, 0.2691379 , 0.2303728 , 0.2694429 , 0.22528844,\n",
       "        0.2796357 , 0.20966214, 0.33374357, 0.23436776, 0.18279938,\n",
       "        0.26539594, 0.18805353, 0.20808706, 0.21318501, 0.22573034,\n",
       "        0.45793912, 0.30006194, 0.19227189, 0.22404437, 0.25256592,\n",
       "        0.22329253, 0.23602058, 0.265123  , 0.22982621, 0.2825263 ,\n",
       "        0.24224079, 0.3530778 , 0.29109704, 0.17068335, 0.19215813,\n",
       "        0.2646246 , 0.26878694, 0.23432118, 0.19760858, 0.3753708 ,\n",
       "        0.2008047 , 0.21996632, 0.2414361 , 0.34506342, 0.21492286],\n",
       "       dtype=float32),\n",
       " array([0.21050344, 0.0957341 , 0.08083237, 0.15225519, 0.12803738,\n",
       "        0.1227454 , 0.08373927, 0.07214013, 0.1284139 , 0.13868423,\n",
       "        0.08483122, 0.11274973, 0.17101407, 0.13686702, 0.12652893,\n",
       "        0.10853489, 0.1262682 , 0.11551951, 0.11087445, 0.14830643,\n",
       "        0.10908521, 0.11471531, 0.14566882, 0.09606592, 0.14488427,\n",
       "        0.14878275, 0.19187893, 0.10543139, 0.09761702, 0.12047444,\n",
       "        0.09096911, 0.12709986, 0.11733536, 0.10807694, 0.15278251,\n",
       "        0.12363148, 0.12536877, 0.12930712, 0.10046042, 0.11722995,\n",
       "        0.15579695, 0.11357369, 0.14621194, 0.12867966, 0.0955845 ,\n",
       "        0.11335807, 0.14713828, 0.10309576, 0.12859404, 0.1235256 ,\n",
       "        0.14272049, 0.15902534, 0.15966064, 0.16928111, 0.10065477,\n",
       "        0.1154905 , 0.07554077, 0.09351017, 0.15604308, 0.13482822,\n",
       "        0.20437896, 0.16111343, 0.14167957, 0.10748232, 0.15282708,\n",
       "        0.12456878, 0.09126687, 0.12698959, 0.11343212, 0.13910504,\n",
       "        0.13011734, 0.13052385, 0.11270097, 0.09303224, 0.12009134,\n",
       "        0.11671156, 0.16501728, 0.12571834, 0.13270979, 0.13499868,\n",
       "        0.12647925, 0.15218706, 0.12290276, 0.11861026, 0.13925454,\n",
       "        0.13773197, 0.1371496 , 0.11876314, 0.1577669 , 0.08511606,\n",
       "        0.18045895, 0.14906543, 0.06937892, 0.12053332, 0.12090723,\n",
       "        0.10973725, 0.12633206, 0.13335103, 0.08838553, 0.1254523 ,\n",
       "        0.14427245, 0.1075369 , 0.0939625 , 0.11935925, 0.09176084,\n",
       "        0.14314273, 0.11738839, 0.10561455, 0.08884504, 0.21979263,\n",
       "        0.13342857, 0.13395594, 0.08328599, 0.11342593, 0.07297854,\n",
       "        0.14683248, 0.10232688, 0.12241547, 0.16707952, 0.2032692 ,\n",
       "        0.15546456, 0.13128695, 0.12761539, 0.14608061, 0.11627549,\n",
       "        0.10969064, 0.10707718, 0.1267838 , 0.2132111 , 0.09512414,\n",
       "        0.11607751, 0.13867182, 0.11370961, 0.1406354 , 0.09587553,\n",
       "        0.12927534, 0.0914115 , 0.18871824, 0.12345534, 0.11642923,\n",
       "        0.16383609, 0.12706682, 0.10348408, 0.13024016, 0.17856577,\n",
       "        0.1559769 , 0.09469786, 0.13098463, 0.13464077, 0.11367232,\n",
       "        0.09865168, 0.12379023, 0.14389865, 0.09290528, 0.14754477,\n",
       "        0.13231075, 0.13844107, 0.11355948, 0.12683903, 0.13286802,\n",
       "        0.12111984, 0.20727155, 0.1197108 , 0.14406374, 0.13595851,\n",
       "        0.14165623, 0.1505937 , 0.15477309, 0.12146272, 0.11467873,\n",
       "        0.14074364, 0.09536376, 0.09845745, 0.12359306, 0.14863987,\n",
       "        0.07290328, 0.11746286, 0.10823712, 0.14340883, 0.15777981,\n",
       "        0.14912316, 0.10783743, 0.18703088, 0.10033217, 0.12283918,\n",
       "        0.11792717, 0.16048628, 0.11660808, 0.06755713, 0.11653182,\n",
       "        0.12219461, 0.13594261, 0.09246366, 0.24193746, 0.15659745,\n",
       "        0.15393712, 0.1435273 , 0.15545078, 0.09178047, 0.11730024,\n",
       "        0.15592852, 0.05443829, 0.15132245, 0.11759237, 0.08991228,\n",
       "        0.08202299, 0.11174837, 0.07734915, 0.1366525 , 0.16671121,\n",
       "        0.12006579, 0.07576759, 0.12181221, 0.108624  , 0.11900351,\n",
       "        0.11055889, 0.09779353, 0.12677976, 0.10057303, 0.12344781,\n",
       "        0.17308077, 0.1028379 , 0.18905437, 0.14012764, 0.09875026,\n",
       "        0.131434  , 0.12547225, 0.12532529, 0.12407554, 0.11079575,\n",
       "        0.12841734, 0.13201709, 0.0882536 , 0.11399026, 0.15093784,\n",
       "        0.08154556, 0.09538137, 0.17268102, 0.09002002, 0.10480054,\n",
       "        0.12196219, 0.121781  , 0.1334097 , 0.09148078, 0.19153383,\n",
       "        0.17377892, 0.1258267 , 0.14922711, 0.10791232, 0.11090345,\n",
       "        0.12883312, 0.14383656, 0.10493136, 0.12257727, 0.14639649,\n",
       "        0.08625935, 0.12859905, 0.27934378, 0.11150685, 0.09152616,\n",
       "        0.12643233, 0.10626549, 0.12889467, 0.11382878, 0.11732602,\n",
       "        0.16760434, 0.09753306, 0.14021437, 0.11346793, 0.16072743,\n",
       "        0.17228189, 0.13440569, 0.11692718, 0.0741331 , 0.11064862,\n",
       "        0.09397761, 0.15688342, 0.17651288, 0.10496233, 0.12591892,\n",
       "        0.06662163, 0.15278219, 0.10759037, 0.13018948, 0.16813542,\n",
       "        0.13932657, 0.11339089, 0.09088082, 0.15235545, 0.13901235,\n",
       "        0.16021824, 0.08738039, 0.11069805, 0.14517072, 0.11034737,\n",
       "        0.12052056, 0.1074117 , 0.14204705, 0.19221725, 0.09857376,\n",
       "        0.19315355, 0.10743535, 0.09671116, 0.13109075, 0.11583964,\n",
       "        0.10787334, 0.16462213, 0.1048489 , 0.14184803, 0.14897645,\n",
       "        0.15091912, 0.11601532, 0.17972682, 0.14837599, 0.08792064,\n",
       "        0.13958374, 0.09093611, 0.08738428, 0.15818164, 0.1222186 ,\n",
       "        0.12364835, 0.16216524, 0.09186911, 0.09400038, 0.1336545 ,\n",
       "        0.1087537 , 0.09398575, 0.09442057, 0.15060706, 0.08563199,\n",
       "        0.11589562, 0.14372349, 0.08708151, 0.11670328, 0.12715654,\n",
       "        0.14291234, 0.15398839, 0.1284034 , 0.15491045, 0.16478357,\n",
       "        0.08555569, 0.13316236, 0.1309547 , 0.11322855, 0.1289073 ,\n",
       "        0.15539286, 0.11112615, 0.12547411, 0.11956671, 0.09689418,\n",
       "        0.11749119, 0.09999014, 0.1847231 , 0.11900245, 0.14504579,\n",
       "        0.07156813, 0.12587541, 0.11046927, 0.07663537, 0.0908736 ,\n",
       "        0.12398288, 0.1115353 , 0.18933211, 0.15499717, 0.08777855,\n",
       "        0.08844236, 0.09252777, 0.12791754, 0.07416046, 0.11519221,\n",
       "        0.12589817, 0.14191525, 0.1186589 , 0.1660574 , 0.15135668,\n",
       "        0.12003934, 0.12520038, 0.09545125, 0.13927671, 0.14424504,\n",
       "        0.16127883, 0.11472178, 0.11947624, 0.13227895, 0.17537189,\n",
       "        0.09964463, 0.06485122, 0.14441133, 0.23497203, 0.16315046,\n",
       "        0.15042603, 0.09606872, 0.07290043, 0.13543752, 0.145385  ,\n",
       "        0.12381999, 0.08185328, 0.09400842, 0.09810324, 0.11894494,\n",
       "        0.11844283, 0.07853067, 0.10968321, 0.12926747, 0.13103735,\n",
       "        0.10643839, 0.10219683, 0.10078545, 0.14751127, 0.14568153,\n",
       "        0.13522232, 0.13120136, 0.09407118, 0.13199578, 0.15371783,\n",
       "        0.10183041, 0.11274745, 0.12576886, 0.11923394, 0.1368519 ,\n",
       "        0.12787227, 0.13354613, 0.14652565, 0.11368732, 0.12549339,\n",
       "        0.0847799 , 0.149479  , 0.12488058, 0.09346262, 0.11038861,\n",
       "        0.11768081, 0.11600743, 0.11109024, 0.12962456, 0.11752099,\n",
       "        0.1316215 , 0.13413225, 0.1104864 , 0.12650968, 0.12058686,\n",
       "        0.09558854, 0.13951807, 0.11657004, 0.13298048, 0.18482056,\n",
       "        0.171295  , 0.1090904 , 0.11234183, 0.11436886, 0.09793836,\n",
       "        0.09859318, 0.21808772, 0.12318791, 0.12430215, 0.13630077,\n",
       "        0.10558093, 0.13644342, 0.10542215, 0.12123218, 0.1490066 ,\n",
       "        0.14937147, 0.09750183, 0.12862898, 0.1368905 , 0.16015   ,\n",
       "        0.08906402, 0.08106572, 0.1171084 , 0.15851426, 0.12665272,\n",
       "        0.10554695, 0.11189257, 0.08149628, 0.12843014, 0.09254942,\n",
       "        0.11056039, 0.09662939, 0.08940607, 0.08156687, 0.10009537,\n",
       "        0.12435541, 0.11951285, 0.12837206, 0.10954696, 0.09711015,\n",
       "        0.09149881, 0.08137891, 0.08404387, 0.15210465, 0.10103087,\n",
       "        0.08203458, 0.1881649 , 0.1435344 , 0.12669677, 0.12804744,\n",
       "        0.11409272, 0.15045018, 0.17236993, 0.13521135, 0.14621364,\n",
       "        0.13101037, 0.15703928, 0.1655344 , 0.10200108, 0.10660499,\n",
       "        0.08026443, 0.17961867, 0.14649424, 0.08449144, 0.10923958,\n",
       "        0.12816514, 0.09245167, 0.10053197, 0.24825075, 0.12010414,\n",
       "        0.1399758 , 0.10437176, 0.14211346, 0.10039683, 0.13902856,\n",
       "        0.08902119, 0.09861863, 0.26507142, 0.15309589, 0.10634527,\n",
       "        0.09893262, 0.12149609, 0.13813631, 0.11972901, 0.0922587 ,\n",
       "        0.10730112, 0.09431988, 0.14614694, 0.10602303, 0.12424491,\n",
       "        0.15275007, 0.14066131, 0.13803017, 0.09378766, 0.11460581,\n",
       "        0.15677191, 0.12438828, 0.15677842, 0.1612176 , 0.12290677,\n",
       "        0.1518346 , 0.09073367, 0.15303676, 0.1132289 , 0.12248324,\n",
       "        0.14714806, 0.11758237, 0.11370991, 0.11041602, 0.05986722,\n",
       "        0.11855657, 0.09619486, 0.12183221, 0.21295391, 0.10092963,\n",
       "        0.11402422, 0.13388775, 0.11951105, 0.13259676, 0.09973697,\n",
       "        0.12875478, 0.0885788 , 0.12812203, 0.1042643 , 0.10093877,\n",
       "        0.1280652 , 0.17397262, 0.13117543, 0.16088127, 0.09226508,\n",
       "        0.15485194, 0.14012523, 0.14390564, 0.11263362, 0.08136196,\n",
       "        0.09883294, 0.11275644, 0.14728887, 0.10077674, 0.16674334,\n",
       "        0.14922197, 0.13566385, 0.12723218, 0.14002886, 0.10884084,\n",
       "        0.12724765, 0.11335091, 0.13025329, 0.12932038, 0.12510505,\n",
       "        0.14308907, 0.14251418, 0.10868978, 0.11747286, 0.11248722,\n",
       "        0.12344842, 0.13025048, 0.19094111, 0.08111787, 0.0644058 ,\n",
       "        0.08928587, 0.128315  , 0.11221828, 0.10587279, 0.1457554 ,\n",
       "        0.09405751, 0.09186068, 0.12030482, 0.13893378, 0.10927469,\n",
       "        0.15462172, 0.08063601, 0.08641917, 0.14468019, 0.13344043,\n",
       "        0.10869296, 0.13899624, 0.13290304, 0.1309989 , 0.11349037,\n",
       "        0.13990517, 0.0947109 , 0.10399655, 0.11776968, 0.17627391,\n",
       "        0.11256105, 0.11275263, 0.13468099, 0.12595959, 0.0941271 ,\n",
       "        0.10570201, 0.10757443, 0.09805446, 0.10175244, 0.11714852,\n",
       "        0.1104935 , 0.11825953, 0.1282963 , 0.14295277, 0.12772356,\n",
       "        0.21165739, 0.13324194, 0.10940864, 0.14808768, 0.10777435,\n",
       "        0.18715592, 0.16175358, 0.15703684, 0.12872465, 0.09886248,\n",
       "        0.13579768, 0.11421081, 0.11343226, 0.0787896 , 0.12925456,\n",
       "        0.16105357, 0.11637436, 0.12194853, 0.1231374 , 0.11875898,\n",
       "        0.17042525, 0.13977599, 0.1130818 , 0.14906983, 0.13591747,\n",
       "        0.14527637, 0.09050788, 0.15861638, 0.09509823, 0.13109824,\n",
       "        0.06911124, 0.13961577, 0.09267959, 0.11509854, 0.15495686,\n",
       "        0.16712183, 0.10385653, 0.10786773, 0.12386355, 0.10290685,\n",
       "        0.08383568, 0.12732232, 0.1232318 , 0.12992087, 0.11160886,\n",
       "        0.18434818, 0.09228688, 0.08932769, 0.14709854, 0.10958675,\n",
       "        0.11336836, 0.12586302, 0.18384251, 0.11359425, 0.10444538,\n",
       "        0.101376  , 0.11747951, 0.13665763, 0.11655596, 0.1823051 ,\n",
       "        0.11064663, 0.14585638, 0.09295224, 0.12710649, 0.15259342,\n",
       "        0.12488902, 0.1693254 , 0.12562664, 0.13201833, 0.1463587 ,\n",
       "        0.10408636, 0.09050066, 0.10112403, 0.09342097, 0.13075599,\n",
       "        0.13410617, 0.10546089, 0.19357537, 0.12786014, 0.1659822 ,\n",
       "        0.16417965, 0.13443658, 0.09972436, 0.13435571, 0.13306886,\n",
       "        0.11011411, 0.10997629, 0.1347616 , 0.16524951, 0.14980415,\n",
       "        0.07715617, 0.15954421, 0.11222996, 0.09580984, 0.15401326,\n",
       "        0.10970166, 0.10404532, 0.15837611, 0.12562807, 0.12245809,\n",
       "        0.13731124, 0.13388582, 0.15602048, 0.11556201, 0.12191871,\n",
       "        0.13025144, 0.13776845, 0.11318446, 0.11999349, 0.10779953,\n",
       "        0.19993676, 0.09083851, 0.16814941, 0.12868355, 0.13232154,\n",
       "        0.11821895, 0.12093081, 0.08528242, 0.10023003, 0.1003834 ,\n",
       "        0.1010385 , 0.23078649, 0.1235964 , 0.11828326, 0.08246563,\n",
       "        0.09422807, 0.09812495, 0.12605718, 0.1486413 , 0.10889944,\n",
       "        0.1483581 , 0.08797419, 0.10918449, 0.10411113, 0.1155329 ,\n",
       "        0.08600601, 0.12545313, 0.1362106 , 0.14364558, 0.11111814,\n",
       "        0.07169639, 0.1178638 , 0.08665301, 0.12770644, 0.11047777,\n",
       "        0.13776855, 0.12184961, 0.17149849, 0.13458619, 0.11904223,\n",
       "        0.14357434, 0.12583527, 0.09855859, 0.10296859, 0.13548006,\n",
       "        0.08216734, 0.14141609, 0.11677179, 0.19226505, 0.09124112,\n",
       "        0.10587086, 0.09446179, 0.10016043, 0.15617283, 0.07606279,\n",
       "        0.09362905, 0.10952775, 0.113964  , 0.12725633, 0.11718585,\n",
       "        0.12955098, 0.10397535, 0.19805339, 0.14029957, 0.17124157,\n",
       "        0.07946068, 0.105905  , 0.14752835, 0.156933  , 0.1164066 ,\n",
       "        0.13254675, 0.12484614, 0.09697074, 0.11919512, 0.1378119 ,\n",
       "        0.13687865, 0.12676005, 0.10363368, 0.12152644, 0.12878135,\n",
       "        0.11789481, 0.08954662, 0.13804707, 0.136377  , 0.12062616,\n",
       "        0.09261388, 0.1372872 , 0.1026328 , 0.11344175, 0.12584735,\n",
       "        0.10685948, 0.11822008, 0.12880771, 0.12999503, 0.15719737,\n",
       "        0.12563036, 0.10771575, 0.14104484, 0.15960547, 0.12303548,\n",
       "        0.14381795, 0.15302183, 0.12792651, 0.09136076, 0.14749241,\n",
       "        0.1465231 , 0.3922177 , 0.11550233, 0.13789007, 0.1091993 ,\n",
       "        0.15150943, 0.10969441, 0.13907343, 0.1292318 , 0.12261238,\n",
       "        0.10656025, 0.15704763, 0.11359234, 0.15421964, 0.14987946,\n",
       "        0.15423141, 0.12008315, 0.09070741, 0.14626862, 0.11407518,\n",
       "        0.12720181, 0.10646055, 0.12322173, 0.09212821, 0.09798822,\n",
       "        0.12779243, 0.15061973, 0.12572691, 0.10528674, 0.10901196,\n",
       "        0.13285024, 0.12106702, 0.12374778, 0.10978152, 0.12477792,\n",
       "        0.16014287, 0.08960352, 0.1316298 , 0.1289515 , 0.13446224,\n",
       "        0.12234539, 0.11529872, 0.09790149, 0.1410102 , 0.10237736,\n",
       "        0.12378354, 0.11718389, 0.11996716, 0.1302628 , 0.14694187,\n",
       "        0.1522042 , 0.09041283, 0.16497599, 0.12439296, 0.12381418,\n",
       "        0.12390196, 0.18153054, 0.10123827, 0.10593772, 0.11815109,\n",
       "        0.11701979, 0.13938032, 0.07356925, 0.13635463, 0.12258106,\n",
       "        0.11276029, 0.14165325, 0.20013806, 0.13843   , 0.1458939 ,\n",
       "        0.09301391, 0.127354  , 0.08278707, 0.10960387, 0.10817333,\n",
       "        0.14754492, 0.14436068, 0.11309695, 0.14050287, 0.11128897,\n",
       "        0.1387596 , 0.13164365, 0.13050552, 0.1340344 , 0.09607339,\n",
       "        0.18233489, 0.0524625 , 0.09745762, 0.20955312, 0.15269688,\n",
       "        0.11233334, 0.10796884, 0.12105163, 0.16126122, 0.13023964,\n",
       "        0.12343252, 0.15219826, 0.09668635, 0.07917993, 0.12376793],\n",
       "       dtype=float32),\n",
       " array([0.5909637 , 0.1900583 , 0.24049371, 0.12412612, 0.25222087,\n",
       "        0.1982358 , 0.1605482 , 0.2478857 , 0.17052934, 0.14732906,\n",
       "        0.21375515, 0.16953985, 0.2035073 , 0.18407123, 0.18490718,\n",
       "        0.19439006, 0.1434199 , 0.1980025 , 0.18639985, 0.15641569,\n",
       "        0.2124111 , 0.12396084, 0.10992549, 0.23128323, 0.15830216,\n",
       "        0.21542066, 0.26525205, 0.21128629, 0.2163716 , 0.14243774,\n",
       "        0.20713416, 0.17456658, 0.16030669, 0.16686776, 0.14643654,\n",
       "        0.15789002, 0.1691806 , 0.11134036, 0.25869775, 0.19392143,\n",
       "        0.12162096, 0.18591887, 0.14354043, 0.20472418, 0.24521974,\n",
       "        0.1643382 , 0.21315359, 0.19742055, 0.31366238, 0.20908462,\n",
       "        0.12325731, 0.12043169, 0.2392276 , 0.23259427, 0.20455803,\n",
       "        0.16595241, 0.27630293, 0.19559965, 0.24908316, 0.21026604,\n",
       "        0.22033253, 0.1369387 , 0.17878546, 0.18667263, 0.16475734,\n",
       "        0.14523037, 0.23123002, 0.21313284, 0.1566432 , 0.3187282 ,\n",
       "        0.1577435 , 0.23884614, 0.13375418, 0.1713931 , 0.15940438,\n",
       "        0.16323271, 0.16539763, 0.10065154, 0.18032528, 0.26696342,\n",
       "        0.14865863, 0.1601934 , 0.18079065, 0.17128278, 0.16819783,\n",
       "        0.16200699, 0.27423412, 0.18133649, 0.16015476, 0.23563904,\n",
       "        0.15162544, 0.15607546, 0.24877559, 0.19174297, 0.22553243,\n",
       "        0.16250622, 0.17988008, 0.27086756, 0.11307364, 0.17915633,\n",
       "        0.26238033, 0.22860645, 0.1744166 , 0.19706523, 0.21301545,\n",
       "        0.20611258, 0.19428985, 0.16433623, 0.20470296, 0.13685301,\n",
       "        0.12920725, 0.20654081, 0.15153427, 0.22384866, 0.17769825,\n",
       "        0.17237009, 0.15234575, 0.17563999, 0.15564716, 0.26426208,\n",
       "        0.12931187, 0.17509168, 0.15748653, 0.12284113, 0.18424325,\n",
       "        0.18314072, 0.23609231, 0.19169877, 0.21620245, 0.21816969,\n",
       "        0.23180203, 0.18005292, 0.37536818, 0.28393635, 0.22641495,\n",
       "        0.20010255, 0.12739733, 0.22426511, 0.15691906, 0.15560499,\n",
       "        0.16484693, 0.19332737, 0.22233143, 0.1688643 , 0.23468627,\n",
       "        0.11949521, 0.21669802, 0.15198861, 0.16073231, 0.17413461,\n",
       "        0.28371263, 0.21046486, 0.17887636, 0.2116747 , 0.16658892,\n",
       "        0.18214235, 0.24006179, 0.20645112, 0.2667715 , 0.18162394,\n",
       "        0.16919093, 0.18525524, 0.19929159, 0.1767563 , 0.17349727,\n",
       "        0.13290238, 0.15428138, 0.15931556, 0.22764447, 0.16773738,\n",
       "        0.18378854, 0.18713884, 0.21949321, 0.16094306, 0.2022576 ,\n",
       "        0.16265568, 0.19631861, 0.15869603, 0.16218552, 0.13811944,\n",
       "        0.16819647, 0.16880399, 0.14827906, 0.19912505, 0.24142885,\n",
       "        0.16499443, 0.21034096, 0.19587083, 0.20675492, 0.17700154,\n",
       "        0.23390858, 0.2540748 , 0.22354162, 0.20147635, 0.19711606,\n",
       "        0.14667815, 0.16053645, 0.1883779 , 0.14547361, 0.09524539,\n",
       "        0.2755723 , 0.15884617, 0.15392011, 0.19483578, 0.22255275,\n",
       "        0.19830506, 0.14516662, 0.19044824, 0.11731445, 0.19341134,\n",
       "        0.2005645 , 0.19598275, 0.17753105, 0.23575927, 0.17046733,\n",
       "        0.18464091, 0.20417681, 0.25637838, 0.21862507, 0.20059109,\n",
       "        0.22545992, 0.26568505, 0.16634648, 0.1483349 , 0.20366731,\n",
       "        0.1535099 , 0.298594  , 0.1883424 , 0.16433668, 0.16859476,\n",
       "        0.18731329, 0.13457914, 0.20913972, 0.1947525 , 0.2460528 ,\n",
       "        0.23152852, 0.18952659, 0.25568536, 0.24444011, 0.19392976,\n",
       "        0.19158979, 0.291577  , 0.1980818 , 0.2028536 , 0.16243452,\n",
       "        0.12696739, 0.18248603, 0.12611574, 0.18847921, 0.16353077,\n",
       "        0.15268423, 0.26109272, 0.16817087, 0.23179433, 0.1452838 ,\n",
       "        0.18393047, 0.16563194, 0.3203486 , 0.21779673, 0.16859268,\n",
       "        0.17056264, 0.1975196 , 0.16516969, 0.24798512, 0.18203788,\n",
       "        0.14758922, 0.17497005, 0.13972256, 0.20362507, 0.20180024,\n",
       "        0.1546407 , 0.15378524, 0.21599305, 0.23656814, 0.1763088 ,\n",
       "        0.21799324, 0.2046315 , 0.28506002, 0.21215306, 0.13390428,\n",
       "        0.13410766, 0.11355228, 0.21470042, 0.17607172, 0.12157454,\n",
       "        0.2047231 , 0.22283193, 0.19454193, 0.16293412, 0.20358604,\n",
       "        0.15701976, 0.2032637 , 0.1730602 , 0.17289115, 0.19814657,\n",
       "        0.22460899, 0.22089884, 0.19324252, 0.22547418, 0.1887693 ,\n",
       "        0.1942965 , 0.15379298, 0.2057076 , 0.14281392, 0.19494899,\n",
       "        0.20021562, 0.19633983, 0.23869535, 0.13974732, 0.1856343 ,\n",
       "        0.27027386, 0.20291121, 0.13222958, 0.24466813, 0.12782018,\n",
       "        0.09724402, 0.17563967, 0.23419908, 0.13367732, 0.17484023,\n",
       "        0.11501761, 0.13741776, 0.13929078, 0.20858686, 0.15442936,\n",
       "        0.11730672, 0.20409968, 0.16695975, 0.20356457, 0.13681921,\n",
       "        0.17922352, 0.16351849, 0.23229222, 0.1820217 , 0.1869745 ,\n",
       "        0.13350445, 0.17859545, 0.31557885, 0.14834967, 0.13947114,\n",
       "        0.22377999, 0.13545837, 0.211876  , 0.22099316, 0.13560355,\n",
       "        0.13134675, 0.15807135, 0.14887665, 0.15926456, 0.21573429,\n",
       "        0.14063214, 0.17534883, 0.16203015, 0.14205056, 0.17703018,\n",
       "        0.1900548 , 0.16687396, 0.18516925, 0.21138844, 0.24096999,\n",
       "        0.17767334, 0.1430691 , 0.14898956, 0.13825089, 0.1159418 ,\n",
       "        0.20896333, 0.21297795, 0.20831287, 0.2093429 , 0.2244847 ,\n",
       "        0.25118873, 0.2345012 , 0.18838598, 0.24818921, 0.25095582,\n",
       "        0.28684148, 0.18854849, 0.15413189, 0.13848045, 0.18044929,\n",
       "        0.21935865, 0.1984229 , 0.19962402, 0.15388758, 0.268003  ,\n",
       "        0.21477771, 0.19180624, 0.30462605, 0.14846693, 0.14545944,\n",
       "        0.27614364, 0.20260672, 0.17196378, 0.24413761, 0.15110931,\n",
       "        0.15179071, 0.20346421, 0.2364577 , 0.17608994, 0.10556559,\n",
       "        0.2680634 , 0.1346646 , 0.24977957, 0.2365608 , 0.13744813,\n",
       "        0.17938629, 0.16007294, 0.19401588, 0.16149712, 0.19266367,\n",
       "        0.1328682 , 0.15901642, 0.15325783, 0.14826868, 0.1401124 ,\n",
       "        0.13997938, 0.20160826, 0.1771214 , 0.16470256, 0.16847083,\n",
       "        0.12791313, 0.15088362, 0.14871798, 0.14684077, 0.1357143 ,\n",
       "        0.1916611 , 0.14807016, 0.18417789, 0.23782714, 0.1507804 ,\n",
       "        0.15545563, 0.2042597 , 0.16484384, 0.23086566, 0.17355277,\n",
       "        0.20416886, 0.18753074, 0.17504074, 0.14453979, 0.16468616,\n",
       "        0.23327067, 0.20633239, 0.1857744 , 0.1512976 , 0.37813112,\n",
       "        0.16458616, 0.12131613, 0.24915005, 0.18880913, 0.22721705,\n",
       "        0.12880433, 0.22964847, 0.19422208, 0.17535278, 0.24541771,\n",
       "        0.22038695, 0.13315716, 0.153771  , 0.18854734, 0.13417523,\n",
       "        0.16633362, 0.20772794, 0.19409004, 0.16821323, 0.12722988,\n",
       "        0.24397156, 0.2109286 , 0.17260173, 0.12960172, 0.18930145,\n",
       "        0.22221583, 0.23491728, 0.17203133, 0.20930536, 0.1589812 ,\n",
       "        0.21892165, 0.18130545, 0.1248884 , 0.19626768, 0.20595208,\n",
       "        0.11769161, 0.20145163, 0.14514737, 0.17187744, 0.2829999 ,\n",
       "        0.17422396, 0.2202924 , 0.16747864, 0.21580674, 0.18148357,\n",
       "        0.21529193, 0.10594009, 0.17018525, 0.1755538 , 0.163459  ,\n",
       "        0.20359296, 0.20981327, 0.18030001, 0.18455571, 0.13334692,\n",
       "        0.15957381, 0.23011358, 0.24785227, 0.22775997, 0.15079118,\n",
       "        0.24215233, 0.15846105, 0.11964961, 0.19058508, 0.1603049 ,\n",
       "        0.14552675, 0.21206233, 0.23046894, 0.53236294, 0.22711675,\n",
       "        0.28166354, 0.1790888 , 0.13683914, 0.22287083, 0.22603516,\n",
       "        0.16773294, 0.19449417, 0.13870187, 0.17886549, 0.21853483,\n",
       "        0.21235484, 0.23894441, 0.13213938, 0.18615545, 0.1616884 ,\n",
       "        0.1886982 , 0.17828426, 0.14877318, 0.21642347, 0.13990732,\n",
       "        0.12286791, 0.26461455, 0.15166095, 0.13876066, 0.15835784,\n",
       "        0.1645634 , 0.17101988, 0.14218451, 0.14804372, 0.18766722,\n",
       "        0.20168674, 0.19959205, 0.21225032, 0.18046883, 0.15550989,\n",
       "        0.16197251, 0.16298859, 0.19850332, 0.12819915, 0.2105654 ,\n",
       "        0.17473038, 0.19804269, 0.21673912, 0.20041075, 0.18352234,\n",
       "        0.19634853, 0.16958095, 0.18506426, 0.1542239 , 0.21468627,\n",
       "        0.16495027, 0.19860393, 0.1736977 , 0.19884935, 0.19360974,\n",
       "        0.19056386, 0.16397424, 0.14301845, 0.13417527, 0.22525331,\n",
       "        0.12870412, 0.19795273, 0.15962303, 0.15241562, 0.22256014,\n",
       "        0.14554507, 0.170593  , 0.13309185, 0.19342212, 0.12135769,\n",
       "        0.14645277, 0.2648569 , 0.1760894 , 0.18022853, 0.26929173,\n",
       "        0.15511164, 0.18581167, 0.16476503, 0.2156516 , 0.18716021,\n",
       "        0.12693882, 0.12540978, 0.14650235, 0.26246282, 0.1381126 ,\n",
       "        0.29428247, 0.17186159, 0.19176126, 0.20303914, 0.2301994 ,\n",
       "        0.21982734, 0.17633647, 0.16384101, 0.2368348 , 0.11627393,\n",
       "        0.18748902, 0.21923539, 0.14521241, 0.1678889 , 0.19299084,\n",
       "        0.17257695, 0.1745682 , 0.15942109, 0.20547245, 0.12492678,\n",
       "        0.18545887, 0.15970631, 0.14925204, 0.1580173 , 0.20005006,\n",
       "        0.17685543, 0.1789515 , 0.19587392, 0.19502169, 0.14903153,\n",
       "        0.17404017, 0.17863914, 0.1955058 , 0.15411134, 0.20189232,\n",
       "        0.20436162, 0.20237358, 0.17850235, 0.15797424, 0.15701462,\n",
       "        0.18977194, 0.15822816, 0.14950341, 0.11228262, 0.17815977,\n",
       "        0.11698249, 0.20770712, 0.2298634 , 0.11603762, 0.15939799,\n",
       "        0.31870583, 0.14645784, 0.17817385, 0.18541151, 0.1890942 ,\n",
       "        0.17519288, 0.2159121 , 0.2145839 , 0.2078352 , 0.25922897,\n",
       "        0.43537825, 0.16382584, 0.21650797, 0.1610604 , 0.22192569,\n",
       "        0.28418338, 0.13216889, 0.1747246 , 0.24571796, 0.19242333,\n",
       "        0.18162704, 0.18577504, 0.1522501 , 0.18505298, 0.19506352,\n",
       "        0.17392403, 0.12118074, 0.2095191 , 0.16566709, 0.19085084,\n",
       "        0.12618232, 0.17372811, 0.2346006 , 0.13551895, 0.22223929,\n",
       "        0.22936925, 0.21825266, 0.19453071, 0.09711681, 0.18101525,\n",
       "        0.29092574, 0.16739596, 0.13037567, 0.09573983, 0.20266524,\n",
       "        0.2289024 , 0.19150028, 0.281305  , 0.1840893 , 0.19682676,\n",
       "        0.20323299, 0.20084263, 0.14292921, 0.18623097, 0.23168418,\n",
       "        0.1537816 , 0.1541465 , 0.214363  , 0.17974395, 0.20242418,\n",
       "        0.2421673 , 0.1775281 , 0.15768003, 0.25272116, 0.14792968,\n",
       "        0.21020834, 0.17349929, 0.16465822, 0.2269943 , 0.16785266,\n",
       "        0.22138704, 0.19600421, 0.2660093 , 0.1495367 , 0.19173843,\n",
       "        0.14275728, 0.27223712, 0.19863616, 0.19168212, 0.21726502,\n",
       "        0.19627246, 0.22841318, 0.19006263, 0.17349431, 0.18786086,\n",
       "        0.18699147, 0.19095477, 0.16629197, 0.19133264, 0.13430014,\n",
       "        0.15794522, 0.20029157, 0.14995107, 0.15577674, 0.17871912,\n",
       "        0.25897622, 0.23351075, 0.15803143, 0.16250023, 0.20212789,\n",
       "        0.17595203, 0.15952173, 0.2544805 , 0.1354299 , 0.13317372,\n",
       "        0.2762764 , 0.13552041, 0.18364665, 0.19543548, 0.14630835,\n",
       "        0.20795977, 0.18842125, 0.1906222 , 0.23158577, 0.19148411,\n",
       "        0.13253668, 0.3601854 , 0.2091966 , 0.1798019 , 0.17224851,\n",
       "        0.19914919, 0.19857687, 0.19389287, 0.1496893 , 0.14256841,\n",
       "        0.20810348, 0.19649662, 0.13968976, 0.19617094, 0.1958118 ,\n",
       "        0.23426998, 0.15752517, 0.18174404, 0.15387526, 0.21750839,\n",
       "        0.21118268, 0.20824087, 0.21344008, 0.11898719, 0.2072037 ,\n",
       "        0.18212411, 0.22404146, 0.2982865 , 0.1364143 , 0.19487865,\n",
       "        0.18862103, 0.19912758, 0.18571503, 0.20652723, 0.19461021,\n",
       "        0.2197143 , 0.20692933, 0.15042327, 0.21688573, 0.27583435,\n",
       "        0.15841259, 0.24726506, 0.17986146, 0.12812884, 0.23779576,\n",
       "        0.19069308, 0.1822079 , 0.19312397, 0.16456756, 0.16474615,\n",
       "        0.19471721, 0.20832904, 0.139345  , 0.32773098, 0.18170255,\n",
       "        0.20993282, 0.18247178, 0.14677347, 0.2037582 , 0.18656379,\n",
       "        0.17990737, 0.10470241, 0.19671988, 0.17409636, 0.19552039,\n",
       "        0.29714474, 0.16202839, 0.23695506, 0.1762126 , 0.20249666,\n",
       "        0.19771978, 0.20448107, 0.15763758, 0.15048453, 0.17814192,\n",
       "        0.2678058 , 0.16990848, 0.1936975 , 0.19891343, 0.13937485,\n",
       "        0.22995093, 0.16099066, 0.13518454, 0.25971574, 0.1704241 ,\n",
       "        0.16567788, 0.13203299, 0.10887737, 0.19393449, 0.18225156,\n",
       "        0.10198882, 0.12001699, 0.17172024, 0.2219049 , 0.15849946,\n",
       "        0.1522683 , 0.92327476, 0.25834724, 0.18741173, 0.23085643,\n",
       "        0.13121383, 0.2046392 , 0.17785195, 0.22483483, 0.17669658,\n",
       "        0.25739637, 0.12853739, 0.14997545, 0.11879586, 0.11413115,\n",
       "        0.26088032, 0.11847555, 0.22347301, 0.11802669, 0.2297118 ,\n",
       "        0.12896891, 0.14915784, 0.18996242, 0.19447859, 0.22028634,\n",
       "        0.17778634, 0.20713097, 0.19249593, 0.14633256, 0.20087595,\n",
       "        0.2106448 , 0.19070095, 0.13649464, 0.170374  , 0.12138855,\n",
       "        0.1532797 , 0.2024483 , 0.15925997, 0.15293562, 0.17929064,\n",
       "        0.1563601 , 0.17710003, 0.18861489, 0.14132176, 0.15821539,\n",
       "        0.17257972, 0.16396448, 0.15999478, 0.2201958 , 0.14384352,\n",
       "        0.26283884, 0.19065215, 0.2208822 , 0.25619692, 0.19564356,\n",
       "        0.21278302, 0.17662102, 0.15596986, 0.21673162, 0.23511663,\n",
       "        0.23009552, 0.19490933, 0.22592749, 0.1380183 , 0.1674019 ,\n",
       "        0.13900614, 0.15798284, 0.27524814, 0.29978108, 0.17232203,\n",
       "        0.21825314, 0.214814  , 0.2404284 , 0.13506933, 0.1900371 ,\n",
       "        0.16839877, 0.15728588, 0.185347  , 0.20819841, 0.19375129,\n",
       "        0.17837603, 0.1699994 , 0.19546661, 0.1980738 , 0.20651989,\n",
       "        0.16432177, 0.21320659, 0.22166817, 0.20404956, 0.16309315,\n",
       "        0.15958002, 0.16100305, 0.15621492, 0.14451405, 0.18636073,\n",
       "        0.18533763, 0.19688584, 0.16150779, 0.15951799, 0.16381907],\n",
       "       dtype=float32),\n",
       " array([0.2344162 , 0.2573145 , 0.15570827, 0.24532828, 0.16463551,\n",
       "        0.21340898, 0.23614174, 0.197639  , 0.1606546 , 0.26391205,\n",
       "        0.3613326 , 0.29299834, 0.1976277 , 0.24882841, 0.2244795 ,\n",
       "        0.1521301 , 0.18484542, 0.21128404, 0.16233005, 0.17857414,\n",
       "        0.17335822, 0.29498   , 0.20408909, 0.19207412, 0.15421467,\n",
       "        0.31371254, 0.2196623 , 0.16763858, 0.24591994, 0.27625474,\n",
       "        0.28039476, 0.22087193, 0.17429234, 0.1804374 , 0.18801516,\n",
       "        0.16424838, 0.23696588, 0.27276164, 0.19274285, 0.27884743,\n",
       "        0.17960975, 0.16180576, 0.2672458 , 0.2106166 , 0.19315839,\n",
       "        0.23787723, 0.3011425 , 0.15803492, 0.17793985, 0.21824469,\n",
       "        0.22509627, 0.20100276, 0.21478873, 0.18260542, 0.18103042,\n",
       "        0.31159487, 0.18771264, 0.18238957, 0.14116548, 0.27858946,\n",
       "        0.16861354, 0.21230538, 0.15744069, 0.20698847, 0.1798795 ,\n",
       "        0.22670048, 0.2273255 , 0.23189664, 0.20373431, 0.18362087,\n",
       "        0.19376093, 0.14648634, 0.20179628, 0.15832257, 0.1776994 ,\n",
       "        0.2280399 , 0.21359533, 0.22985414, 0.17225796, 0.19909674,\n",
       "        0.22962381, 0.1953366 , 0.2536136 , 0.22662894, 0.18425435,\n",
       "        0.20508645, 0.16999361, 0.15410987, 0.18831307, 0.16942364,\n",
       "        0.21568885, 0.17138876, 0.18515034, 0.23682858, 0.17031847,\n",
       "        0.29665467, 0.19861248, 0.19117591, 0.20966062, 0.17260216,\n",
       "        0.29217932, 0.183765  , 0.21333313, 0.2837702 , 0.21133329,\n",
       "        0.2771848 , 0.17032437, 0.22273017, 0.20020127, 0.24549924,\n",
       "        0.21655345, 0.21781208, 0.22259268, 0.20576893, 0.19143786,\n",
       "        0.18165104, 0.25316137, 0.20551579, 0.19456552, 0.17736255,\n",
       "        0.16564995, 0.26203385, 0.19813913, 0.21878777, 0.24418506,\n",
       "        0.19294827, 0.21190141, 0.18855803, 0.1981762 , 0.23435645,\n",
       "        0.18872608, 0.20312627, 0.19560367, 0.19269225, 0.1584989 ,\n",
       "        0.21100497, 0.17538634, 0.21923453, 0.2021821 , 0.24117638,\n",
       "        0.18563055, 0.17306948, 0.16882817, 0.15920408, 0.20489474,\n",
       "        0.20898567, 0.30063805, 0.18936887, 0.21729578, 0.25983638,\n",
       "        0.29525262, 0.3538843 , 0.14761111, 0.19710219, 0.49722636,\n",
       "        0.275949  , 0.17401403, 0.2372583 , 0.18153806, 0.20266233],\n",
       "       dtype=float32),\n",
       " array([0.1002776 , 0.11978293, 0.09437257, 0.14649296, 0.19868347,\n",
       "        0.12521768, 0.13429314, 0.0787845 , 0.09402154, 0.15533312,\n",
       "        0.1808928 , 0.12673375, 0.11328213, 0.10829964, 0.10883931,\n",
       "        0.11092217, 0.08361092, 0.14203672, 0.09710864, 0.12628001,\n",
       "        0.12884721, 0.10768609, 0.14559127, 0.11035499, 0.11856821,\n",
       "        0.12591097, 0.12485337, 0.09614601, 0.1651882 , 0.14878528,\n",
       "        0.09151232, 0.19507082, 0.10212338, 0.18764602, 0.07364551,\n",
       "        0.09836027, 0.18771426, 0.1302153 , 0.10668726, 0.13516708,\n",
       "        0.14430311, 0.12169417, 0.13244082, 0.13276012, 0.17097633,\n",
       "        0.11562071, 0.12564607, 0.14555171, 0.17274691, 0.18709905,\n",
       "        0.1374164 , 0.15266532, 0.18233901, 0.19302872, 0.19959779,\n",
       "        0.0861233 , 0.09846985, 0.15017606, 0.07744125, 0.0878482 ,\n",
       "        0.15006189, 0.16946779, 0.11361035, 0.13764066, 0.125031  ,\n",
       "        0.09345479, 0.17251563, 0.21258172, 0.11588665, 0.15950018,\n",
       "        0.12045859, 0.10971818, 0.13892595, 0.22896686, 0.09710809,\n",
       "        0.1476391 , 0.15649126, 0.1271463 , 0.19444634, 0.16350506,\n",
       "        0.0019469 , 0.13569884, 0.12430686, 0.1257427 , 0.12675826,\n",
       "        0.12648897, 0.1559271 , 0.10333321, 0.12255194, 0.10950209,\n",
       "        0.11059321, 0.12634793, 0.1280877 , 0.1445563 , 0.11124176,\n",
       "        0.16416976, 0.15703279, 0.10697395, 0.18011224, 0.09610809,\n",
       "        0.1306564 , 0.12468699, 0.13722476, 0.18075885, 0.14163953,\n",
       "        0.12593548, 0.10706553, 0.16759315, 0.10291077, 0.08364885,\n",
       "        0.13751501, 0.11162865, 0.16716772, 0.08851365, 0.07593724,\n",
       "        0.15178318, 0.11437911, 0.12286828, 0.12938748, 0.11409787,\n",
       "        0.13645297, 0.10302379, 0.1452882 , 0.12120972, 0.1288598 ,\n",
       "        0.15759844, 0.12444941, 0.08885947, 0.12153828, 0.10371991,\n",
       "        0.10088072, 0.10865685, 0.11091708, 0.10745776, 0.12298587,\n",
       "        0.13283557, 0.12075377, 0.12550938, 0.13124841, 0.14558609,\n",
       "        0.15707402, 0.15284431, 0.14377542, 0.10580862, 0.13301487,\n",
       "        0.08948587, 0.08590252, 0.1001314 , 0.12433816, 0.19185826,\n",
       "        0.12808956, 0.13268195, 0.11602531, 0.1354099 , 0.1613886 ,\n",
       "        0.13378412, 0.15842801, 0.11735148, 0.14708081, 0.12419767,\n",
       "        0.1225426 , 0.08107426, 0.10645125, 0.1419782 , 0.18543787,\n",
       "        0.13396168, 0.1691406 , 0.14774184, 0.13153942, 0.1094588 ,\n",
       "        0.14647721, 0.12194812, 0.15808421, 0.1969449 , 0.12522267,\n",
       "        0.12094335, 0.12474844, 0.11191916, 0.13815811, 0.0573219 ,\n",
       "        0.11545116, 0.1328019 , 0.15108946, 0.11679213, 0.13856624,\n",
       "        0.13076638, 0.14827114, 0.09453638, 0.13341765, 0.16196096,\n",
       "        0.092743  , 0.13428155, 0.129558  , 0.15056315, 0.12445277,\n",
       "        0.14123894, 0.10466223, 0.09926841, 0.10171082, 0.1052596 ,\n",
       "        0.10645243, 0.11420375, 0.10832719, 0.12617764, 0.0958797 ,\n",
       "        0.17251058, 0.23800501, 0.14427947, 0.08558635, 0.10939665,\n",
       "        0.11631111, 0.11435831, 0.11530276, 0.10557218, 0.17754771,\n",
       "        0.11972746, 0.12642416, 0.14132673, 0.16139339, 0.18439458,\n",
       "        0.09462285, 0.08976316, 0.09474301, 0.14685188, 0.16068898,\n",
       "        0.13005106, 0.23248751, 0.12436453, 0.10673022, 0.19003625,\n",
       "        0.2197097 , 0.13416836, 0.09591676, 0.25882518, 0.12238715,\n",
       "        0.11627995, 0.1508305 , 0.15802583, 0.11739298, 0.08379555,\n",
       "        0.10645229, 0.1462372 , 0.11070138, 0.12823841, 0.08300665,\n",
       "        0.12156218, 0.10286225, 0.13021147, 0.1325344 , 0.11889268,\n",
       "        0.22340569, 0.09707867, 0.08150888, 0.15303172, 0.11847671,\n",
       "        0.10130038, 0.15949273, 0.15258592, 0.08036033, 0.0865783 ,\n",
       "        0.08630102, 0.16151549, 0.13396738, 0.1175442 , 0.08979622,\n",
       "        0.1319714 , 0.12488732, 0.13347536, 0.19816883, 0.13179693,\n",
       "        0.1243971 , 0.16539215, 0.12923142, 0.09734394, 0.15984015,\n",
       "        0.13345672, 0.12800175, 0.10369704, 0.12776627, 0.12923045,\n",
       "        0.15378007, 0.10746393, 0.21095893, 0.16403152, 0.10686924,\n",
       "        0.08395591, 0.14716502, 0.13998336, 0.11883628, 0.1263494 ,\n",
       "        0.14499956, 0.0871912 , 0.13597567, 0.0911667 , 0.10254646,\n",
       "        0.11871433, 0.12126467, 0.1046736 , 0.16626833, 0.08169192,\n",
       "        0.11378086, 0.1403666 , 0.0844954 , 0.1158428 , 0.14931583,\n",
       "        0.13522902, 0.09312614, 0.1101964 , 0.10305553, 0.11568187,\n",
       "        0.13897109, 0.08690083, 0.16489176, 0.20819348, 0.1116145 ,\n",
       "        0.12609844, 0.10060362, 0.14421308, 0.15017608, 0.1274552 ,\n",
       "        0.09406845, 0.10417149, 0.09448426, 0.14807072, 0.11051668,\n",
       "        0.23739189, 0.10186112, 0.14374915, 0.14115524, 0.22683659,\n",
       "        0.11046264, 0.14133704, 0.08772724, 0.12733461, 0.11848682,\n",
       "        0.1188415 , 0.12731701, 0.17275256, 0.13611831, 0.1988668 ,\n",
       "        0.11380923, 0.09380748, 0.12028201, 0.10580569, 0.10402771,\n",
       "        0.13398986, 0.13580616, 0.17236784, 0.00326442, 0.09032037,\n",
       "        0.09915859, 0.15839744, 0.14729369, 0.14304335, 0.09567365,\n",
       "        0.14382385, 0.09651058, 0.13226129, 0.1365103 , 0.18556926,\n",
       "        0.13685286, 0.20319718, 0.12282284, 0.13194922, 0.08390205,\n",
       "        0.1362459 , 0.10364258, 0.14584564, 0.1422274 , 0.1580609 ,\n",
       "        0.13291736, 0.10251546, 0.14566272, 0.12721854, 0.13941708,\n",
       "        0.13714772, 0.14975326, 0.17815684, 0.1295937 , 0.10580758,\n",
       "        0.13413942, 0.10978819, 0.14525783, 0.16734241, 0.11188745,\n",
       "        0.18881406, 0.11460473, 0.1308089 , 0.11766858, 0.07822733,\n",
       "        0.21238174, 0.0941365 , 0.15674703, 0.13093826, 0.11591393,\n",
       "        0.07160649, 0.11670024, 0.09269586, 0.10734689, 0.12870291,\n",
       "        0.13818   , 0.14229622, 0.15156752, 0.12274645, 0.00187577,\n",
       "        0.14419717, 0.07212255, 0.14821824, 0.16529031, 0.11839106,\n",
       "        0.12978014, 0.19161043, 0.09711687, 0.07340733, 0.17186171,\n",
       "        0.14709428, 0.12138856, 0.12056571, 0.15370457, 0.09761737,\n",
       "        0.138596  , 0.09206437, 0.10409027, 0.1085787 , 0.16508687,\n",
       "        0.12480881, 0.089224  , 0.15995517, 0.12339731, 0.14910978,\n",
       "        0.17396298, 0.0778634 , 0.11777902, 0.08894599, 0.12059736,\n",
       "        0.17258923, 0.14002004, 0.1545774 , 0.0618214 , 0.12213473,\n",
       "        0.09677665, 0.15240408, 0.11573189, 0.08373696, 0.13459796,\n",
       "        0.12975264, 0.10129889, 0.1286111 , 0.16806434, 0.24731292,\n",
       "        0.12112622, 0.10013106, 0.18109746, 0.13035548, 0.19800417,\n",
       "        0.08977588, 0.10689261, 0.11900268, 0.21087603, 0.11907197,\n",
       "        0.16132613, 0.17628628, 0.10058481, 0.17714739, 0.13345984,\n",
       "        0.12051675, 0.11405815, 0.10653527, 0.11961838, 0.1189908 ,\n",
       "        0.13341567, 0.13626967, 0.08713967, 0.12802722, 0.13973282,\n",
       "        0.14639682, 0.21647973, 0.12079458, 0.11202474, 0.09106898,\n",
       "        0.11323842, 0.12780593, 0.15696183, 0.11816439, 0.11648915,\n",
       "        0.11071821, 0.13812697, 0.12774801, 0.12582776, 0.11393991,\n",
       "        0.09549681, 0.11947662, 0.15592465, 0.16027252, 0.17718263,\n",
       "        0.10933504, 0.1201136 , 0.16698441, 0.08571362, 0.16677919,\n",
       "        0.10893822, 0.12761857, 0.11934295, 0.07798393, 0.1782164 ,\n",
       "        0.13861986, 0.13507085, 0.15147483, 0.10354041, 0.12113254,\n",
       "        0.12561545, 0.14029355, 0.10085576, 0.15348502, 0.12117435,\n",
       "        0.11202835, 0.11123016, 0.11096079, 0.12848195, 0.12492637,\n",
       "        0.13439554, 0.14580032, 0.13585198, 0.10678847, 0.15253365,\n",
       "        0.11641764, 0.14175248, 0.14941928, 0.17018257, 0.13199683,\n",
       "        0.12254497, 0.16172777, 0.12599248, 0.11213975, 0.11107109,\n",
       "        0.10677432, 0.18329255, 0.11187546, 0.1434202 , 0.20296006,\n",
       "        0.1363737 , 0.18763834, 0.10911991, 0.1132445 , 0.16270903,\n",
       "        0.10252597, 0.11208374, 0.11869758, 0.10134224, 0.13521709,\n",
       "        0.12572016, 0.14415638, 0.08352752, 0.14827654, 0.22060114,\n",
       "        0.24629796, 0.10738862, 0.1367993 , 0.17367281, 0.08622558,\n",
       "        0.21374553, 0.15258259, 0.14296238, 0.10522871, 0.09787648,\n",
       "        0.14977036, 0.19823037, 0.09483392, 0.2432836 , 0.10392368,\n",
       "        0.08511733, 0.11028613, 0.1788494 , 0.08720569, 0.16571873,\n",
       "        0.13756825, 0.1603861 , 0.10405131, 0.12675694, 0.12531176,\n",
       "        0.12917729, 0.23244366, 0.09940109, 0.12914355, 0.22242548,\n",
       "        0.05812552, 0.11568693, 0.11056326, 0.15220864, 0.10823524,\n",
       "        0.12806167, 0.10421836, 0.11793909, 0.14745833, 0.12004261,\n",
       "        0.11132558, 0.09435876, 0.09857874, 0.08882037, 0.08219837,\n",
       "        0.13910429, 0.14453572, 0.11292127, 0.07758741, 0.11879276,\n",
       "        0.20391427, 0.11448919, 0.1468278 , 0.12416544, 0.11031923,\n",
       "        0.18313244, 0.12590791, 0.11459342, 0.1940897 , 0.27225074,\n",
       "        0.16912526, 0.1202787 , 0.13688529, 0.1420341 , 0.14587894,\n",
       "        0.09360567, 0.13773118, 0.17443316, 0.19741063, 0.13977998,\n",
       "        0.1326531 , 0.09951526, 0.10868125, 0.09440202, 0.14212056,\n",
       "        0.18942058, 0.14554925, 0.21761571, 0.1842052 , 0.15473875,\n",
       "        0.15986198, 0.12239202, 0.11231803, 0.1164379 , 0.0918741 ,\n",
       "        0.10946723, 0.13133217, 0.11992363, 0.07484397, 0.09619737,\n",
       "        0.21751048, 0.10421044, 0.13097204, 0.10823873, 0.14669731,\n",
       "        0.10803179, 0.10974988, 0.08866899, 0.19266844, 0.10768379,\n",
       "        0.114025  , 0.10790523, 0.09030019, 0.24549744, 0.13538314,\n",
       "        0.103935  , 0.12612793, 0.09162105, 0.10339855, 0.22081083,\n",
       "        0.16488801, 0.12226484, 0.11454054, 0.11438523, 0.11709857,\n",
       "        0.16428685, 0.11421122, 0.1421357 , 0.13004582, 0.17121503,\n",
       "        0.09228737, 0.20901388, 0.10077944, 0.1915062 , 0.13060607,\n",
       "        0.12752275, 0.10807986, 0.15450901, 0.09976479, 0.13743295,\n",
       "        0.0882116 , 0.10311662, 0.10254093, 0.13763255, 0.14318052,\n",
       "        0.13113502, 0.12581646, 0.08264098, 0.10100854, 0.1392941 ,\n",
       "        0.13305186, 0.11958256, 0.11917593, 0.14519927, 0.19814308,\n",
       "        0.13675067, 0.09082451, 0.09769301, 0.16961972, 0.1158869 ,\n",
       "        0.11717875, 0.09173504, 0.08513974, 0.18120854, 0.14742446,\n",
       "        0.11023049, 0.14132255, 0.10115763, 0.10005815, 0.14799026,\n",
       "        0.09337317, 0.11974734, 0.08718241, 0.103077  , 0.16299902,\n",
       "        0.08741664, 0.1480488 , 0.16697203, 0.12524466, 0.14267762,\n",
       "        0.08733839, 0.12388584, 0.14813451, 0.14817873, 0.12766849,\n",
       "        0.14882377, 0.09872808, 0.14928532, 0.14788891, 0.15503494,\n",
       "        0.10729102, 0.14945374, 0.09423397, 0.12656872, 0.11846658,\n",
       "        0.08425246, 0.14651752, 0.17641552, 0.08590648, 0.09403558,\n",
       "        0.10057053, 0.1368224 , 0.11222958, 0.1998888 , 0.20687418,\n",
       "        0.11572992, 0.09108585, 0.15937877, 0.10948574, 0.13461402,\n",
       "        0.12669973, 0.13581981, 0.10806904, 0.11018054, 0.14587681,\n",
       "        0.13913637, 0.10329963, 0.1629519 , 0.0964755 , 0.10520142,\n",
       "        0.09889545, 0.10721359, 0.09265587, 0.11399015, 0.12256644,\n",
       "        0.12277732, 0.12593552, 0.16633448, 0.08299245, 0.12519996,\n",
       "        0.04820725, 0.12055837, 0.19883265, 0.11533355, 0.11264975,\n",
       "        0.09618319, 0.17159033, 0.13494928, 0.17204766, 0.12029986,\n",
       "        0.14805765, 0.14632042, 0.11965888, 0.1234137 , 0.1276782 ,\n",
       "        0.1392599 , 0.15029667, 0.11013404, 0.13625352, 0.10091861,\n",
       "        0.10949352, 0.17288323, 0.19576061, 0.17104809, 0.08573173,\n",
       "        0.08982343, 0.12457861, 0.16970392, 0.1693518 , 0.09938373,\n",
       "        0.07743055, 0.13383809, 0.15410453, 0.08015715, 0.09089285,\n",
       "        0.11122426, 0.07479261, 0.07311657, 0.13615258, 0.00186457,\n",
       "        0.16740121, 0.11273079, 0.2574911 , 0.10673025, 0.14724368,\n",
       "        0.1253446 , 0.09767924, 0.13734376, 0.13147838, 0.18857649,\n",
       "        0.1451199 , 0.08840833, 0.13017154, 0.25150636, 0.15817972,\n",
       "        0.1556124 , 0.126247  , 0.08829595, 0.17272308, 0.12615925,\n",
       "        0.08653829, 0.09681552, 0.14973022, 0.12259684, 0.14215012,\n",
       "        0.1086343 , 0.10864174, 0.16330986, 0.11321586, 0.14733163,\n",
       "        0.23514387, 0.16673347, 0.1838318 , 0.16102093, 0.12866762,\n",
       "        0.19207637, 0.09285868, 0.2177757 , 0.09186468, 0.12514752,\n",
       "        0.12797469, 0.11087126, 0.1567078 , 0.13735649, 0.15007496,\n",
       "        0.12315395, 0.11583212, 0.13309985, 0.13902089, 0.0872096 ,\n",
       "        0.12814675, 0.17573906, 0.15735137, 0.04585299, 0.15059276,\n",
       "        0.09696217, 0.19089518, 0.1436862 , 0.11601971, 0.08481986,\n",
       "        0.14801267, 0.08652993, 0.1116892 , 0.11495868, 0.08966561,\n",
       "        0.12890567, 0.14030814, 0.13656518, 0.19809356, 0.11888096,\n",
       "        0.14825006, 0.15301228, 0.09894133, 0.07715814, 0.12744862,\n",
       "        0.13759312, 0.09645116, 0.12350677, 0.1559398 , 0.09045226,\n",
       "        0.1530832 , 0.13838166, 0.12850514, 0.2361162 , 0.15044303,\n",
       "        0.08851389, 0.11366676, 0.13012476, 0.17304756, 0.08996537,\n",
       "        0.12548622, 0.15274344, 0.17613132, 0.18667613, 0.12947668,\n",
       "        0.11184105, 0.10942031, 0.11122469, 0.26583195, 0.08473212,\n",
       "        0.18395844, 0.17102689, 0.15836048, 0.12513581, 0.11685505,\n",
       "        0.19868565, 0.14177842, 0.12147795, 0.11047721, 0.1032472 ,\n",
       "        0.12082778, 0.1069094 , 0.17067304, 0.1397429 , 0.14494054,\n",
       "        0.10500439, 0.12697554, 0.12000496, 0.14067085, 0.13510638,\n",
       "        0.09700166, 0.098542  , 0.12235741, 0.15128054, 0.0905629 ,\n",
       "        0.15408331, 0.10725845, 0.11834309, 0.14078526, 0.20116332,\n",
       "        0.08627521, 0.11568949, 0.14888036, 0.14611614, 0.1808201 ,\n",
       "        0.10272986, 0.12408287, 0.11388378, 0.15663442, 0.17296034,\n",
       "        0.11327399, 0.12993182, 0.13369393, 0.12595911, 0.18272656],\n",
       "       dtype=float32),\n",
       " array([0.14641388, 0.20813228, 0.23747218, 0.18599337, 0.2906449 ,\n",
       "        0.17039372, 0.25498343, 0.13584779, 0.20513117, 0.11376486,\n",
       "        0.12274639, 0.15931657, 0.19173826, 0.19136532, 0.15298343,\n",
       "        0.16317183, 0.2708293 , 0.17155805, 0.18486592, 0.15894772,\n",
       "        0.16468239, 0.2294396 , 0.24081184, 0.17522155, 0.15354992,\n",
       "        0.18570757, 0.16145016, 0.23405622, 0.20583096, 0.22052082,\n",
       "        0.13263953, 0.19241455, 0.21464415, 0.27559394, 0.22936708,\n",
       "        0.29152003, 0.40863365, 0.19625476, 0.18568815, 0.10600632,\n",
       "        0.22509213, 0.17435057, 0.168199  , 0.22659911, 0.20265007,\n",
       "        0.16249217, 0.18434975, 0.16993465, 0.13517334, 0.26105997,\n",
       "        0.17194134, 0.1405405 , 0.23407812, 0.2890785 , 0.2874561 ,\n",
       "        0.22762382, 0.22704113, 0.15991935, 0.25872236, 0.13977922,\n",
       "        0.16313441, 0.14632405, 0.22003566, 0.16533174, 0.20172448,\n",
       "        0.20606846, 0.2896718 , 0.2023091 , 0.21050273, 0.14431979,\n",
       "        0.1690795 , 0.23107849, 0.12634967, 0.31318882, 0.22474846,\n",
       "        0.12557161, 0.170529  , 0.16377705, 0.22515279, 0.19063102,\n",
       "        0.01059278, 0.22865303, 0.26583144, 0.1525252 , 0.20124762,\n",
       "        0.21621194, 0.27770388, 0.15143667, 0.17607278, 0.2228694 ,\n",
       "        0.18153304, 0.18361406, 0.24922468, 0.15510358, 0.19637452,\n",
       "        0.35697818, 0.1993293 , 0.10197197, 0.28073493, 0.13427113,\n",
       "        0.20247276, 0.16985758, 0.15735295, 0.14621463, 0.16875404,\n",
       "        0.14165601, 0.18173113, 0.15180059, 0.16281271, 0.21892911,\n",
       "        0.15202482, 0.17566077, 0.23819548, 0.1607431 , 0.27551353,\n",
       "        0.11353238, 0.19206436, 0.16052042, 0.16277204, 0.1825921 ,\n",
       "        0.14585695, 0.16785742, 0.14741437, 0.21795377, 0.21502922,\n",
       "        0.25883242, 0.1824954 , 0.22334415, 0.14282736, 0.23423691,\n",
       "        0.19201016, 0.28818905, 0.23725678, 0.12939121, 0.14059481,\n",
       "        0.14866605, 0.16137259, 0.23487324, 0.16954806, 0.19779712,\n",
       "        0.2317123 , 0.24482404, 0.14605738, 0.17827368, 0.16023694,\n",
       "        0.16214776, 0.2655468 , 0.19098409, 0.23841953, 0.20680436,\n",
       "        0.18245526, 0.16102543, 0.11612438, 0.29547846, 0.21116434,\n",
       "        0.12491199, 0.1470369 , 0.14667095, 0.19959533, 0.21254736,\n",
       "        0.16369851, 0.15322466, 0.18098582, 0.1600046 , 0.39929724,\n",
       "        0.18577604, 0.27144474, 0.26880273, 0.14794146, 0.11521393,\n",
       "        0.16227348, 0.16177832, 0.23889698, 0.1443493 , 0.15031691,\n",
       "        0.20989326, 0.14330661, 0.16401836, 0.13448802, 0.19363748,\n",
       "        0.18161502, 0.18748216, 0.17364752, 0.22227034, 0.16981292,\n",
       "        0.18133293, 0.13555612, 0.323147  , 0.12997568, 0.16097282,\n",
       "        0.19130781, 0.12230463, 0.14801586, 0.12686455, 0.16933447,\n",
       "        0.1733269 , 0.17777857, 0.17521255, 0.18518785, 0.1551231 ,\n",
       "        0.17274018, 0.16917695, 0.19693154, 0.23770277, 0.20647202,\n",
       "        0.24198575, 0.47447327, 0.20124939, 0.13521042, 0.14055853,\n",
       "        0.15319134, 0.15877229, 0.1611573 , 0.1582095 , 0.2183995 ,\n",
       "        0.20466374, 0.14610197, 0.20026316, 0.21230768, 0.20383397,\n",
       "        0.10912886, 0.22391142, 0.1978106 , 0.15156294, 0.17161486,\n",
       "        0.15272062, 0.2115512 , 0.18613546, 0.19370426, 0.28387436,\n",
       "        0.21141466, 0.16069812, 0.18991213, 0.5745291 , 0.16874968,\n",
       "        0.15878478, 0.12184075, 0.14147581, 0.21761344, 0.15914425,\n",
       "        0.17720294, 0.13783038, 0.1869573 , 0.25750893, 0.24104235,\n",
       "        0.21806858, 0.15509342, 0.18789987, 0.178372  , 0.18121944,\n",
       "        0.15693316, 0.18712923, 0.23306131, 0.14755628, 0.15931447,\n",
       "        0.16826196, 0.31293482, 0.2759805 , 0.20671815, 0.20337594,\n",
       "        0.21146706, 0.28539497, 0.16697107, 0.1889349 , 0.18958518,\n",
       "        0.24317928, 0.17293319, 0.15916628, 0.22202282, 0.1367819 ,\n",
       "        0.11897124, 0.1682829 , 0.12008875, 0.16561899, 0.20572819,\n",
       "        0.17743191, 0.1294825 , 0.13512602, 0.17784813, 0.15665978,\n",
       "        0.1386028 , 0.17270082, 0.29294106, 0.27569515, 0.16369854,\n",
       "        0.20459267, 0.12129733, 0.16200921, 0.14090937, 0.20226263,\n",
       "        0.18806663, 0.22245082, 0.2642308 , 0.14264826, 0.18862191,\n",
       "        0.23948595, 0.16085725, 0.20324029, 0.15957987, 0.18836996,\n",
       "        0.1215483 , 0.14179587, 0.219214  , 0.17205605, 0.14799792,\n",
       "        0.1697541 , 0.18035713, 0.19946723, 0.20314498, 0.15630639,\n",
       "        0.14189957, 0.23975413, 0.2367146 , 0.28220406, 0.2782752 ,\n",
       "        0.18075444, 0.13157545, 0.1639975 , 0.15804571, 0.23327236,\n",
       "        0.16046007, 0.18358162, 0.22197476, 0.13825911, 0.21187678,\n",
       "        0.19435728, 0.16199383, 0.2610873 , 0.18062803, 0.29980668,\n",
       "        0.13731956, 0.2646644 , 0.17953926, 0.15189916, 0.14207223,\n",
       "        0.17513157, 0.1484707 , 0.18027455, 0.16228393, 0.28250667,\n",
       "        0.1318749 , 0.19670215, 0.16431116, 0.2159656 , 0.17592075,\n",
       "        0.15049756, 0.14599584, 0.29447025, 0.0205976 , 0.19558239,\n",
       "        0.10577404, 0.13897696, 0.23330344, 0.15635706, 0.17888242,\n",
       "        0.14088473, 0.25614068, 0.17446549, 0.18341912, 0.25280806,\n",
       "        0.10969613, 0.2940599 , 0.16987644, 0.13667217, 0.2889058 ,\n",
       "        0.13827838, 0.20141682, 0.2182749 , 0.2877032 , 0.17959332,\n",
       "        0.23469625, 0.14284644, 0.17040892, 0.14973338, 0.3919421 ,\n",
       "        0.11087817, 0.27820346, 0.20209137, 0.11426516, 0.16045341,\n",
       "        0.3026303 , 0.15273774, 0.16771503, 0.19768213, 0.13516927,\n",
       "        0.45502985, 0.13011353, 0.15876141, 0.15350254, 0.17717063,\n",
       "        0.28448483, 0.21595478, 0.19725932, 0.22067182, 0.14446305,\n",
       "        0.15226305, 0.21207148, 0.22808011, 0.16795833, 0.16770922,\n",
       "        0.12424182, 0.1343437 , 0.23904893, 0.13633655, 0.01292024,\n",
       "        0.14102088, 0.20619072, 0.24703178, 0.1988621 , 0.20096582,\n",
       "        0.16405231, 0.20822527, 0.22244388, 0.2553418 , 0.21839286,\n",
       "        0.21040083, 0.15170145, 0.2068911 , 0.1631676 , 0.19117299,\n",
       "        0.25774246, 0.20192973, 0.19540973, 0.13260314, 0.1231337 ,\n",
       "        0.15683135, 0.12338655, 0.21091458, 0.14584206, 0.18352519,\n",
       "        0.19640477, 0.2606964 , 0.15590426, 0.20923482, 0.21543409,\n",
       "        0.13870777, 0.18068475, 0.18653484, 0.15021402, 0.23391476,\n",
       "        0.32282972, 0.1641654 , 0.17637388, 0.23293903, 0.15253554,\n",
       "        0.15684846, 0.16899197, 0.18916878, 0.3141449 , 0.38026437,\n",
       "        0.16664232, 0.2082837 , 0.20761299, 0.1920801 , 0.17452553,\n",
       "        0.18125086, 0.16326644, 0.16494687, 0.27951106, 0.18028852,\n",
       "        0.17516194, 0.13339145, 0.23055898, 0.22419661, 0.11737917,\n",
       "        0.27694955, 0.13136783, 0.1862624 , 0.20724677, 0.25047904,\n",
       "        0.15688774, 0.16346808, 0.21888249, 0.10557286, 0.15088195,\n",
       "        0.3791271 , 0.12617728, 0.14101277, 0.24784404, 0.21169646,\n",
       "        0.1832042 , 0.19665855, 0.12358146, 0.22971673, 0.20084578,\n",
       "        0.21226723, 0.12175975, 0.17104043, 0.1850225 , 0.1579536 ,\n",
       "        0.1433513 , 0.16759433, 0.13931006, 0.17226276, 0.10587616,\n",
       "        0.17734888, 0.16981785, 0.17257811, 0.2066373 , 0.19466904,\n",
       "        0.16984947, 0.20554146, 0.18941854, 0.18313639, 0.22796206,\n",
       "        0.11563939, 0.1256158 , 0.16014269, 0.12164653, 0.18218386,\n",
       "        0.16751297, 0.1397743 , 0.24029563, 0.15350176, 0.17329143,\n",
       "        0.14110628, 0.28762892, 0.25327098, 0.21120027, 0.18305703,\n",
       "        0.17082767, 0.21586846, 0.12677288, 0.1894101 , 0.23672457,\n",
       "        0.19369623, 0.12389739, 0.23787339, 0.11818022, 0.13398513,\n",
       "        0.16080195, 0.22628492, 0.16890815, 0.2530012 , 0.20723262,\n",
       "        0.1675182 , 0.1822341 , 0.16349682, 0.11805759, 0.3770949 ,\n",
       "        0.2685058 , 0.20289482, 0.1419729 , 0.12124208, 0.16242622,\n",
       "        0.24647683, 0.1704857 , 0.15458092, 0.22035943, 0.17435944,\n",
       "        0.1849146 , 0.14050849, 0.24501435, 0.21153003, 0.3435214 ,\n",
       "        0.22233771, 0.13272269, 0.18668813, 0.217084  , 0.21954714,\n",
       "        0.3034587 , 0.21079507, 0.16340832, 0.15323974, 0.1873142 ,\n",
       "        0.12407185, 0.25789183, 0.21517618, 0.31111574, 0.20411608,\n",
       "        0.2189586 , 0.1734876 , 0.25332895, 0.21929285, 0.17341308,\n",
       "        0.275128  , 0.23360197, 0.15929253, 0.20014858, 0.28192496,\n",
       "        0.1046927 , 0.27526984, 0.20772628, 0.1318838 , 0.31607223,\n",
       "        0.1584864 , 0.22776455, 0.20056161, 0.12472407, 0.2502801 ,\n",
       "        0.17584723, 0.14307636, 0.21919827, 0.25236285, 0.19163081,\n",
       "        0.11304237, 0.2173437 , 0.20649876, 0.17835698, 0.23958582,\n",
       "        0.19057153, 0.12452311, 0.15780625, 0.18756117, 0.17765976,\n",
       "        0.24168506, 0.20444073, 0.12496939, 0.25238279, 0.26110002,\n",
       "        0.2548066 , 0.16790378, 0.21612008, 0.29485527, 0.5109251 ,\n",
       "        0.22892661, 0.16121759, 0.12615599, 0.16520227, 0.30746663,\n",
       "        0.18284604, 0.28254822, 0.33487564, 0.27047992, 0.17374645,\n",
       "        0.26857966, 0.17407593, 0.17447628, 0.2212087 , 0.1251964 ,\n",
       "        0.196819  , 0.28342357, 0.39667293, 0.26848373, 0.23447548,\n",
       "        0.2890389 , 0.2354304 , 0.29865724, 0.1335805 , 0.20122913,\n",
       "        0.22115557, 0.10522864, 0.11651431, 0.21645229, 0.1923442 ,\n",
       "        0.2130295 , 0.18074258, 0.3698177 , 0.19884808, 0.16167985,\n",
       "        0.18578263, 0.14518662, 0.2024902 , 0.22645546, 0.17398617,\n",
       "        0.17001653, 0.22126544, 0.22013028, 0.27961355, 0.21863134,\n",
       "        0.1494384 , 0.24915995, 0.14674397, 0.19946033, 0.16449234,\n",
       "        0.14883274, 0.23870526, 0.16765097, 0.16946208, 0.15991245,\n",
       "        0.2121287 , 0.16220278, 0.15385047, 0.21748072, 0.25990063,\n",
       "        0.19971146, 0.21715072, 0.2280343 , 0.20436487, 0.21060137,\n",
       "        0.17144974, 0.16528456, 0.17382474, 0.1846119 , 0.12255087,\n",
       "        0.1795667 , 0.15873732, 0.19219078, 0.18612827, 0.17231393,\n",
       "        0.19487253, 0.24125105, 0.24540965, 0.181739  , 0.16386667,\n",
       "        0.16409591, 0.196194  , 0.22256848, 0.14592415, 0.20226568,\n",
       "        0.1903201 , 0.17814289, 0.15332337, 0.16405387, 0.12420783,\n",
       "        0.15454577, 0.13727146, 0.18621136, 0.13190584, 0.16549051,\n",
       "        0.12582166, 0.11500327, 0.19559726, 0.1850879 , 0.16219695,\n",
       "        0.22130498, 0.18593366, 0.18147585, 0.1801466 , 0.21572657,\n",
       "        0.21172273, 0.16161692, 0.16317205, 0.13300529, 0.14608566,\n",
       "        0.26268658, 0.2626937 , 0.18036792, 0.22393167, 0.1734575 ,\n",
       "        0.24699946, 0.12703294, 0.14213796, 0.19484206, 0.21810602,\n",
       "        0.17592211, 0.18680662, 0.28090045, 0.18245183, 0.15285102,\n",
       "        0.18077506, 0.16846675, 0.23016019, 0.13397938, 0.2525912 ,\n",
       "        0.15876149, 0.13645753, 0.17524454, 0.13805744, 0.3181975 ,\n",
       "        0.17637517, 0.17026775, 0.14565477, 0.1542482 , 0.18626833,\n",
       "        0.1825126 , 0.18599081, 0.2119753 , 0.20146735, 0.1803592 ,\n",
       "        0.15498525, 0.15978129, 0.13963053, 0.20312935, 0.20793317,\n",
       "        0.30704713, 0.18426612, 0.24933535, 0.22772524, 0.11880121,\n",
       "        0.09593654, 0.17224514, 0.29821318, 0.10961974, 0.11935227,\n",
       "        0.21027556, 0.1407532 , 0.3128995 , 0.18564937, 0.19465347,\n",
       "        0.2651306 , 0.14932342, 0.18123819, 0.2192887 , 0.19409113,\n",
       "        0.23399177, 0.13348363, 0.13576481, 0.17810985, 0.3185496 ,\n",
       "        0.12435162, 0.15960494, 0.19558312, 0.1222409 , 0.1985003 ,\n",
       "        0.16918477, 0.2736619 , 0.23200047, 0.1884272 , 0.18038364,\n",
       "        0.17074016, 0.17678078, 0.13066597, 0.2922124 , 0.12326578,\n",
       "        0.25274068, 0.14370093, 0.16287927, 0.2150831 , 0.24158719,\n",
       "        0.15817839, 0.19638532, 0.1630401 , 0.15271074, 0.01588397,\n",
       "        0.2842154 , 0.12986898, 0.29120746, 0.16401534, 0.1425393 ,\n",
       "        0.19879656, 0.23615389, 0.21540459, 0.13055094, 0.17412028,\n",
       "        0.14590932, 0.21830621, 0.23178904, 0.30019954, 0.18507038,\n",
       "        0.19666445, 0.16879167, 0.25605527, 0.33650452, 0.10958457,\n",
       "        0.20740129, 0.24223432, 0.20155151, 0.15320475, 0.15853505,\n",
       "        0.20879506, 0.17698795, 0.21463934, 0.14655001, 0.17866094,\n",
       "        0.3810743 , 0.27330124, 0.23428062, 0.15334791, 0.23086421,\n",
       "        0.38767394, 0.22182497, 0.38346115, 0.1962143 , 0.12687922,\n",
       "        0.16975658, 0.15401368, 0.15714057, 0.20745933, 0.16575122,\n",
       "        0.17741321, 0.19025244, 0.14667733, 0.12940522, 0.19011657,\n",
       "        0.16386583, 0.13244672, 0.16239864, 0.31034526, 0.25498754,\n",
       "        0.14322476, 0.31109977, 0.14864145, 0.1362027 , 0.12982555,\n",
       "        0.22316784, 0.16684084, 0.22802556, 0.15843366, 0.22605681,\n",
       "        0.16841353, 0.15223277, 0.15457712, 0.31072387, 0.15287182,\n",
       "        0.14637162, 0.17840247, 0.21334466, 0.22324729, 0.15002635,\n",
       "        0.12426007, 0.18906295, 0.21243037, 0.19978918, 0.18249923,\n",
       "        0.2270924 , 0.27179942, 0.11982244, 0.370291  , 0.18009044,\n",
       "        0.21665211, 0.19935088, 0.19834058, 0.10884698, 0.19955808,\n",
       "        0.20607363, 0.13009961, 0.1344855 , 0.2795959 , 0.28498542,\n",
       "        0.1518916 , 0.13872509, 0.15578113, 0.3849901 , 0.21303879,\n",
       "        0.22838953, 0.22910114, 0.21088555, 0.24606185, 0.1681631 ,\n",
       "        0.3397308 , 0.26025298, 0.16460447, 0.12901363, 0.18495463,\n",
       "        0.20203221, 0.15507069, 0.2659184 , 0.145213  , 0.12521642,\n",
       "        0.2380368 , 0.21540399, 0.16907537, 0.20887771, 0.1090636 ,\n",
       "        0.17800938, 0.18549027, 0.1762242 , 0.15401423, 0.17396499,\n",
       "        0.13440366, 0.2228024 , 0.19625546, 0.2107265 , 0.2303793 ,\n",
       "        0.19572598, 0.18073389, 0.18268229, 0.2082101 , 0.14270668,\n",
       "        0.18210214, 0.14807685, 0.17229846, 0.16596599, 0.1967894 ,\n",
       "        0.10498085, 0.10110781, 0.27645907, 0.16271707, 0.2851493 ],\n",
       "       dtype=float32),\n",
       " array([0.24897562, 0.5321077 , 0.35877457, 0.5439918 , 0.26547983,\n",
       "        0.44781795, 0.5326773 , 0.2625413 , 0.47608837, 0.1989718 ,\n",
       "        0.9834072 , 0.4431798 , 0.23490222, 0.28257832, 0.56620765,\n",
       "        0.51008373, 0.5987363 , 0.7635731 , 0.2576522 , 0.30906153,\n",
       "        0.26158196, 0.27428848, 0.24906678, 0.34127164, 0.42294374,\n",
       "        0.7594051 , 0.7038359 , 0.43384165, 0.2449997 , 0.40870726,\n",
       "        0.5954534 , 0.59456295, 0.25539735, 0.23354071, 0.5225647 ,\n",
       "        0.28358734, 0.45737952, 0.2739697 , 0.44973683, 0.42333075,\n",
       "        0.4936165 , 0.30517733, 0.28606302, 0.6801039 , 0.2587839 ,\n",
       "        0.26918325, 0.4615655 , 0.22945112, 0.65234375, 0.31713727,\n",
       "        0.64397466, 0.6659459 , 0.45134965, 0.2988196 , 0.25858575,\n",
       "        0.7121664 , 0.68765265, 0.44363454, 0.5450468 , 0.53795624,\n",
       "        0.22459364, 0.71687764, 0.29341802, 0.7827891 , 0.5478765 ,\n",
       "        0.59959584, 0.4060605 , 0.29206488, 0.5226481 , 0.24823296,\n",
       "        0.27659053, 0.47379443, 0.33311135, 0.29031163, 0.28147152,\n",
       "        0.26597476, 0.3168265 , 0.35450602, 0.35851908, 0.46263236,\n",
       "        0.35663015, 0.33043665, 0.3463715 , 0.66842383, 0.22998841,\n",
       "        0.34528694, 0.2592333 , 0.42544895, 0.2674538 , 0.52277124,\n",
       "        0.52781075, 0.27962622, 0.25051498, 0.30806202, 0.26796606,\n",
       "        0.22994554, 0.29517734, 0.31144732, 0.32988927, 0.3324093 ,\n",
       "        0.27195802, 0.3370638 , 0.3415767 , 0.5216411 , 0.8189206 ,\n",
       "        0.5812668 , 0.25728506, 0.27434954, 0.49007726, 0.26150385,\n",
       "        0.22342223, 0.75207114, 0.38518882, 0.27143887, 0.22788525,\n",
       "        0.29639703, 0.2498918 , 0.3247972 , 0.37760204, 0.29612043,\n",
       "        0.6167176 , 0.31765145, 0.4021319 , 0.5429003 , 0.28133264,\n",
       "        0.25158432, 0.24949498, 0.37876332, 0.28733447, 0.7255187 ,\n",
       "        0.3024534 , 0.38916868, 0.30458713, 0.26159447, 0.47022492,\n",
       "        0.2566758 , 0.28404406, 0.28427532, 0.44057125, 0.2548055 ,\n",
       "        0.27970916, 0.44898537, 0.5034121 , 0.63624215, 0.23404567,\n",
       "        0.42160103, 0.75099844, 0.43073675, 0.54966784, 0.21723588,\n",
       "        0.47545013, 0.35220718, 0.3218677 , 0.6438753 , 1.0023093 ,\n",
       "        0.3304232 , 0.37547812, 0.46532735, 0.38096663, 0.48435256],\n",
       "       dtype=float32),\n",
       " array([0.14224887, 0.14008625, 0.27846897, 0.30815065, 0.31371722,\n",
       "        0.36267754, 0.35506073, 0.24625534, 0.25277472, 0.31023282,\n",
       "        0.11544055, 0.14942694, 0.32175612, 0.3164921 , 0.20842807,\n",
       "        0.31587267, 0.3747634 , 0.3682165 , 0.32628235, 0.32857966,\n",
       "        0.33036992, 0.18619905, 0.23635957, 0.1335805 , 0.14661399,\n",
       "        0.33238864, 0.24106614, 0.33312163, 0.2164505 , 0.16864944,\n",
       "        0.28880432, 0.23704399, 0.18029532, 0.0013222 , 0.14580174,\n",
       "        0.3367094 , 0.27949145, 0.29077587, 0.1253295 , 0.1608965 ,\n",
       "        0.26497316, 0.26307485, 0.1737106 , 0.2806275 , 0.20251885,\n",
       "        0.16435812, 0.2756351 , 0.11701328, 0.25012177, 0.34480384,\n",
       "        0.1095707 , 0.26913092, 0.2506764 , 0.32427478, 0.13579854,\n",
       "        0.2763297 , 0.15948957, 0.13371566, 0.14266443, 0.26099366,\n",
       "        0.23743434, 0.27473933, 0.25117412, 0.31773815, 0.21304923,\n",
       "        0.37607178, 0.1668454 , 0.2866141 , 0.20160715, 0.15764517,\n",
       "        0.10806108, 0.32501745, 0.29788116, 0.26759753, 0.34039548,\n",
       "        0.33750695, 0.25623864, 0.22727095, 0.3245304 , 0.29000834,\n",
       "        0.22834404, 0.297419  , 0.28519785, 0.27522078, 0.24881189,\n",
       "        0.26586485, 0.32400265, 0.1572604 , 0.29536137, 0.2243705 ,\n",
       "        0.27211037, 0.15332466, 0.20650426, 0.26380277, 0.13331364,\n",
       "        0.15741734, 0.32077947, 0.30975163, 0.16304757, 0.1946254 ,\n",
       "        0.3867468 , 0.22489953, 0.29652464, 0.27318966, 0.11031082,\n",
       "        0.19963473, 0.12861523, 0.22251113, 0.3256808 , 0.11949543,\n",
       "        0.29250103, 0.1235095 , 0.28048426, 0.11943333, 0.25123426,\n",
       "        0.28162363, 0.31738925, 0.3045524 , 0.19623564, 0.3074855 ,\n",
       "        0.1421709 , 0.13841201, 0.19319256, 0.13150845, 0.23440681,\n",
       "        0.1315805 , 0.33895972, 0.21627063, 0.3206787 , 0.24468353,\n",
       "        0.2856872 , 0.29628807, 0.3011559 , 0.3522951 , 0.11363076,\n",
       "        0.35251725, 0.2888562 , 0.20830804, 0.44976583, 0.15319575,\n",
       "        0.23401901, 0.13714081, 0.27511466, 0.30799082, 0.22112961,\n",
       "        0.22245485, 0.19183984, 0.32449144, 0.19330263, 0.37087503,\n",
       "        0.2614388 , 0.28623316, 0.2779    , 0.31067964, 0.36759365,\n",
       "        0.1183894 , 0.24291001, 0.31610057, 0.24270837, 0.25310302,\n",
       "        0.16208602, 0.18122163, 0.33075318, 0.33302632, 0.28284964,\n",
       "        0.1633958 , 0.17932951, 0.14334151, 0.24748509, 0.32160404,\n",
       "        0.18386269, 0.23949485, 0.34735835, 0.22226831, 0.11913291,\n",
       "        0.23695597, 0.1556659 , 0.12282114, 0.2711664 , 0.24400912,\n",
       "        0.18913151, 0.32793254, 0.24674729, 0.3194055 , 0.13261914,\n",
       "        0.24802357, 0.1672015 , 0.30755743, 0.3413642 , 0.16051628,\n",
       "        0.25466707, 0.25107613, 0.24126548, 0.26317456, 0.2545357 ,\n",
       "        0.2866591 , 0.1624285 , 0.18249239, 0.3241466 , 0.30304828,\n",
       "        0.3349028 , 0.33384788, 0.13178514, 0.2854903 , 0.24873373,\n",
       "        0.34436718, 0.26680383, 0.2925928 , 0.25093737, 0.33248988,\n",
       "        0.20894556, 0.31077543, 0.26259014, 0.29137263, 0.2599309 ,\n",
       "        0.33036163, 0.12987624, 0.3191957 , 0.18982261, 0.12942584,\n",
       "        0.17653148, 0.16361563, 0.16645935, 0.22260319, 0.20815901,\n",
       "        0.20911613, 0.15274999, 0.17500708, 0.13360308, 0.3102693 ,\n",
       "        0.36181474, 0.16695176, 0.19567302, 0.11881744, 0.31793633,\n",
       "        0.36156538, 0.287095  , 0.41218248, 0.37177923, 0.30919936,\n",
       "        0.34902033, 0.2982524 , 0.26511368, 0.18200627, 0.17577173,\n",
       "        0.2599109 , 0.36404356, 0.31042534, 0.24815254, 0.17690954,\n",
       "        0.12538202, 0.26639208, 0.31914535, 0.30322936, 0.20408785,\n",
       "        0.3215502 , 0.37597802, 0.33216032, 0.3361447 , 0.362905  ,\n",
       "        0.15490782, 0.11254197, 0.1845594 , 0.13239194, 0.14892784,\n",
       "        0.3294132 , 0.12953006, 0.34048876, 0.182918  , 0.31385565,\n",
       "        0.30055034, 0.31985497, 0.14458288, 0.33421123, 0.29820406,\n",
       "        0.266995  , 0.3168121 , 0.14531897, 0.25899175, 0.17564881,\n",
       "        0.1121106 , 0.3329085 , 0.29831538, 0.34076363, 0.26499775,\n",
       "        0.32903692, 0.21570425, 0.23119806, 0.25271395, 0.17168209,\n",
       "        0.30685106, 0.13823722, 0.11700455, 0.31834033, 0.1431031 ,\n",
       "        0.13731734, 0.12434184, 0.35408598, 0.1427693 , 0.2847893 ,\n",
       "        0.12451118, 0.1594653 , 0.294816  , 0.16195013, 0.17933626,\n",
       "        0.3104809 , 0.13152106, 0.37021184, 0.28703463, 0.31875315,\n",
       "        0.2835668 , 0.21980089, 0.16872038, 0.14205931, 0.16093534,\n",
       "        0.3187054 , 0.31848314, 0.2883606 , 0.3310808 , 0.18057944,\n",
       "        0.3350798 , 0.17811927, 0.32488763, 0.17280473, 0.18301652,\n",
       "        0.14295794, 0.3374581 , 0.15121861, 0.10014937, 0.23931396,\n",
       "        0.14060162, 0.30144694, 0.25902998, 0.11626016, 0.14433163,\n",
       "        0.14901023, 0.2773936 , 0.2342948 , 0.34667435, 0.19044252,\n",
       "        0.2761774 , 0.1255488 , 0.20646933, 0.21753757, 0.32246497,\n",
       "        0.13897823, 0.3197231 , 0.25832975, 0.17615406, 0.32949194,\n",
       "        0.3140282 , 0.16699357, 0.14980671, 0.31546417, 0.15083255,\n",
       "        0.3186683 , 0.30017167, 0.14922388, 0.33093032, 0.2978945 ,\n",
       "        0.22309119, 0.3770008 , 0.11414587, 0.15890515, 0.396097  ,\n",
       "        0.14847814, 0.37646055, 0.13337016, 0.32488132, 0.10183625,\n",
       "        0.09101525, 0.3259493 , 0.16111366, 0.2432121 , 0.28576583,\n",
       "        0.27336362, 0.27367726, 0.17638512, 0.14761598, 0.12694174,\n",
       "        0.33450663, 0.16971031, 0.27293712, 0.16732141, 0.11607172,\n",
       "        0.28759712, 0.2787792 , 0.2914821 , 0.18281904, 0.2174151 ,\n",
       "        0.25493246, 0.15569092, 0.36744428, 0.21063536, 0.31318524,\n",
       "        0.0941121 , 0.34817636, 0.2179931 , 0.3116525 , 0.28408092,\n",
       "        0.14826646, 0.15105924, 0.2997238 , 0.4462366 , 0.13962482,\n",
       "        0.30037227, 0.16297407, 0.14204086, 0.37775922, 0.2641392 ,\n",
       "        0.17782022, 0.15389271, 0.23976894, 0.3450743 , 0.33696553,\n",
       "        0.12057529, 0.17481205, 0.1355457 , 0.19192433, 0.2964348 ,\n",
       "        0.1967109 , 0.13849598, 0.3207832 , 0.32186493, 0.21299122,\n",
       "        0.20257089, 0.33285937, 0.29864776, 0.3450719 , 0.14164875,\n",
       "        0.21581931, 0.20363322, 0.22502084, 0.29785588, 0.146742  ,\n",
       "        0.2893575 , 0.1503841 , 0.3017441 , 0.30041692, 0.15269825,\n",
       "        0.12354715, 0.11427843, 0.25490397, 0.3172226 , 0.19103655,\n",
       "        0.3241798 , 0.13110678, 0.2650541 , 0.29539362, 0.10901821,\n",
       "        0.28582147, 0.25265032, 0.15602824, 0.26624176, 0.13694972,\n",
       "        0.14311019, 0.27510452, 0.19351569, 0.26470295, 0.3171036 ,\n",
       "        0.30460113, 0.16071868, 0.13534899, 0.3383449 , 0.17358565,\n",
       "        0.10175684, 0.31169316, 0.3270177 , 0.27839282, 0.24403444,\n",
       "        0.2939239 , 0.11849708, 0.15597382, 0.25926754, 0.30889532,\n",
       "        0.31999546, 0.2117922 , 0.15587106, 0.392911  , 0.3539777 ,\n",
       "        0.29952353, 0.36323   , 0.14206243, 0.37740448, 0.15247712,\n",
       "        0.35256627, 0.32201737, 0.30232075, 0.21543342, 0.12814225,\n",
       "        0.26630184, 0.21198502, 0.1502827 , 0.13594417, 0.17735626,\n",
       "        0.2128435 , 0.11237341, 0.13346636, 0.2236726 , 0.18716459,\n",
       "        0.29186952, 0.12477633, 0.4029102 , 0.12966159, 0.17772554,\n",
       "        0.33452767, 0.3047009 , 0.11327709, 0.09445179, 0.14541788,\n",
       "        0.3607157 , 0.12323809, 0.3460998 , 0.3077989 , 0.38199568,\n",
       "        0.19953871, 0.3054154 , 0.31221873, 0.10581128, 0.2705164 ,\n",
       "        0.2147392 , 0.35583442, 0.32098028, 0.10739765, 0.1334741 ,\n",
       "        0.29185307, 0.27526248, 0.18377243, 0.21995384, 0.16204993,\n",
       "        0.12448638, 0.13960752, 0.40416545, 0.18570828, 0.2857334 ,\n",
       "        0.18460457, 0.27263016, 0.2756504 , 0.34189874, 0.12835348,\n",
       "        0.3222254 , 0.2547379 , 0.10877527, 0.16116555, 0.34223992,\n",
       "        0.12810934, 0.12690642, 0.12374313, 0.18166503, 0.27384034,\n",
       "        0.12732998, 0.34108075, 0.14948633, 0.22435774, 0.31196645,\n",
       "        0.28723675, 0.3016536 , 0.3395003 , 0.12115442, 0.12463742,\n",
       "        0.19076541, 0.2848226 , 0.10450629, 0.31210634, 0.30726156,\n",
       "        0.15436865, 0.3042748 , 0.27592963, 0.23751761, 0.274432  ,\n",
       "        0.32273474, 0.31697103, 0.34377587, 0.3067324 , 0.31438577,\n",
       "        0.2988609 , 0.30273497, 0.14989533, 0.14402308, 0.20635416,\n",
       "        0.14959937, 0.3047002 , 0.2881977 , 0.25089407, 0.29991123,\n",
       "        0.18054192, 0.19330837, 0.2869046 , 0.31743813, 0.32746035,\n",
       "        0.25009286, 0.3298869 , 0.12180785, 0.24100526, 0.35806233,\n",
       "        0.14830025, 0.2498778 , 0.28620154, 0.1269784 , 0.34197223,\n",
       "        0.2960866 , 0.31904405, 0.31098837, 0.3231808 , 0.31160143,\n",
       "        0.14114428, 0.21780272, 0.3216931 , 0.21310998, 0.32037747,\n",
       "        0.17291276, 0.31477866, 0.15159889, 0.3368122 , 0.13411725,\n",
       "        0.13168436, 0.11895711, 0.20182896, 0.3099989 , 0.32623503,\n",
       "        0.31284449, 0.12623344, 0.21455966, 0.2946927 , 0.15391338,\n",
       "        0.34188485, 0.27658114, 0.22489251, 0.20891976, 0.14201504,\n",
       "        0.33280334, 0.12803318, 0.2995763 , 0.27888504, 0.20235609,\n",
       "        0.16170941, 0.27616352, 0.13097057, 0.22752991, 0.31362644,\n",
       "        0.1559055 , 0.14609495, 0.4135227 , 0.29717934, 0.31866184,\n",
       "        0.16149296, 0.27439868, 0.26420456, 0.30758062, 0.31183848,\n",
       "        0.29678997, 0.2064088 , 0.16166514, 0.32238993, 0.21393193,\n",
       "        0.3185282 , 0.26676908, 0.13090654, 0.29800606, 0.25065637,\n",
       "        0.2483609 , 0.29629847, 0.3330538 , 0.12929502, 0.16575013,\n",
       "        0.1463144 , 0.19175676, 0.2814215 , 0.3695376 , 0.12312362,\n",
       "        0.12855513, 0.39323863, 0.2049898 , 0.3474977 , 0.32040018,\n",
       "        0.30919755, 0.468676  , 0.13045903, 0.28629804, 0.13658793,\n",
       "        0.15615012, 0.16229351, 0.33324143, 0.15174234, 0.30578548,\n",
       "        0.133474  , 0.14177914, 0.19160749, 0.16435069, 0.21514578,\n",
       "        0.15345372, 0.153803  , 0.25696987, 0.30899552, 0.41630715,\n",
       "        0.17858589, 0.30902836, 0.2487978 , 0.2809803 , 0.36061415,\n",
       "        0.35097754, 0.30244353, 0.22712265, 0.12953171, 0.32102528,\n",
       "        0.3138947 , 0.29563308, 0.33671057, 0.10087628, 0.19116876,\n",
       "        0.30214047, 0.30967674, 0.2066574 , 0.1318609 , 0.3679532 ,\n",
       "        0.3279615 , 0.30782837, 0.30413395, 0.3879375 , 0.2804221 ,\n",
       "        0.24196383, 0.13578205, 0.1574108 , 0.33661562, 0.14817831,\n",
       "        0.35222474, 0.136209  , 0.16120268, 0.340166  , 0.36416715,\n",
       "        0.18562922, 0.20166098, 0.25120002, 0.11792856, 0.29584303,\n",
       "        0.178415  , 0.34224546, 0.4291757 , 0.14366284, 0.30577514,\n",
       "        0.31090462, 0.27294096, 0.3926216 , 0.3163289 , 0.33210045,\n",
       "        0.26729447, 0.3236823 , 0.27816683, 0.21751156, 0.29240376,\n",
       "        0.24556845, 0.31046167, 0.23314439, 0.40796286, 0.16057914,\n",
       "        0.28274393, 0.26992893, 0.32256594, 0.28959957, 0.33123997,\n",
       "        0.20772548, 0.29825446, 0.18594743, 0.42958543, 0.3366626 ,\n",
       "        0.37342134, 0.33249316, 0.30277932, 0.33266515, 0.11940651,\n",
       "        0.3011164 , 0.3917748 , 0.15024856, 0.13375175, 0.2242968 ,\n",
       "        0.33064166, 0.26826358, 0.34084433, 0.28477484, 0.2935846 ,\n",
       "        0.14538918, 0.2242576 , 0.29780495, 0.14273527, 0.25368387,\n",
       "        0.14539622, 0.31444308, 0.30158332, 0.18992856, 0.15760607,\n",
       "        0.31265873, 0.2760132 , 0.19782373, 0.18186164, 0.29783055,\n",
       "        0.16948803, 0.12977493, 0.1748774 , 0.2766825 , 0.11560927,\n",
       "        0.17173882, 0.29188493, 0.15124695, 0.1542785 , 0.29034138,\n",
       "        0.2960303 , 0.14394557, 0.11677299, 0.22639918, 0.13927524,\n",
       "        0.30184916, 0.12686288, 0.17289676, 0.29821375, 0.26371253,\n",
       "        0.12050566, 0.18135442, 0.3223384 , 0.33854547, 0.3645375 ,\n",
       "        0.3108008 , 0.15065862, 0.22875391, 0.24468797, 0.19158818,\n",
       "        0.34024605, 0.33735394, 0.32753214, 0.23271264, 0.35810003,\n",
       "        0.36194807, 0.19323148, 0.30322945, 0.1555135 , 0.13428712,\n",
       "        0.32904065, 0.3139845 , 0.33383328, 0.28699523, 0.2983142 ,\n",
       "        0.31228167, 0.3668318 , 0.14669006, 0.29749852, 0.30613267,\n",
       "        0.29683247, 0.2615613 , 0.32507342, 0.11609829, 0.25879356,\n",
       "        0.14855267, 0.27000698, 0.31066325, 0.30122617, 0.28609896,\n",
       "        0.15628359, 0.24770203, 0.30633855, 0.3356281 , 0.20758463,\n",
       "        0.20017895, 0.12250672, 0.13276759, 0.23784539, 0.31947252,\n",
       "        0.1296395 , 0.3133361 , 0.11276469, 0.2999046 , 0.17855617,\n",
       "        0.11905484, 0.34070364, 0.37107462, 0.13045032, 0.11851075,\n",
       "        0.30678242, 0.3398913 , 0.3490745 , 0.1354684 , 0.3994178 ,\n",
       "        0.23689477, 0.1541749 , 0.14608465, 0.1156846 , 0.23651926,\n",
       "        0.33764392, 0.1868894 , 0.1265242 , 0.14951962, 0.28390968,\n",
       "        0.29641992, 0.20383245, 0.14862143, 0.37827793, 0.31043833,\n",
       "        0.2007219 , 0.17473444, 0.11063949, 0.29900458, 0.27007622,\n",
       "        0.27239397, 0.24237485, 0.26879835, 0.33940917, 0.10995726,\n",
       "        0.2761975 , 0.11734558, 0.29045182, 0.31374833, 0.31630972,\n",
       "        0.11694676, 0.14931725, 0.25203228, 0.13853085, 0.271905  ,\n",
       "        0.13075711, 0.22941725, 0.18985537, 0.13357334, 0.2439828 ,\n",
       "        0.13520314, 0.13638096, 0.28278944, 0.06564483, 0.22680625,\n",
       "        0.27203077, 0.18407078, 0.30599   , 0.24567924, 0.34855047,\n",
       "        0.14460301, 0.13291036, 0.19102521, 0.33694717, 0.25672647,\n",
       "        0.28323054, 0.16940154, 0.12412778, 0.19304383, 0.25887632,\n",
       "        0.14700772, 0.11989098, 0.27637842, 0.33852378, 0.12383071,\n",
       "        0.18573847, 0.2669828 , 0.24318486, 0.384779  , 0.33808038,\n",
       "        0.2473074 , 0.28478223, 0.14407933, 0.31580102, 0.12197956,\n",
       "        0.2997139 , 0.20565465, 0.20587803, 0.28546724, 0.3294928 ],\n",
       "       dtype=float32),\n",
       " array([0.11675113, 0.13917927, 0.20549388, 0.2197002 , 0.2062056 ,\n",
       "        0.24075422, 0.21795675, 0.16866365, 0.18182665, 0.2062727 ,\n",
       "        0.12721132, 0.09978748, 0.21784876, 0.19642067, 0.16744985,\n",
       "        0.20504992, 0.2656834 , 0.26006955, 0.21851034, 0.2484539 ,\n",
       "        0.22818421, 0.10424267, 0.17976795, 0.13824451, 0.14783818,\n",
       "        0.21469858, 0.19019867, 0.228967  , 0.13411836, 0.14668447,\n",
       "        0.21166226, 0.1855836 , 0.10678798, 0.01866557, 0.11635201,\n",
       "        0.2612442 , 0.20413639, 0.2185332 , 0.1350845 , 0.1488975 ,\n",
       "        0.20787483, 0.21635182, 0.1475295 , 0.2133208 , 0.14304592,\n",
       "        0.16654003, 0.20121798, 0.14709298, 0.21425025, 0.24407291,\n",
       "        0.13299876, 0.19690305, 0.19378525, 0.22755602, 0.15331209,\n",
       "        0.21138519, 0.11230408, 0.13871564, 0.1719393 , 0.18814932,\n",
       "        0.16615948, 0.19004154, 0.15585433, 0.21733962, 0.18445814,\n",
       "        0.28482383, 0.11566185, 0.2074427 , 0.1329837 , 0.10255449,\n",
       "        0.09756026, 0.19562648, 0.21668528, 0.18102793, 0.2262833 ,\n",
       "        0.21704671, 0.20142955, 0.14208078, 0.24473214, 0.15286039,\n",
       "        0.14702618, 0.18313168, 0.19876648, 0.21473564, 0.19427706,\n",
       "        0.1929781 , 0.2236168 , 0.13508174, 0.19136745, 0.14363204,\n",
       "        0.17753097, 0.13628294, 0.14513786, 0.22313188, 0.13270216,\n",
       "        0.09672075, 0.21329603, 0.20882863, 0.15585794, 0.17096551,\n",
       "        0.28179193, 0.17621838, 0.19538888, 0.20422655, 0.1271427 ,\n",
       "        0.1333606 , 0.12917116, 0.15894546, 0.24053848, 0.14690304,\n",
       "        0.20663042, 0.16232331, 0.22570841, 0.10555233, 0.21007478,\n",
       "        0.22125287, 0.21534264, 0.20159034, 0.1767857 , 0.22774668,\n",
       "        0.14487801, 0.15168118, 0.15221071, 0.11208494, 0.20297033,\n",
       "        0.09961475, 0.2311535 , 0.1521649 , 0.21783301, 0.20902896,\n",
       "        0.20576242, 0.24553205, 0.21523936, 0.22171178, 0.12086476,\n",
       "        0.23172866, 0.21071641, 0.17455946, 0.2554666 , 0.1399159 ,\n",
       "        0.14905383, 0.13351466, 0.21801393, 0.22360618, 0.14289579,\n",
       "        0.16215391, 0.14818723, 0.20347576, 0.15634626, 0.25362453,\n",
       "        0.18758012, 0.21891662, 0.2148845 , 0.22091459, 0.27517658,\n",
       "        0.12496779, 0.26912677, 0.23038824, 0.15696858, 0.2042037 ,\n",
       "        0.1331976 , 0.16179378, 0.24646246, 0.23658799, 0.22024097,\n",
       "        0.15047903, 0.16499282, 0.13081843, 0.19114582, 0.19668567,\n",
       "        0.17051305, 0.14659895, 0.24433675, 0.16074903, 0.15849914,\n",
       "        0.13718963, 0.14380969, 0.15270722, 0.20543708, 0.17919767,\n",
       "        0.18902689, 0.19992854, 0.18353264, 0.2449315 , 0.18354034,\n",
       "        0.19189297, 0.11940975, 0.22998132, 0.22499618, 0.1133566 ,\n",
       "        0.19535235, 0.17661285, 0.14129531, 0.18038283, 0.1719816 ,\n",
       "        0.20565072, 0.12003879, 0.11691619, 0.20969035, 0.23956276,\n",
       "        0.27179393, 0.2483911 , 0.11725984, 0.20590305, 0.20587602,\n",
       "        0.25333226, 0.20400095, 0.22834513, 0.18868195, 0.20635065,\n",
       "        0.13340539, 0.21664536, 0.2043454 , 0.20498788, 0.1639207 ,\n",
       "        0.25788352, 0.16491917, 0.20379439, 0.14808862, 0.11181896,\n",
       "        0.1250906 , 0.1194112 , 0.13587616, 0.24289438, 0.13700964,\n",
       "        0.17608099, 0.15715057, 0.16143848, 0.1145745 , 0.19934185,\n",
       "        0.2464631 , 0.19163196, 0.14192516, 0.14292665, 0.1754842 ,\n",
       "        0.2487305 , 0.21610886, 0.3087483 , 0.26574543, 0.24575724,\n",
       "        0.23290078, 0.20364054, 0.18745692, 0.14835978, 0.13148066,\n",
       "        0.21103042, 0.2541611 , 0.20268244, 0.18870121, 0.1305876 ,\n",
       "        0.13007477, 0.2043946 , 0.22372025, 0.19510093, 0.19069396,\n",
       "        0.19527407, 0.22996996, 0.2597074 , 0.25341097, 0.22506453,\n",
       "        0.14187121, 0.09755963, 0.13083012, 0.11724494, 0.12618047,\n",
       "        0.23374678, 0.13058987, 0.2671845 , 0.16744497, 0.20453762,\n",
       "        0.22891583, 0.21609107, 0.15255752, 0.22158329, 0.20335843,\n",
       "        0.19498432, 0.25428528, 0.11986735, 0.23162654, 0.1508372 ,\n",
       "        0.11629017, 0.22741722, 0.21279834, 0.24424107, 0.1956681 ,\n",
       "        0.21516119, 0.18477097, 0.12736729, 0.19270779, 0.14091866,\n",
       "        0.19401906, 0.12040442, 0.13466297, 0.21178988, 0.13306546,\n",
       "        0.10554537, 0.14617823, 0.22305296, 0.16310234, 0.21287197,\n",
       "        0.14816165, 0.12404647, 0.21760967, 0.154348  , 0.16109414,\n",
       "        0.24881493, 0.12924872, 0.23153763, 0.20762517, 0.20654246,\n",
       "        0.22230044, 0.17833449, 0.11670795, 0.10798524, 0.14620715,\n",
       "        0.23365371, 0.2224933 , 0.20813164, 0.22871393, 0.14455442,\n",
       "        0.23729558, 0.14725685, 0.29479286, 0.1331499 , 0.12354596,\n",
       "        0.10123917, 0.22239365, 0.12872188, 0.2015208 , 0.16322239,\n",
       "        0.1553844 , 0.23214537, 0.20067984, 0.15198232, 0.12459383,\n",
       "        0.15513761, 0.22825626, 0.1954034 , 0.21674411, 0.151602  ,\n",
       "        0.19409971, 0.13162875, 0.1731251 , 0.14894296, 0.245258  ,\n",
       "        0.14612393, 0.22556594, 0.19982353, 0.13900897, 0.23009981,\n",
       "        0.21583411, 0.1280494 , 0.15329342, 0.20008536, 0.1572829 ,\n",
       "        0.25791237, 0.21020333, 0.13071053, 0.23448884, 0.21826915,\n",
       "        0.16796294, 0.26862788, 0.11049926, 0.12973143, 0.25052473,\n",
       "        0.1175387 , 0.2512236 , 0.14161947, 0.22587514, 0.1132555 ,\n",
       "        0.2332809 , 0.23315422, 0.14156312, 0.17359346, 0.19194037,\n",
       "        0.20576607, 0.19139877, 0.1579579 , 0.11427156, 0.10154409,\n",
       "        0.2517427 , 0.1400265 , 0.22626415, 0.16127092, 0.1296279 ,\n",
       "        0.20673089, 0.20163117, 0.19956611, 0.15648097, 0.17952314,\n",
       "        0.20568141, 0.1354211 , 0.21296602, 0.21664132, 0.21559142,\n",
       "        0.09665643, 0.22867182, 0.16787921, 0.21896662, 0.22549431,\n",
       "        0.15240891, 0.12200986, 0.21650161, 0.25336388, 0.10713929,\n",
       "        0.19232847, 0.14008372, 0.1071287 , 0.2891558 , 0.22836511,\n",
       "        0.11413252, 0.11768986, 0.20031482, 0.24837592, 0.22722884,\n",
       "        0.13112436, 0.15825707, 0.11647096, 0.16059229, 0.18334973,\n",
       "        0.13278718, 0.12020057, 0.2112052 , 0.21057951, 0.16072318,\n",
       "        0.15659069, 0.22880848, 0.22395407, 0.21346883, 0.15042701,\n",
       "        0.1265233 , 0.15858468, 0.17176141, 0.19321197, 0.13094376,\n",
       "        0.2025677 , 0.10722386, 0.2162176 , 0.22885583, 0.12820575,\n",
       "        0.16722201, 0.17960578, 0.20509069, 0.21205571, 0.12927757,\n",
       "        0.21192528, 0.15140979, 0.19461595, 0.21875145, 0.1366921 ,\n",
       "        0.20650357, 0.19734985, 0.13526036, 0.21814415, 0.12577297,\n",
       "        0.15088883, 0.19890572, 0.16787651, 0.17275582, 0.19753097,\n",
       "        0.21930444, 0.14443584, 0.15881015, 0.2511791 , 0.12836947,\n",
       "        0.14876565, 0.24382928, 0.22559647, 0.1946157 , 0.1983503 ,\n",
       "        0.18202263, 0.10163557, 0.14539927, 0.19161695, 0.23002577,\n",
       "        0.19903415, 0.1566445 , 0.14001137, 0.25320014, 0.23211208,\n",
       "        0.22186935, 0.277715  , 0.13029875, 0.28463808, 0.12567762,\n",
       "        0.23865338, 0.21986924, 0.22710341, 0.17557263, 0.10919541,\n",
       "        0.18612756, 0.20663176, 0.11762799, 0.14455566, 0.13210788,\n",
       "        0.1563932 , 0.14458711, 0.18560131, 0.16357258, 0.20175931,\n",
       "        0.21882208, 0.14550301, 0.25241295, 0.20011197, 0.14199297,\n",
       "        0.22011387, 0.21910685, 0.14777084, 0.14349787, 0.10645203,\n",
       "        0.25835106, 0.12707266, 0.20538652, 0.21009104, 0.3316514 ,\n",
       "        0.16046195, 0.22821116, 0.20947716, 0.13800769, 0.23515561,\n",
       "        0.17253754, 0.21681435, 0.22617541, 0.14521855, 0.11716091,\n",
       "        0.22408192, 0.22184254, 0.1433074 , 0.17995109, 0.13741644,\n",
       "        0.14041503, 0.19119349, 0.2593148 , 0.12590693, 0.22425252,\n",
       "        0.14330511, 0.20416157, 0.18216488, 0.23952949, 0.1490431 ,\n",
       "        0.24777229, 0.19614998, 0.15034285, 0.17114921, 0.25696665,\n",
       "        0.1715932 , 0.16658571, 0.12077251, 0.13168655, 0.23259373,\n",
       "        0.13750531, 0.22925803, 0.15757707, 0.16607305, 0.21966687,\n",
       "        0.2102209 , 0.2089421 , 0.2513927 , 0.14332427, 0.13758844,\n",
       "        0.12067154, 0.21331882, 0.12645172, 0.20961183, 0.24909163,\n",
       "        0.12908462, 0.2033257 , 0.21772933, 0.24109666, 0.22787707,\n",
       "        0.22923899, 0.22090039, 0.23458487, 0.2472436 , 0.23428714,\n",
       "        0.21004139, 0.22558007, 0.14810425, 0.12383585, 0.1704746 ,\n",
       "        0.11575935, 0.2194152 , 0.20690143, 0.1605358 , 0.21185438,\n",
       "        0.14593166, 0.15119645, 0.22481948, 0.24377929, 0.24135095,\n",
       "        0.2021172 , 0.20637338, 0.14423344, 0.1798587 , 0.24566346,\n",
       "        0.1261501 , 0.18946229, 0.20512436, 0.14710732, 0.20625399,\n",
       "        0.24213274, 0.21648337, 0.22272575, 0.2135149 , 0.2086925 ,\n",
       "        0.08439081, 0.14068806, 0.20236795, 0.18563236, 0.26296595,\n",
       "        0.13364021, 0.20308943, 0.14014018, 0.23179853, 0.121576  ,\n",
       "        0.12489821, 0.12255789, 0.18019025, 0.22625719, 0.21760969,\n",
       "        0.22742634, 0.1217256 , 0.17767033, 0.21260601, 0.13954124,\n",
       "        0.22961694, 0.22002506, 0.1413526 , 0.12656862, 0.12390266,\n",
       "        0.2648187 , 0.14060585, 0.23303944, 0.2066319 , 0.18150428,\n",
       "        0.13579893, 0.21362165, 0.114569  , 0.18232286, 0.20453466,\n",
       "        0.14121436, 0.1235203 , 0.24344917, 0.21368045, 0.21411526,\n",
       "        0.18456517, 0.22562836, 0.1953835 , 0.25144652, 0.20820026,\n",
       "        0.21692786, 0.15418305, 0.15642746, 0.19915624, 0.19192433,\n",
       "        0.20619251, 0.21491359, 0.14471471, 0.20688005, 0.18407875,\n",
       "        0.11276014, 0.2456471 , 0.24238099, 0.10987668, 0.16284539,\n",
       "        0.11680411, 0.17733675, 0.19289592, 0.2652439 , 0.12088102,\n",
       "        0.1571414 , 0.2695961 , 0.14325947, 0.2408582 , 0.22171505,\n",
       "        0.22205225, 0.27025834, 0.1700928 , 0.20605622, 0.10376161,\n",
       "        0.24818602, 0.19199125, 0.20498031, 0.12241206, 0.22790246,\n",
       "        0.12468786, 0.12788744, 0.23985557, 0.14368811, 0.18110617,\n",
       "        0.1334093 , 0.11381095, 0.16894782, 0.23863691, 0.35202336,\n",
       "        0.23225193, 0.20130573, 0.18348   , 0.20239305, 0.26353177,\n",
       "        0.21343048, 0.22660871, 0.14036365, 0.1317737 , 0.23475048,\n",
       "        0.21924484, 0.20248525, 0.22949189, 0.16263328, 0.18410465,\n",
       "        0.20025273, 0.21395776, 0.15590858, 0.14082772, 0.27292284,\n",
       "        0.22696279, 0.23054036, 0.21097164, 0.2653973 , 0.20623153,\n",
       "        0.17541283, 0.13592114, 0.17380676, 0.22014603, 0.10277487,\n",
       "        0.2514531 , 0.15818442, 0.15883139, 0.2483266 , 0.24881144,\n",
       "        0.10051405, 0.16920477, 0.18271738, 0.16453679, 0.21073072,\n",
       "        0.12074262, 0.21979707, 0.24080408, 0.153636  , 0.22636893,\n",
       "        0.20221008, 0.15549456, 0.26202524, 0.22523381, 0.609916  ,\n",
       "        0.18434644, 0.21689288, 0.18190126, 0.13661332, 0.1980894 ,\n",
       "        0.18317036, 0.2208885 , 0.14770441, 0.26129645, 0.12956445,\n",
       "        0.22961555, 0.19969779, 0.21758342, 0.225292  , 0.22901419,\n",
       "        0.1773401 , 0.21200518, 0.14334469, 0.2472565 , 0.25351873,\n",
       "        0.23972613, 0.22389086, 0.21461925, 0.20217897, 0.14761636,\n",
       "        0.22840679, 0.25842714, 0.12766035, 0.11873302, 0.19550906,\n",
       "        0.25012112, 0.17448248, 0.25351232, 0.20191461, 0.19390886,\n",
       "        0.14182773, 0.15727857, 0.21114172, 0.12232853, 0.20384774,\n",
       "        0.14604056, 0.22131188, 0.22241229, 0.13216162, 0.15411274,\n",
       "        0.225692  , 0.20557809, 0.13888519, 0.13038835, 0.21636191,\n",
       "        0.1727116 , 0.11743258, 0.18262883, 0.2092535 , 0.10824008,\n",
       "        0.13390274, 0.20287499, 0.16015115, 0.18694355, 0.20948672,\n",
       "        0.20143235, 0.12360234, 0.14303291, 0.14255753, 0.1306663 ,\n",
       "        0.19925544, 0.1559774 , 0.13960078, 0.20606495, 0.19843778,\n",
       "        0.12006358, 0.11889897, 0.22007312, 0.19612063, 0.18361755,\n",
       "        0.24080482, 0.14464444, 0.18417723, 0.20283061, 0.12702234,\n",
       "        0.2410964 , 0.22513126, 0.25131682, 0.19652195, 0.23548216,\n",
       "        0.22550084, 0.11526272, 0.19575715, 0.12059301, 0.1422514 ,\n",
       "        0.23026386, 0.22864576, 0.26752323, 0.20225513, 0.20283078,\n",
       "        0.22162028, 0.2295986 , 0.12040386, 0.24296296, 0.22964685,\n",
       "        0.20716819, 0.15449458, 0.23712431, 0.1708983 , 0.19150896,\n",
       "        0.09830383, 0.21826975, 0.20483053, 0.22106224, 0.2151867 ,\n",
       "        0.16055988, 0.17620315, 0.21679026, 0.2322181 , 0.16814935,\n",
       "        0.18481812, 0.15752645, 0.0863347 , 0.18859684, 0.22464514,\n",
       "        0.10387395, 0.21842282, 0.10232533, 0.21549723, 0.13569339,\n",
       "        0.12923308, 0.21544804, 0.23370476, 0.1563026 , 0.08971426,\n",
       "        0.21138872, 0.21539705, 0.22992885, 0.09015135, 0.23869069,\n",
       "        0.17774369, 0.13001333, 0.11508059, 0.12878707, 0.20619597,\n",
       "        0.23314591, 0.14528072, 0.24832979, 0.12187327, 0.20850845,\n",
       "        0.22119144, 0.17584234, 0.1299016 , 0.28610033, 0.22821355,\n",
       "        0.12869278, 0.14358379, 0.16359462, 0.20535408, 0.2032818 ,\n",
       "        0.19681004, 0.1971416 , 0.19351922, 0.23514727, 0.17540696,\n",
       "        0.21606326, 0.15877882, 0.21482536, 0.19272879, 0.20510952,\n",
       "        0.10080752, 0.15623744, 0.20041281, 0.15146194, 0.21724555,\n",
       "        0.13023168, 0.1876842 , 0.14769755, 0.15314265, 0.15371753,\n",
       "        0.14194398, 0.10925985, 0.21458162, 0.16074917, 0.1982299 ,\n",
       "        0.18516015, 0.14222868, 0.19079949, 0.19670366, 0.23580298,\n",
       "        0.15544304, 0.12410412, 0.14857775, 0.21458174, 0.22980462,\n",
       "        0.21145925, 0.13475405, 0.14135616, 0.13576372, 0.20441936,\n",
       "        0.1246669 , 0.16803177, 0.21652605, 0.22920913, 0.14344631,\n",
       "        0.16040723, 0.18706867, 0.18821046, 0.30304697, 0.2114762 ,\n",
       "        0.19660343, 0.21628019, 0.1581774 , 0.21275902, 0.11430261,\n",
       "        0.24006972, 0.17002217, 0.1422389 , 0.22943354, 0.21232645],\n",
       "       dtype=float32),\n",
       " array([0.23665492, 0.23544487, 0.24120773, 0.23393945, 0.22703145,\n",
       "        0.22910906, 0.23643382, 0.23316896, 0.23542154, 0.2368126 ,\n",
       "        0.22844183, 0.23185036, 0.2427865 , 0.24446705, 0.23473896,\n",
       "        0.23703548, 0.23479094, 0.23393402, 0.24310075, 0.23758425,\n",
       "        0.23628013, 0.23632136, 0.23200402, 0.23786265, 0.2345343 ,\n",
       "        0.24374579, 0.23160432, 0.23840354, 0.22799645, 0.23172005,\n",
       "        0.23474877, 0.2372328 , 0.23307548, 0.23132256, 0.23218106,\n",
       "        0.2359265 , 0.23206662, 0.23983186, 0.232616  , 0.23836997,\n",
       "        0.24287963, 0.23575604, 0.23187159, 0.23971483, 0.22853386,\n",
       "        0.22880496, 0.23244555, 0.23749271, 0.22905314, 0.22448161,\n",
       "        0.22824109, 0.2257783 , 0.22447506, 0.22525048, 0.23994935,\n",
       "        0.23857717, 0.24189326, 0.23050949, 0.24598232, 0.23423935,\n",
       "        0.23161744, 0.235806  , 0.23299089, 0.23794386, 0.23708196,\n",
       "        0.2300891 , 0.22548063, 0.22998679, 0.22722882, 0.23295274,\n",
       "        0.230089  , 0.23147953, 0.23921564, 0.23769806, 0.23199846,\n",
       "        0.23406844, 0.23817958, 0.23582353, 0.23433308, 0.23756567,\n",
       "        0.24521597, 0.23056015, 0.24027453, 0.23796742, 0.23386608,\n",
       "        0.24137294, 0.2349033 , 0.2368536 , 0.23219183, 0.23935598,\n",
       "        0.2366277 , 0.23448017, 0.23286861, 0.23650527, 0.23344116,\n",
       "        0.24653518, 0.24344075, 0.23710263, 0.23503694, 0.2398471 ,\n",
       "        0.23447987, 0.23558174, 0.22903046, 0.22943412, 0.23603785,\n",
       "        0.23107874, 0.23438488, 0.22915053, 0.23380683, 0.22819898,\n",
       "        0.23142888, 0.24382102, 0.2309172 , 0.22999929, 0.2395629 ,\n",
       "        0.23336306, 0.23617943, 0.22885823, 0.2379684 , 0.23720035,\n",
       "        0.23354134, 0.23168087, 0.23045047, 0.23850627, 0.24274287,\n",
       "        0.23538986, 0.23790613, 0.23540114, 0.23582195, 0.23225492,\n",
       "        0.23256287, 0.23476575, 0.2281877 , 0.22976565, 0.23151304,\n",
       "        0.23380785, 0.2385716 , 0.22804762, 0.23492995, 0.23511097,\n",
       "        0.23821197, 0.23443204, 0.24191102, 0.2282107 , 0.23731297,\n",
       "        0.23563768, 0.24225649, 0.23559019, 0.23091525, 0.22590335,\n",
       "        0.24342366, 0.23580407, 0.2368339 , 0.22825696, 0.22908023,\n",
       "        0.23796676, 0.23595595, 0.2326569 , 0.2359882 , 0.23597352,\n",
       "        0.2320052 , 0.23451845, 0.24296364, 0.23450279, 0.23333052,\n",
       "        0.23997174, 0.23055887, 0.23708355, 0.23222713, 0.24021041,\n",
       "        0.2293913 , 0.23756585, 0.2376113 , 0.23134212, 0.23388937,\n",
       "        0.2441358 , 0.22725257, 0.23340008, 0.23897476, 0.22996041,\n",
       "        0.23311414, 0.23197341, 0.23166814, 0.23172787, 0.2340755 ,\n",
       "        0.24163409, 0.23237723, 0.23791096, 0.23968303, 0.23887303,\n",
       "        0.22896114, 0.22604094, 0.23877959, 0.22304174, 0.23305461,\n",
       "        0.2343053 , 0.25496647, 0.23597711, 0.22976367, 0.23518321,\n",
       "        0.22726545, 0.24379808, 0.23487361, 0.23511228, 0.23508604,\n",
       "        0.22577769, 0.2410628 , 0.23982039, 0.22905883, 0.23311687,\n",
       "        0.23417515, 0.23018996, 0.2300274 , 0.23377171, 0.23321447,\n",
       "        0.23002866, 0.233118  , 0.23228271, 0.23652568, 0.23426338,\n",
       "        0.22853749, 0.23076183, 0.23705113, 0.24003993, 0.23999025,\n",
       "        0.23428558, 0.23870565, 0.2420091 , 0.23279925, 0.23531625,\n",
       "        0.22976923, 0.22607002, 0.23368973, 0.23566534, 0.23053204,\n",
       "        0.2356235 , 0.23098837, 0.23059523, 0.22706585, 0.2303725 ,\n",
       "        0.23773566, 0.23963171, 0.23428904, 0.23672013, 0.23323607,\n",
       "        0.23653524, 0.23464015, 0.22910848, 0.23414268, 0.23435283,\n",
       "        0.23283538, 0.23430716, 0.23157665, 0.23688954, 0.23656473,\n",
       "        0.23309995, 0.23093158, 0.23667975, 0.23671374, 0.23581702,\n",
       "        0.22925839, 0.23877299, 0.23107477, 0.23582125, 0.2354054 ,\n",
       "        0.23445143, 0.22794972, 0.23381576, 0.23413585, 0.2310291 ,\n",
       "        0.23375967, 0.24304524, 0.24185458, 0.23415656, 0.2347712 ,\n",
       "        0.23381484, 0.23137803, 0.2390058 , 0.23344284, 0.23543863,\n",
       "        0.24063903, 0.24033341, 0.23442312, 0.22698496, 0.23790345,\n",
       "        0.23557955, 0.23121762, 0.23677115, 0.25162557, 0.23352922,\n",
       "        0.23532738, 0.23559566, 0.23059765, 0.23992993, 0.2319183 ,\n",
       "        0.2310388 , 0.23596369, 0.23264691, 0.24124867, 0.23459597,\n",
       "        0.24227329, 0.23925251, 0.22866723, 0.2278536 , 0.2356307 ,\n",
       "        0.23100792, 0.23692764, 0.23508947, 0.22850938, 0.23278354,\n",
       "        0.24420705, 0.2361172 , 0.23339652, 0.23010689, 0.22398993,\n",
       "        0.23343728, 0.2291417 , 0.23495397, 0.22947353, 0.23695166],\n",
       "       dtype=float32),\n",
       " array([1.51081  , 1.4852315, 1.5666837, ..., 1.504977 , 1.4555138,\n",
       "        1.5257411], dtype=float32)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80,
   "source": [
    "\n",
    "# Step 2: Prune layers\n",
    "def prune_layer(layer, prune_indices, is_input=False):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        if is_input:\n",
    "            # Prune input channels\n",
    "            weight = layer.weight.detach().cpu()\n",
    "            new_weight = weight[:, torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "            \n",
    "            layer.in_channels = new_weight.size(1)\n",
    "            layer.weight = nn.Parameter(new_weight).to(device=DEVICE)\n",
    "        else:\n",
    "            # Prune output channels\n",
    "            weight = layer.weight.detach().cpu()\n",
    "            new_weight = weight[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "            \n",
    "            layer.out_channels = new_weight.size(0)\n",
    "            layer.weight = nn.Parameter(new_weight).to(device=DEVICE)\n",
    "            \n",
    "            # Adjust the 'groups' parameter if it's a depthwise convolution\n",
    "            if layer.groups == layer.in_channels:\n",
    "                layer.groups = new_weight.size(0)\n",
    "                layer.in_channels = new_weight.size(0)\n",
    "                \n",
    "    \n",
    "    elif isinstance(layer, nn.BatchNorm2d):\n",
    "        # Prune BatchNorm parameters\n",
    "        layer.weight = nn.Parameter(layer.weight.detach()[torch.tensor(prune_indices)].clone()).to(device=DEVICE)\n",
    "        layer.bias = nn.Parameter(layer.bias.detach()[torch.tensor(prune_indices)].clone()).to(device=DEVICE)\n",
    "        layer.running_mean = layer.running_mean.detach()[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "        layer.running_var = layer.running_var.detach()[torch.tensor(prune_indices)].clone().to(device=DEVICE)\n",
    "        \n",
    "        layer.num_features = layer.weight.size(0)\n",
    "\n",
    "# Traverse the model and prune connected layers\n",
    "for i, (name, module) in enumerate(layer_list):\n",
    "    if name in pruned_channels:\n",
    "        prune_indices = pruned_channels[name]\n",
    "        \n",
    "        # Prune the current BatchNorm layer\n",
    "        prune_layer(module, prune_indices)\n",
    "        \n",
    "        # Prune the preceding Conv2d (output channels)\n",
    "        if i > 0:\n",
    "            prev_name, prev_module = layer_list[i - 1]\n",
    "            if isinstance(prev_module, nn.Conv2d):\n",
    "                prune_layer(prev_module, prune_indices, is_input=False)\n",
    "                if prev_module.groups == prev_module.in_channels:\n",
    "                    j = i - 2\n",
    "                    while j > 0:\n",
    "                        prev_name, prev_module = layer_list[j]\n",
    "                        if isinstance(prev_module, nn.BatchNorm2d):\n",
    "                            prune_layer(prev_module, prune_indices)\n",
    "                            prev_name, prev_module = layer_list[j-1]\n",
    "                            if isinstance(prev_module, nn.Conv2d):\n",
    "                                prune_layer(prev_module, prune_indices, is_input=False)\n",
    "                                break\n",
    "                        j -= 1\n",
    "                        \n",
    "\n",
    "        # Prune the following Conv2d (input channels)\n",
    "        j = i\n",
    "        # if i < len(layer_list) - 1:\n",
    "        #     next_name, next_module = layer_list[i + 2]      # Next conv2d comes after ReLU6 activation layer\n",
    "        #     if isinstance(next_module, nn.Conv2d):\n",
    "        #         prune_layer(next_module, prune_indices, is_input=True)\n",
    "        while j < len(layer_list) - 1:\n",
    "            next_name, next_module = layer_list[j + 1]\n",
    "            if isinstance(next_module, nn.Conv2d):\n",
    "                prune_layer(next_module, prune_indices, is_input=True)\n",
    "                break\n",
    "            j += 1\n"
   ],
   "id": "4c538c30a950a3f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
